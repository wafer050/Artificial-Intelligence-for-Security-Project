{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUZdLiGMZ-qd"
      },
      "source": [
        "#\n",
        "<h1><span style=\"color:blue\">AI for Security Project: Clank Ops</span></h1>\n",
        "\n",
        "<p><em>Dataset:</em> <a href=\"https://huggingface.co/datasets/qualifire/prompt-injections-benchmark/viewer/default/test?row=85&views%5B%5D=test\" target=\"_blank\">Jailbreak attacks on LLMs</a></p>\n",
        "\n",
        "<h2>Here we are! Four beautiful minds at work:</h2>\n",
        "<ul>\n",
        "    <li><strong>Tiziano Rossi</strong></li>\n",
        "    <li><strong>Benedetta Tesi</strong></li>\n",
        "    <li><strong>Francesco Friolo</strong></li>\n",
        "    <li><strong>Francesca Falco</strong></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r_0yoRmmqVi"
      },
      "source": [
        "#Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dpvYTxjIrwt"
      },
      "source": [
        "Mounts Google Drive to enable file access and persistence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29f6208b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Mount Google Drive to read your dataset and save outputs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGRS8TesIz8-"
      },
      "source": [
        "Define the path to the dataset stored on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U3MEfj8myLZ"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/AI_for_security/dataset.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMRmHmUJEyc"
      },
      "source": [
        "Loads the dataset and prints a few samples to inspect labels and text content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QdC4NPWnhC5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "from textwrap import fill\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)  # niente \"...\"\n",
        "for i, row in df.head(5).iterrows():        # cambia 10 come vuoi\n",
        "    print(f\"#{i}  [{row['label']}]\")\n",
        "    print(fill(str(row['text']), width=100)) # va a capo ogni ~100 caratteri\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zotjccx1KJCd"
      },
      "source": [
        "Displays the number of rows and columns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6noJELw5WQrF"
      },
      "outputs": [],
      "source": [
        "rows, cols = df.shape\n",
        "print(f\"NUmber of rows: {rows} \\nNumber of columns: {cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQC2OHt3KNiD"
      },
      "source": [
        "Sets the random seed to ensure reproducibility of experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq4kQ74uUdWS"
      },
      "outputs": [],
      "source": [
        "random_seed = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTtNaN1d29l"
      },
      "source": [
        "#training/test/validation split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9APRd1MKoyU"
      },
      "source": [
        "Splits the dataset into training, validation, and test sets using stratification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC-bc7Fmd67n"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        "\n",
        "X_train_temp, X_test_val_temp, y_train, y_test_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=random_seed,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_val_temp, X_test_temp, y_val, y_test = train_test_split(\n",
        "    X_test_val_temp,\n",
        "    y_test_val,\n",
        "    test_size=0.5,\n",
        "    random_state=random_seed,\n",
        "    stratify=y_test_val\n",
        ")\n",
        "\n",
        "idx_train = X_train_temp.index\n",
        "idx_val = X_val_temp.index\n",
        "idx_test = X_test_temp.index\n",
        "\n",
        "print(f\"--- Suddivisione 70% / 15% / 15% ---\")\n",
        "print(f\"Training Set (Indici):   {len(idx_train)} samples\")\n",
        "print(f\"Validation Set (Indici): {len(idx_val)} samples\")\n",
        "print(f\"Test Set (Indici):       {len(idx_test)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6aF1t_oWqSW"
      },
      "source": [
        "#Data exploration through blacklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhtKAURl7x1a"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFrmYkDeWqSX"
      },
      "source": [
        "## NLTK installation\n",
        "\n",
        "We are going to use the **NLTK** (Nartural Language ToolKit) package, a library for a suite of libraries and programs for natural language processing (NLP) in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6y_Xx5rWqSX"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VVnwxecWqSX"
      },
      "source": [
        "Now we can import the package, to get access to all its functions..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPTFd229WqSX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7NlNwBVWqSX"
      },
      "source": [
        "and we can download the packages to use for text analysis (more on this later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikmMn_vhWqSY"
      },
      "outputs": [],
      "source": [
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJJPX1GjWqSY"
      },
      "source": [
        "## Continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzunynPIWqSY"
      },
      "outputs": [],
      "source": [
        "# Tokenize text into individual words using NLTK\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzqnElExWqSY"
      },
      "outputs": [],
      "source": [
        "punctuations = list(string.punctuation)   # list of punctuation characters\n",
        "punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5rKUrO8WqSY"
      },
      "source": [
        "Define the set of stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioKpxc7JWqSY"
      },
      "outputs": [],
      "source": [
        "# Load the set of English stopwords to remove common non-informative words\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNx7_fE1WqSY"
      },
      "source": [
        "Stopwords are common words (e.g., “the”, “and”, “is”) that usually carry little semantic information and are therefore often removed during text preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCA9Eqz1WqSY"
      },
      "source": [
        "Define a stemmer to be used for preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcDooiV4WqSY"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkfCxsH1WqSY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import islice\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Configuration parameters\n",
        "TEXT_COL = \"text\"          # column containing raw text\n",
        "STOP_LANG = \"english\"      # stopwords language\n",
        "USE_SNOWBALL = False       # choose stemmer type\n",
        "LOWERCASE = True           # lowercase tokens\n",
        "\n",
        "# Safety check\n",
        "assert TEXT_COL in df.columns, f\"La colonna '{TEXT_COL}' non esiste in df!\"\n",
        "\n",
        "# Extract text data indexed by original dataframe indices\n",
        "data = {idx: row for idx, row in df[TEXT_COL].items()}\n",
        "\n",
        "# Tokenize each document\n",
        "preprocessed_data = {k: word_tokenize(v if isinstance(v, str) else str(v)) for k, v in data.items()}\n",
        "\n",
        "# Preview tokenization\n",
        "for key, value in islice(preprocessed_data.items(), 3):\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Prepare punctuation handling\n",
        "punctuations = list(string.punctuation)\n",
        "punct_str = \"\".join(punctuations)\n",
        "\n",
        "# Remove punctuation tokens\n",
        "def strip_and_filter_punct(tokens):\n",
        "    out = []\n",
        "    for tok in tokens:\n",
        "        t = tok.strip(punct_str)\n",
        "        if t and t not in punctuations:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "preprocessed_data = {\n",
        "    key: strip_and_filter_punct(tokens)\n",
        "    for key, tokens in preprocessed_data.items()\n",
        "}\n",
        "\n",
        "# Preview after punctuation removal\n",
        "for key, value in islice(preprocessed_data.items(), 3):\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Load stopwords\n",
        "stop_set = set(stopwords.words(STOP_LANG))\n",
        "\n",
        "# Remove stopwords and optionally lowercase\n",
        "def remove_stopwords(tokens):\n",
        "\n",
        "    if LOWERCASE:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    kept = [t for t in tokens if t not in stop_set]\n",
        "    return kept if len(tokens) > 2 else []\n",
        "\n",
        "preprocessed_data = {\n",
        "    key: remove_stopwords(tokens)\n",
        "    for key, tokens in preprocessed_data.items()\n",
        "}\n",
        "\n",
        "# Preview after stopword removal\n",
        "for key, value in islice(preprocessed_data.items(), 3):\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Initialize stemmer\n",
        "if USE_SNOWBALL:\n",
        "    stemmer = SnowballStemmer(STOP_LANG)\n",
        "else:\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming\n",
        "preprocessed_data = {\n",
        "    key: [stemmer.stem(token) for token in tokens]\n",
        "    for key, tokens in preprocessed_data.items()\n",
        "}\n",
        "\n",
        "# Preview after stemming\n",
        "for key, value in islice(preprocessed_data.items(), 3):\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Store processed tokens back into the dataframe\n",
        "processed_series = pd.Series(preprocessed_data)\n",
        "df[\"tokens_processed\"] = df.index.map(processed_series)\n",
        "\n",
        "# Final check\n",
        "df[[\"text\", \"label\", \"tokens_processed\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDWQEIjBWqSZ"
      },
      "source": [
        "Remove stopwords (additionally we drop lists of tokens that contain less than two elements)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn9U931AWqSZ"
      },
      "outputs": [],
      "source": [
        "STOP_LANG = \"english\"  # language used for stopword removal\n",
        "stop_set = set(stopwords.words(STOP_LANG))  # load stopwords as a set for fast lookup\n",
        "\n",
        "# Remove stopwords from each tokenized document\n",
        "# Documents with very few tokens are replaced with an empty list\n",
        "preprocessed_data = {\n",
        "    file_name: [token for token in tokens if token not in stop_set] if len(tokens) > 2 else []\n",
        "    for file_name, tokens in preprocessed_data.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzOoSbrWqSZ"
      },
      "source": [
        "Finally we apply stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoa6B0LXWqSZ"
      },
      "outputs": [],
      "source": [
        "# Apply stemming to each token to reduce words to their root form\n",
        "preprocessed_data = {\n",
        "    file_name: [stemmer.stem(token) for token in tokens]\n",
        "    for file_name, tokens in preprocessed_data.items()\n",
        "}\n",
        "\n",
        "for key, value in islice(preprocessed_data.items(), 3):\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYrbqnEsWqSZ"
      },
      "source": [
        "number of unique stems found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzX7vPOtWqSZ"
      },
      "outputs": [],
      "source": [
        "# Collect all unique stems appearing in the dataset\n",
        "all_stems = set()\n",
        "for tokens in preprocessed_data.values():\n",
        "    all_stems.update(tokens)\n",
        "\n",
        "# Print the total number of unique stems found\n",
        "print(f\"\\nTotal number of unique stems found: {len(all_stems)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l9ikIM3WqSZ"
      },
      "source": [
        "training/test split (temporary, just for the blacklist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzy5byWeWqSZ"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_temp\n",
        "X_test = X_val_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUf0XbLBWqSa"
      },
      "outputs": [],
      "source": [
        "# Blacklist-based model (training + validation)\n",
        "from itertools import islice\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1) Binary label mapping\n",
        "def to_binary_label(y):\n",
        "    \"\"\"\n",
        "    Map labels to binary format:\n",
        "    jailbreak -> 1, benign -> 0\n",
        "    \"\"\"\n",
        "    if isinstance(y, str):\n",
        "        y = y.strip().lower()\n",
        "        if y == \"jailbreak\":\n",
        "            return 1\n",
        "        elif y == \"benign\":\n",
        "            return 0\n",
        "    return int(y)\n",
        "\n",
        "# Create label dictionary indexed by dataframe index\n",
        "labels = df[\"label\"].map(to_binary_label).to_dict()\n",
        "\n",
        "# 2) Build blacklist from training set\n",
        "jailbreak_words = set()\n",
        "benign_words = set()\n",
        "\n",
        "# Collect stems from training samples only\n",
        "for idx in tqdm(X_train_temp.index, desc=\"Building blacklist (train set)\"):\n",
        "    stems = preprocessed_data.get(idx, [])\n",
        "    if stems:\n",
        "        if labels[idx] == 1:\n",
        "            jailbreak_words.update(stems)\n",
        "        else:\n",
        "            benign_words.update(stems)\n",
        "\n",
        "# Words appearing only in jailbreak samples\n",
        "blacklist = jailbreak_words - benign_words\n",
        "\n",
        "# 3) Quick blacklist inspection\n",
        "print(\"Sample blacklist stems:\")\n",
        "for token in islice(blacklist, 5):\n",
        "    print(token)\n",
        "\n",
        "print(f\"\\nBlacklist size: {len(blacklist)}\")\n",
        "\n",
        "# 4) Keep only real English words\n",
        "from nltk.corpus import words as nltk_words\n",
        "\n",
        "word_set = set(nltk_words.words())\n",
        "word_blacklist = word_set.intersection(blacklist)\n",
        "\n",
        "print(\"Real-word blacklist size:\", len(word_blacklist))\n",
        "\n",
        "# 5) Validation set evaluation\n",
        "tp = fp = tn = fn = 0\n",
        "y_true_val = []\n",
        "y_pred_val = []\n",
        "\n",
        "for idx in tqdm(X_val_temp.index, desc=\"Evaluating on validation set\"):\n",
        "    true_label = labels[idx]\n",
        "    stems = preprocessed_data.get(idx, [])\n",
        "\n",
        "    # Predict jailbreak if any blacklist word is present\n",
        "    pred_label = 1 if stems and set(stems) & blacklist else 0\n",
        "\n",
        "    y_true_val.append(true_label)\n",
        "    y_pred_val.append(pred_label)\n",
        "\n",
        "    if pred_label == 1 and true_label == 1:\n",
        "        tp += 1\n",
        "    elif pred_label == 1 and true_label == 0:\n",
        "        fp += 1\n",
        "    elif pred_label == 0 and true_label == 1:\n",
        "        fn += 1\n",
        "    else:\n",
        "        tn += 1\n",
        "\n",
        "# 6) Metrics and report\n",
        "print(\"\\nConfusion Matrix (validation):\")\n",
        "print(f\"| TP {tp:5d} | FN {fn:5d} |\")\n",
        "print(f\"| FP {fp:5d} | TN {tn:5d} |\")\n",
        "print(\"\\nClassification Report (validation):\")\n",
        "print(classification_report(y_true_val, y_pred_val,\n",
        "                            target_names=[\"benign\", \"jailbreak\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjezC_cLbq5U"
      },
      "source": [
        "The blacklist-based classifier achieves moderate performance on the validation set (accuracy ≈ 0.68). It shows a reasonable recall for the jailbreak class (0.63), indicating that a significant portion of malicious prompts is detected. However, precision for jailbreak remains lower (0.59), reflecting a non-negligible number of false positives. Overall, the results confirm that a simple lexical blacklist captures part of the jailbreak signal, but lacks sufficient discriminative power compared to learning-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaHpx3aBWqSa"
      },
      "source": [
        "comparison with random classifier (taking into account the jailbreak frequency):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W47Vo2D_WqSa"
      },
      "outputs": [],
      "source": [
        "# BINARY DATA PREPARATION (0/1)\n",
        "\n",
        "# 1. True labels for the Validation Set (0/1)\n",
        "# We use y_val (string Series) obtained from the 70/15/15 split\n",
        "# y_val is already aligned in size with X_val_temp\n",
        "y_val_binary = y_val.map(to_binary_label).values\n",
        "\n",
        "# 2. True labels for the Training Set (0/1)\n",
        "# We use y_train (string Series) obtained from the 70/15/15 split\n",
        "y_train_binary = y_train.map(to_binary_label).values\n",
        "\n",
        "# RANDOM CLASSIFIER (STRATIFIED) - BASELINE\n",
        "\n",
        "# Positive class prevalence (jailbreak) in the training split\n",
        "p_pos_train = y_train_binary.mean()\n",
        "print(f\"\\nPositive class prevalence in training: {p_pos_train:.5f}\")\n",
        "\n",
        "# RANDOM GENERATOR with fixed seed (for reproducibility)\n",
        "rng = np.random.default_rng(random_seed)\n",
        "\n",
        "# Generate STRATIFIED random predictions\n",
        "# (predict y=1 with probability equal to the training prevalence)\n",
        "# The prediction vector must match the Validation Set size\n",
        "len_val_set = len(y_val_binary)\n",
        "y_pred_rand_strat = (rng.random(len_val_set) < p_pos_train).astype(int)\n",
        "\n",
        "# EVALUATION FUNCTION\n",
        "def evaluate_predictions(y_true, y_pred, title):\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "\n",
        "    acc = (tp + tn) / len(y_true)\n",
        "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "\n",
        "    # F1 SCORE\n",
        "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
        "\n",
        "    # SPECIFICITY (TN / (TN + FP))\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(\"Counts:\")\n",
        "    print(f\"TP: {tp}\")\n",
        "    print(f\"FN: {fn}\")\n",
        "    print(f\"FP: {fp}\")\n",
        "    print(f\"TN: {tn}\")\n",
        "\n",
        "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
        "    print(f\"| TP {tp:5d} | FN {fn:5d} |\")\n",
        "    print(f\"| FP {fp:5d} | TN {tn:5d} |\")\n",
        "\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"Accuracy:    {acc:.5f}\")\n",
        "    print(f\"Precision:   {prec:.5f}\")\n",
        "    print(f\"Recall:      {rec:.5f}\")\n",
        "    print(f\"F1 score:    {f1:.5f}\")\n",
        "    print(f\"Specificity: {spec:.5f}\")\n",
        "\n",
        "# EVALUATION ON THE VALIDATION SET\n",
        "evaluate_predictions(\n",
        "    y_val_binary,        #Binary labels for the Validation Set\n",
        "    y_pred_rand_strat,\n",
        "    \"Random classifier (stratified, evaluated on Validation Set)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew6Yl-yncgOJ"
      },
      "source": [
        "This stratified random classifier reflects the class distribution observed in the training set and serves as a baseline.\n",
        "As expected, performance is close to chance level: accuracy is around 51% and both precision and recall for the jailbreak class are below 0.4.\n",
        "These results confirm that meaningful structure must be learned from the data to outperform a naive random strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAn54uweIKF"
      },
      "source": [
        "The Blacklist is noticebly better than the Random Classifier, which makes us think that there is some kind of correlation between the meaning of the words in prompts and the jailbreak/benign label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06BsUzrkWqSa"
      },
      "outputs": [],
      "source": [
        "# Initialize placeholders for train/test splits (will be assigned later)\n",
        "X_train, X_test, y_train_temporary = None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE3qsy_SRZ9L"
      },
      "source": [
        "#Data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khdI9U_8RZ9L"
      },
      "source": [
        "In this data visualization section we explore the dataset before training any models, to better understand its structure and potential patterns.\n",
        "We first look at the class distribution (benign vs jailbreak) to check whether the dataset is balanced or imbalanced. Then we examine the prompt lengths across classes to see if jailbreak prompts tend to be longer or differently structured than benign ones.\n",
        "\n",
        "Next, we analyze the most frequent words in each class (after tokenization, stopword removal, and stemming) to identify which terms are particularly characteristic of jailbreaks compared to benign prompts. Finally, we inspect the presence of blacklist words (e.g., ignore, previous, override, forget, write, create) and compare how often they appear in each class, to assess whether they can serve as useful signals for detecting jailbreak attempts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf0TrJqBRZ9L"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='label', data=df, palette='YlOrBr')\n",
        "plt.title('Distribuzione classi: benign vs jailbreak')\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.show()\n",
        "\n",
        "#This plot shows how many samples belong to each class,\n",
        "#comparing the count of benign prompts versus jailbreak prompts.\n",
        "#It lets you quickly see whether the dataset is balanced or if one class is more frequent than the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZFoaYUHRZ9M"
      },
      "outputs": [],
      "source": [
        "df['length'] = df['text'].str.split().apply(len)\n",
        "sns.boxplot(x='label', y='length', data=df, palette='YlOrBr')\n",
        "plt.title('Distribution of length of prompts')\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "\n",
        "#This boxplot compares the prompt length (in words) between the two classes.\n",
        "#It shows the median, spread, and outliers of lengths for benign vs jailbreak prompts,\n",
        "#letting you see whether jailbreaks tend to be longer or shorter than benign prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0oglfZmg2Ws"
      },
      "source": [
        "Normalizing the data might be a good idea!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1cJeXyZRZ9M"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return [stemmer.stem(w) for w in words if w not in stop]\n",
        "\n",
        "df['tokens'] = df['text'].apply(preprocess)\n",
        "\n",
        "# parole più frequenti nei jailbreak\n",
        "jail_tokens = [w for tokens in df[df.label=='jailbreak'].tokens for w in tokens]\n",
        "benign_tokens = [w for tokens in df[df.label=='benign'].tokens for w in tokens]\n",
        "\n",
        "jail_freq = Counter(jail_tokens).most_common(20)\n",
        "benign_freq = Counter(benign_tokens).most_common(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxcZ4FpvRZ9M"
      },
      "outputs": [],
      "source": [
        "j_df = pd.DataFrame(jail_freq, columns=['word','count'])\n",
        "b_df = pd.DataFrame(benign_freq, columns=['word','count'])\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
        "sns.barplot(y='word', x='count', data=j_df, ax=axes[0], palette='YlOrBr')\n",
        "sns.barplot(y='word', x='count', data=b_df, ax=axes[1], palette='YlOrBr')\n",
        "axes[0].set_title('Top parole jailbreak')\n",
        "axes[1].set_title('Top parole benign')\n",
        "for ax in axes: ax.set_facecolor('#fff8dc')\n",
        "plt.show()\n",
        "\n",
        "#This figure shows two side-by-side barplots with the most frequent stems for each class:\n",
        "#on the left, the top words in jailbreak prompts, and on the right, the top words in benign prompts.\n",
        "#It highlights which terms are most common in each class and allows you to visually compare their lexical profiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quVxC3uFRZ9M"
      },
      "outputs": [],
      "source": [
        "blacklist = ['ignore', 'ethical', 'data', 'information', 'write', 'create']\n",
        "df['blacklisted'] = df['text'].str.lower().apply(lambda t: any(word in t for word in blacklist))\n",
        "\n",
        "sns.countplot(x='label', hue='blacklisted', data=df, palette='YlOrBr')\n",
        "plt.title('Blacklist words per class')\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.show()\n",
        "\n",
        "#This plot shows, for each class (benign vs jailbreak), how many prompts contain at least one word from the blacklist and how many do not.\n",
        "#It lets you see whether blacklist terms like “ignore”, “ethical”, “data”, “information”, “write”,\n",
        "#or “create” appear more often in jailbreak prompts than in benign ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8FyTJ19RZ9M"
      },
      "outputs": [],
      "source": [
        "blacklist = ['ignore', 'previous', 'override', 'forget', 'write', 'create']\n",
        "df['blacklisted'] = df['text'].str.lower().apply(lambda t: any(word in t for word in blacklist))\n",
        "\n",
        "sns.countplot(x='label', hue='blacklisted', data=df, palette='YlOrBr')\n",
        "plt.title('Blacklist words per class')\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.show()\n",
        "\n",
        "#This plot shows, for each class (benign vs jailbreak), how many prompts contain at least one of the updated blacklist\n",
        "#words (“ignore”, “previous”, “override”, “forget”, “write”, “create”) and how many do not.\n",
        "#It lets you check whether these jailbreak-like trigger words appear more frequently in jailbreak prompts than in benign ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RXz-ztvWcY1"
      },
      "source": [
        "# Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcXPWJO_WcY1"
      },
      "source": [
        "In order to train Word2Vec on the data we first need to convert it to the right format.\n",
        "- For training Word2Vec, it is usual to **separate data into individual sentences** and then tokenize those sentences separately\n",
        "- So let's use regular expressions to remove the end-of-line characters from each document and then split the documents into sentences using a regular expression that looks for question marks, exclamation marks, and periods, followed by a space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NkXjSZXUWcY1"
      },
      "outputs": [],
      "source": [
        "pip install gensim pandas nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgRO12_EWcY1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "\n",
        "# 1. Load dataset\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "\n",
        "# 2. Clean text: remove line breaks\n",
        "texts = [re.sub(r\"\\s+\", \" \", t).strip() for t in texts]\n",
        "\n",
        "# 3. Split into sentences\n",
        "def split_into_sentences(text):\n",
        "    return re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
        "\n",
        "sentences = []\n",
        "for t in texts:\n",
        "    sentences.extend(split_into_sentences(t))\n",
        "\n",
        "# 4. Tokenize:\n",
        "#    re.sub('\\W', ' ', sentence).lower().split()\n",
        "tokenized_sentences = [\n",
        "    re.sub(r\"\\W\", \" \", sentence).lower().split()\n",
        "    for sentence in sentences\n",
        "]\n",
        "\n",
        "# 5. Remove sentences with only 1 word\n",
        "tokenized_sentences = [\n",
        "    sentence for sentence in tokenized_sentences\n",
        "    if len(sentence) > 1\n",
        "]\n",
        "\n",
        "# Visual check\n",
        "for s in tokenized_sentences[:10]:\n",
        "    print(s)\n",
        "\n",
        "# 6. Train Word2Vec\n",
        "model = Word2Vec(\n",
        "    tokenized_sentences,\n",
        "    vector_size=30,\n",
        "    min_count=5,\n",
        "    window=10\n",
        ")\n",
        "\n",
        "# 7. Vocabulary size\n",
        "print(\"Vocabulary size:\", len(model.wv))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2c2UwT1WcY2"
      },
      "source": [
        "## Visualising the embedding vectors using t-SNE\n",
        "\n",
        "We'll now visualise some of the word vectors in a 3 dimensional space using t-SNE.\n",
        "\n",
        "The vocabulary of word vectors is quite large (around 25,000). Giving them all to t-SNE will cause it to take far too long to converge.\n",
        "- So let's first choose a random subset of 500 terms to show:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tqRuuMwWcY2"
      },
      "outputs": [],
      "source": [
        "# Randomly sample 500 words from the Word2Vec vocabulary to inspect learned tokens\n",
        "import random\n",
        "\n",
        "# random.seed(50)  # <-- fixed seed\n",
        "\n",
        "sample = random.sample(list(model.wv.key_to_index), 500)\n",
        "print(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ktSlBOWcY2"
      },
      "source": [
        "Now we'll get the word vectors for the sampled terms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y72PtAPUWcY2"
      },
      "outputs": [],
      "source": [
        "word_vectors = model.wv[sample]\n",
        "word_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0lsuaB2NEci"
      },
      "source": [
        "Displays the dimensionality of the Word2Vec embedding matrix (number of words x embedding size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtoktjuFWcY2"
      },
      "outputs": [],
      "source": [
        "word_vectors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsMQ2jPhWcY2"
      },
      "source": [
        "We provide the word vectors to t-SNE to fit the model and reduce the data to 3 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6Ue1Oy_WcY2"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embedding = tsne.fit_transform(word_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jerrTUMuWcY3"
      },
      "source": [
        "Now transform the data into 3 columns (for x, y, and z):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_K0mJvkWcY3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x, y, z = np.transpose(tsne_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9mBNPDBWcY3"
      },
      "source": [
        "And generate the 3d plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuXaCkDhWcY4"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_3d(x=x, y=y, z=z)\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdHLff6BWcY4"
      },
      "source": [
        "Well that's a not a particularly interesting 3d plot!\n",
        "- How about we label some of the points on the graph to see what words they correspond to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc_yypHEWcY4"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\n",
        "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwBqG4UeWcY4"
      },
      "source": [
        "We perform a simple vector arithmetic operation in the embedding space to explore semantic relationships.\n",
        "Starting from a word associated with jailbreak prompts, we add and subtract unrelated words to observe whether the resulting vector remains close to other jailbreak-related terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi06FJiAWcY4"
      },
      "outputs": [],
      "source": [
        "v_answer = model.wv['answer']\n",
        "v_use = model.wv['use']\n",
        "v_character = model.wv['character']\n",
        "\n",
        "vec = v_answer + (v_use - v_character)\n",
        "\n",
        "model.wv.similar_by_vector(vec, topn=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PMWFwtMOmCo"
      },
      "source": [
        "The resulting nearest neighbors suggest that the vector arithmetic shifted the original word toward a more instruction- and prompt-related semantic space. Terms like prompt, questions, and violates indicate that the embedding captures contextual associations linked to structured or potentially sensitive user inputs, rather than a strictly semantic notion of “jailbreak.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "risqdCm5WcY4"
      },
      "source": [
        "we got \"questions\", which was a jailbreak word accoridng to the other embbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsRjnJgBWcY4"
      },
      "source": [
        "Let's extend the random set of terms with a set of colours to see if they cluster:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrnsYK3uWcY5"
      },
      "source": [
        "## Loading Pre-trained Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4z3Hm7cWcY5"
      },
      "outputs": [],
      "source": [
        "#Load pre-trained GloVe embeddings trained on Twitter and Wikipedia corpora.\n",
        "import gensim.downloader as api\n",
        "\n",
        "model_twitter = api.load(\"glove-twitter-50\")\n",
        "model_wiki = api.load(\"glove-wiki-gigaword-50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlp34BwAWcY5"
      },
      "source": [
        "How big are their vocabularies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55yqWf_1WcY5"
      },
      "outputs": [],
      "source": [
        "print(f\"Vocabulary size twitter model:   {len(model_twitter)}\")\n",
        "print(f\"Vocabulary size wikipedia model: {len(model_wiki)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5LwndGaWcY5"
      },
      "source": [
        "Woah, they are big vocabularies!\n",
        "- The twitter one has over a million tokens!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrjXyK3aWcY5"
      },
      "outputs": [],
      "source": [
        "term = 'jailbreak'    # target word\n",
        "print(f'Twitter embedding, most similar words to: {term}')\n",
        "model_twitter.most_similar(term)    # inspect semantic neighbors in GloVe-Twitter space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO9p_HjdRmtZ"
      },
      "source": [
        "The Twitter embedding associates “jailbreak” mainly with mobile devices, operating systems, and firmware-related terms, reflecting its common usage in a technological and consumer context rather than in a security-oriented sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PT3p-AeWcY6"
      },
      "source": [
        "And in the Wikipedia embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg-dkzdwWcY7"
      },
      "outputs": [],
      "source": [
        "print(f'Wikipedia embedding, most similar words to: {term}')\n",
        "model_wiki.most_similar(term)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC1toGLDRwPi"
      },
      "source": [
        "The Wikipedia embedding captures a more abstract and heterogeneous notion of “jailbreak”, associating it with violent, military, or disruptive events rather than with the technical meaning observed in the Twitter embedding. This highlights how the semantic representation of the same term strongly depends on the training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jn-V8DbWcY7"
      },
      "source": [
        "Which embedding is better?\n",
        "\n",
        "Try some other words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7nHikjBWcY7"
      },
      "outputs": [],
      "source": [
        "term = 'benign'\n",
        "print(f'Twitter embedding:   {model_twitter.most_similar(term)}')\n",
        "print(f'Wikipedia embedding: {model_wiki.most_similar(term)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU6rm8X6SD89"
      },
      "source": [
        "In both embeddings, “benign” is strongly linked to medical contexts, highlighting a semantic mismatch with its use as a class label in this task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppDU6lVdWcY8"
      },
      "source": [
        "Embedding spaces have interesting geometric properties, where translation between different word vectors caries semantic meaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRCu8ZuIWcY8"
      },
      "outputs": [],
      "source": [
        "# Vector arithmetic: combine semantic relations (answer + use − character)\n",
        "vec = model_wiki.get_vector('answer') + (model_wiki.get_vector('use') - model_wiki.get_vector('character'))\n",
        "vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcjQa_bIWcY8"
      },
      "source": [
        "- Then look for the most similar vectors to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_2lzWgxWcY8"
      },
      "outputs": [],
      "source": [
        "#The vector arithmetic moves the semantic meaning of “answer” toward prompt- and instruction-related concepts.\n",
        "model_wiki.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3CePwyMWcY8"
      },
      "source": [
        "Try it again but with the Twitter based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKeb_jDtWcY8"
      },
      "outputs": [],
      "source": [
        "model = model_twitter\n",
        "vec = model.get_vector('answer') + (model.get_vector('use') - model.get_vector('character'))\n",
        "model.similar_by_vector(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T_EK2OHTFrC"
      },
      "source": [
        "The resulting vector is mainly associated with generic communication and interaction terms (e.g., ask, tell, respond), rather than jailbreak-related concepts. This suggests that, in the Twitter embedding space, the vector arithmetic captures semantic usage patterns of language rather than security- or policy-specific meanings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUISfhC-yHHX"
      },
      "source": [
        "# data structure to save all models for further use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xKLyG0Az9ME"
      },
      "source": [
        "This section defines a central model registry used to store and organize all trained classifiers and their variants. The `register_model` function saves each model in a nested dictionary structure keyed by model name, embedding type, and variant name, along with its trained instance and parameters. It also prints a message indicating whether a new model has been added or an existing one has been overwritten, making it easier to manage and reuse different models across experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w-73TIGyOU6"
      },
      "outputs": [],
      "source": [
        "models_registry = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97oJjtyghSAD"
      },
      "source": [
        "an example of the data structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkAEewB3yvGG"
      },
      "outputs": [],
      "source": [
        "# models_registry = {\n",
        "#     \"SVC\": {\n",
        "#         \"w2v\": {\n",
        "#             \"grid_C_0.1_5.0\": {\n",
        "#                 \"model\": <trained SVC>,\n",
        "#                 \"params\": {...},\n",
        "#                 \"embedding\": \"w2v\",\n",
        "#             },\n",
        "#             \"default\": {\n",
        "#                 \"model\": <trained SVC>,\n",
        "#                 \"params\": {...},\n",
        "#                 \"embedding\": \"w2v\",\n",
        "#             }\n",
        "#         },\n",
        "#         \"glove_twitter\": { ... },\n",
        "#         ...\n",
        "#     },\n",
        "#     \"LogisticRegression\": { ... },\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHPy1XZOy4xm"
      },
      "outputs": [],
      "source": [
        "models_registry = {}\n",
        "\n",
        "def register_model(\n",
        "    registry,\n",
        "    model_name: str,\n",
        "    embedding_name: str,\n",
        "    variant_name: str,\n",
        "    model,\n",
        "    params: dict | None = None,\n",
        "):\n",
        "\n",
        "    # Crea i livelli se non esistono\n",
        "    if model_name not in registry:\n",
        "        registry[model_name] = {}\n",
        "\n",
        "    if embedding_name not in registry[model_name]:\n",
        "        registry[model_name][embedding_name] = {}\n",
        "\n",
        "    # Messaggio se il variant esiste già\n",
        "    if variant_name in registry[model_name][embedding_name]:\n",
        "        print(f\"⚠️  ATTENZIONE: stai sovrascrivendo il modello \"\n",
        "              f\"[{model_name}] embedding [{embedding_name}] variante [{variant_name}]\")\n",
        "    else:\n",
        "        print(f\"✅ Nuovo modello registrato: \"\n",
        "              f\"[{model_name}] embedding [{embedding_name}] variante [{variant_name}]\")\n",
        "\n",
        "    # Salvataggio\n",
        "    registry[model_name][embedding_name][variant_name] = {\n",
        "        \"model\": model,\n",
        "        \"params\": params,\n",
        "        \"embedding\": embedding_name,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otQCXzhEnnlr"
      },
      "source": [
        "#text vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wihWUET1FR1"
      },
      "source": [
        "This section builds and manages all the text vectorizations used in the experiments. It first creates a bag-of-words representation from stemmed tokens with a CountVectorizer, then loads or computes several dense embeddings: Word2Vec, two pre-trained GloVe variants, FastText, and SBERT.  Finally, all embedding matrices are normalized using statistics from the training set, so that downstream models can compare and use these different representations in a consistent way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGZaKyOYhb5S"
      },
      "source": [
        "To speed up the rest of the notebook, we also save the vectorizations on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfgXOCbHn2HT"
      },
      "outputs": [],
      "source": [
        "embedding_names = [\"w2v\", \"glove_twitter\", \"glove_wiki\", \"fasttext\", \"sbert\"]\n",
        "all_representations = [\"bow\"] + embedding_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d_OBYRUnnlr"
      },
      "source": [
        "##with the stems of the words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kMdBFm5hpUy"
      },
      "source": [
        "we use \"preprocessed_data\" from the Blacklist chapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbfvHZ8Innlr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X_train_stems = [preprocessed_data.get(idx, []) for idx in idx_train]\n",
        "X_val_stems = [preprocessed_data.get(idx, []) for idx in idx_val]\n",
        "X_test_stems = [preprocessed_data.get(idx, []) for idx in idx_test]\n",
        "\n",
        "X_train_joined = [\" \".join(stems) for stems in X_train_stems]\n",
        "X_val_joined = [\" \".join(stems) for stems in X_val_stems]\n",
        "X_test_joined = [\" \".join(stems) for stems in X_test_stems]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train_vector = vectorizer.fit_transform(X_train_joined)\n",
        "X_val_vector = vectorizer.transform(X_val_joined)\n",
        "X_test_vector = vectorizer.transform(X_test_joined)\n",
        "\n",
        "print(\"\\n--- Vettorizzazione Finale ---\")\n",
        "print(f\"Training Matrix Shape:     {X_train_vector.shape}\")\n",
        "print(f\"Validation Matrix Shape:   {X_val_vector.shape}\")\n",
        "print(f\"Test Matrix Shape:         {X_test_vector.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAMez6HhiQtJ"
      },
      "outputs": [],
      "source": [
        "#load the other embeddings if already done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpVpIKBZiVGo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/AI_for_security\"\n",
        "FILENAME = \"other_embeddings_data.pkl\"\n",
        "LOAD_PATH = os.path.join(DRIVE_BASE_PATH, FILENAME)\n",
        "\n",
        "# 1. Initialize control variable\n",
        "loading_ok = False\n",
        "\n",
        "try:\n",
        "    with open(LOAD_PATH, 'rb') as f:\n",
        "        loaded_embeddings = pickle.load(f)\n",
        "    print(\"✅ Dizionario caricato con successo!\")\n",
        "\n",
        "    other_embeddings = loaded_embeddings\n",
        "\n",
        "    # 2. finished loading\n",
        "    loading_ok = True\n",
        "\n",
        "except FileNotFoundError:\n",
        "    # Initialize empty dictionary if there is no file\n",
        "    other_embeddings = {\n",
        "        \"train\": {},\n",
        "        \"val\": {},\n",
        "        \"test\": {}\n",
        "    }\n",
        "    print(f\"❌ File non trovato in: {LOAD_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Errore durante il caricamento: {e}\")\n",
        "\n",
        "print(f\"Stato caricamento: {loading_ok}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kax8zWl9H51C"
      },
      "source": [
        "## w2v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZCShpwSebkf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_en = set(stopwords.words(\"english\"))\n",
        "\n",
        "def text_to_vector(text, w2v):\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text)).strip().lower()\n",
        "\n",
        "    sentences = split_into_sentences(text)\n",
        "\n",
        "    tokens = []\n",
        "    for sent in sentences:\n",
        "        # only keep characters\n",
        "        sent = re.sub(r\"[^a-zA-Z]\", \" \", sent)\n",
        "        tokens.extend(sent.split())\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [t for t in tokens if t not in stopwords_en]\n",
        "\n",
        "\n",
        "    # Generate the average vector\n",
        "    vecs = [w2v[w] for w in tokens if w in w2v.key_to_index]\n",
        "\n",
        "    if not vecs:\n",
        "        return np.zeros(w2v.vector_size)\n",
        "\n",
        "    return np.mean(vecs, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr0PGa5RICJh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "if not loading_ok:\n",
        "    print(\"⚠️ Dati non trovati. Avvio il calcolo di Word2Vec...\")\n",
        "\n",
        "    # Feature matrixes based on Word2Vec\n",
        "    X_train_w2v = np.vstack([\n",
        "        text_to_vector(df.loc[i, \"text\"], model)\n",
        "        for i in idx_train\n",
        "    ])\n",
        "\n",
        "    X_val_w2v = np.vstack([\n",
        "        text_to_vector(df.loc[i, \"text\"], model)\n",
        "        for i in idx_val\n",
        "    ])\n",
        "\n",
        "    X_test_w2v = np.vstack([\n",
        "        text_to_vector(df.loc[i, \"text\"], model)\n",
        "        for i in idx_test\n",
        "    ])\n",
        "\n",
        "    other_embeddings[\"train\"][\"w2v\"] = X_train_w2v\n",
        "    other_embeddings[\"val\"][\"w2v\"]   = X_val_w2v\n",
        "    other_embeddings[\"test\"][\"w2v\"]  = X_test_w2v\n",
        "\n",
        "    print(\"Calcolo completato.\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ Dati già caricati dal Drive. Recupero le variabili dal dizionario...\")\n",
        "    X_train_w2v = other_embeddings[\"train\"][\"w2v\"]\n",
        "    X_val_w2v   = other_embeddings[\"val\"][\"w2v\"]\n",
        "    X_test_w2v  = other_embeddings[\"test\"][\"w2v\"]\n",
        "\n",
        "print(f\"\\nDimensioni finali W2V:\")\n",
        "print(f\"Train: {X_train_w2v.shape}\")\n",
        "print(f\"Val:   {X_val_w2v.shape}\")\n",
        "print(f\"Test:  {X_test_w2v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bokGjQRVJttB"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "\n",
        "  other_embeddings[\"train\"][\"w2v\"] = X_train_w2v\n",
        "  other_embeddings[\"val\"][\"w2v\"]   = X_val_w2v\n",
        "  other_embeddings[\"test\"][\"w2v\"]  = X_test_w2v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIVGsqILsau"
      },
      "source": [
        "## Glove pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waESkyiULu8G"
      },
      "outputs": [],
      "source": [
        "## GloVe pre-trained\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Function to compute the average embedding vector of a document/prompt\n",
        "def get_avg_embedding(stems, embedding_model, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Compute the average embedding of a document.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stems : list of pre-processed tokens/stems for a prompt.\n",
        "    embedding_model : loaded GloVe model (e.g., model_twitter or model_wiki).\n",
        "    embedding_dim : dimensionality of the embedding vectors (e.g., 50).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.array of shape (embedding_dim,)\n",
        "        The mean vector over all known word embeddings in the prompt.\n",
        "        If no token is found in the vocabulary, a zero vector is returned.\n",
        "    \"\"\"\n",
        "\n",
        "    # List to collect vectors of words that are found in the model vocabulary\n",
        "    word_vectors = []\n",
        "\n",
        "    for word in stems:\n",
        "        # Check if the word is present in the embedding model vocabulary\n",
        "        if word in embedding_model:\n",
        "            word_vectors.append(embedding_model[word])\n",
        "\n",
        "    if word_vectors:\n",
        "        # Compute the arithmetic mean of all the word vectors\n",
        "        # This is a dense representation of the whole prompt\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        # If no words are found (all tokens are OOV), return a zero vector\n",
        "        return np.zeros(embedding_dim)\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqD00THuMDYe"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "    print(\"⚠️ Data not found. Computing GloVe Twitter embeddings...\")\n",
        "\n",
        "    # Choose the embedding model (e.g., GloVe Twitter 50D)\n",
        "    embedding_model = model_twitter\n",
        "    EMBEDDING_DIM = 50\n",
        "\n",
        "    # 1. Training set embeddings\n",
        "    X_train_glove_t = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_train_stems, desc=\"GloVe Twitter: Training Set\")\n",
        "    ])\n",
        "\n",
        "    # 2. Validation set embeddings\n",
        "    X_val_glove_t = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_val_stems, desc=\"GloVe Twitter: Validation Set\")\n",
        "    ])\n",
        "\n",
        "    # 3. Test set embeddings\n",
        "    X_test_glove_t = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_test_stems, desc=\"GloVe Twitter: Test Set\")\n",
        "    ])\n",
        "\n",
        "    # Store results in the dictionary (important for later saving to Drive)\n",
        "    other_embeddings[\"train\"][\"glove_twitter\"] = X_train_glove_t\n",
        "    other_embeddings[\"val\"][\"glove_twitter\"]   = X_val_glove_t\n",
        "    other_embeddings[\"test\"][\"glove_twitter\"]  = X_test_glove_t\n",
        "\n",
        "    print(\"GloVe Twitter computation completed.\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ GloVe Twitter data recovered from dictionary.\")\n",
        "\n",
        "    # Recover variables from the loaded dictionary\n",
        "    # (If you skip this, X_train_glove_t / X_val_glove_t / X_test_glove_t will not exist)\n",
        "    X_train_glove_t = other_embeddings[\"train\"][\"glove_twitter\"]\n",
        "    X_val_glove_t   = other_embeddings[\"val\"][\"glove_twitter\"]\n",
        "    X_test_glove_t  = other_embeddings[\"test\"][\"glove_twitter\"]\n",
        "\n",
        "# Final shape check\n",
        "print(f\"\\n--- GloVe Twitter 50D Embeddings ---\")\n",
        "print(f\"Training Emb Shape:   {X_train_glove_t.shape}\")\n",
        "print(f\"Validation Emb Shape: {X_val_glove_t.shape}\")\n",
        "print(f\"Test Emb Shape:       {X_test_glove_t.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp6Ck4rIOdW7"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "\n",
        "  other_embeddings[\"train\"][\"glove_twitter\"] = X_train_glove_t\n",
        "  other_embeddings[\"val\"][\"glove_twitter\"]   = X_val_glove_t\n",
        "  other_embeddings[\"test\"][\"glove_twitter\"]  = X_test_glove_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s65kQvfvMwWH"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "    print(\"⚠️ Data not found. Computing GloVe Wikipedia embeddings...\")\n",
        "\n",
        "    # Choose the embedding model (GloVe Wikipedia 50D)\n",
        "    embedding_model = model_wiki\n",
        "    EMBEDDING_DIM = 50\n",
        "\n",
        "    # 1. Training set embeddings\n",
        "    X_train_glove_w = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_train_stems, desc=\"GloVe Wiki: Training Set\")\n",
        "    ])\n",
        "\n",
        "    # 2. Validation set embeddings\n",
        "    X_val_glove_w = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_val_stems, desc=\"GloVe Wiki: Validation Set\")\n",
        "    ])\n",
        "\n",
        "    # 3. Test set embeddings\n",
        "    X_test_glove_w = np.vstack([\n",
        "        get_avg_embedding(stems, embedding_model, EMBEDDING_DIM)\n",
        "        for stems in tqdm(X_test_stems, desc=\"GloVe Wiki: Test Set\")\n",
        "    ])\n",
        "\n",
        "    # Store results in the dictionary\n",
        "    other_embeddings[\"train\"][\"glove_wiki\"] = X_train_glove_w\n",
        "    other_embeddings[\"val\"][\"glove_wiki\"]   = X_val_glove_w\n",
        "    other_embeddings[\"test\"][\"glove_wiki\"]  = X_test_glove_w\n",
        "\n",
        "    print(\"GloVe Wikipedia computation completed.\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ GloVe Wikipedia data recovered from dictionary.\")\n",
        "\n",
        "    # Recover variables from the loaded dictionary\n",
        "    X_train_glove_w = other_embeddings[\"train\"][\"glove_wiki\"]\n",
        "    X_val_glove_w   = other_embeddings[\"val\"][\"glove_wiki\"]\n",
        "    X_test_glove_w  = other_embeddings[\"test\"][\"glove_wiki\"]\n",
        "\n",
        "# Final shape check\n",
        "print(f\"\\n--- GloVe Wikipedia 50D Embeddings ---\")\n",
        "print(f\"Training Emb Shape:   {X_train_glove_w.shape}\")\n",
        "print(f\"Validation Emb Shape: {X_val_glove_w.shape}\")\n",
        "print(f\"Test Emb Shape:       {X_test_glove_w.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxmfvKbOU00"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "\n",
        "  other_embeddings[\"train\"][\"glove_wiki\"] = X_train_glove_w\n",
        "  other_embeddings[\"val\"][\"glove_wiki\"]   = X_val_glove_w\n",
        "  other_embeddings[\"test\"][\"glove_wiki\"]  = X_test_glove_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joSeSd5vS4GO"
      },
      "source": [
        "## FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ait3SMe7TFh2"
      },
      "source": [
        "### pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP2MelsPTlK9"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xpE-myrTp-G"
      },
      "outputs": [],
      "source": [
        "# Download and overwrite if the file exists (-O specifies the output filename)\n",
        "!wget -O cc.en.300.bin.gz http://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "\n",
        "# Decompress forcing overwrite\n",
        "!gzip -d -f cc.en.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2je01IaHTu-1"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "if not loading_ok:\n",
        "  ft_model = fasttext.load_model('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scsA7IOVXfCq"
      },
      "outputs": [],
      "source": [
        "# Simplified version for FastText (get_word_vector naturally handles OOV words)\n",
        "def get_avg_embedding_fast(stems, embedding_model):\n",
        "    \"\"\"\n",
        "    Compute the average FastText embedding for a list of tokens.\n",
        "\n",
        "    This function uses FastText's get_word_vector, which does not raise\n",
        "    OOV errors and can construct embeddings for unseen words via subword info.\n",
        "    \"\"\"\n",
        "    if not stems:\n",
        "        return np.zeros(embedding_model.get_dimension())\n",
        "\n",
        "    # FastText provides a fast, direct method for getting word vectors\n",
        "    vectors = [embedding_model.get_word_vector(word) for word in stems]\n",
        "    return np.mean(vectors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq8ZblzwS9rx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# FastText embeddings (300D)\n",
        "# -------------------------------------------------------------------\n",
        "if not loading_ok:\n",
        "    print(\"⚠️ Data not found. Computing FastText (300D) embeddings...\")\n",
        "\n",
        "    embedding_model = ft_model\n",
        "    EMBEDDING_DIM = 300\n",
        "\n",
        "    # 1. Training set embeddings\n",
        "    X_train_ft = np.vstack([\n",
        "        get_avg_embedding_fast(stems, embedding_model)\n",
        "        for stems in tqdm(X_train_stems, desc=\"FastText: Training\")\n",
        "    ])\n",
        "\n",
        "    # 2. Validation set embeddings\n",
        "    X_val_ft = np.vstack([\n",
        "        get_avg_embedding_fast(stems, embedding_model)\n",
        "        for stems in tqdm(X_val_stems, desc=\"FastText: Validation\")\n",
        "    ])\n",
        "\n",
        "    # 3. Test set embeddings\n",
        "    X_test_ft = np.vstack([\n",
        "        get_avg_embedding_fast(stems, embedding_model)\n",
        "        for stems in tqdm(X_test_stems, desc=\"FastText: Test\")\n",
        "    ])\n",
        "\n",
        "    # Store results in the dictionary\n",
        "    other_embeddings[\"train\"][\"fasttext\"] = X_train_ft\n",
        "    other_embeddings[\"val\"][\"fasttext\"]   = X_val_ft\n",
        "    other_embeddings[\"test\"][\"fasttext\"]  = X_test_ft\n",
        "\n",
        "    print(\"FastText computation completed.\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ FastText data recovered from dictionary.\")\n",
        "\n",
        "    # Recover variables from the loaded dictionary\n",
        "    X_train_ft = other_embeddings[\"train\"][\"fasttext\"]\n",
        "    X_val_ft   = other_embeddings[\"val\"][\"fasttext\"]\n",
        "    X_test_ft  = other_embeddings[\"test\"][\"fasttext\"]\n",
        "\n",
        "# Final shape check\n",
        "print(f\"\\n--- FastText 300D Embeddings ---\")\n",
        "print(f\"Training Emb Shape:   {X_train_ft.shape}\")\n",
        "print(f\"Validation Emb Shape: {X_val_ft.shape}\")\n",
        "print(f\"Test Emb Shape:       {X_test_ft.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbZTbILeTO2X"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "  other_embeddings[\"train\"][\"fasttext\"] = X_train_ft\n",
        "  other_embeddings[\"val\"][\"fasttext\"]   = X_val_ft\n",
        "  other_embeddings[\"test\"][\"fasttext\"]  = X_test_ft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTl1h7dhMlNt"
      },
      "source": [
        "## Sentence Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z3U6YClMnb8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L12-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PctmXsv2MtPE"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "\n",
        "    # Recover texts for the three splits from the main dataframe\n",
        "    texts_train = df.loc[idx_train, \"text\"].astype(str).tolist()\n",
        "    texts_val   = df.loc[idx_val,   \"text\"].astype(str).tolist()\n",
        "    texts_test  = df.loc[idx_test,  \"text\"].astype(str).tolist()\n",
        "\n",
        "    print(\"Encoding TRAIN with SBERT...\")\n",
        "    X_train_sbert = sbert_model.encode(\n",
        "        texts_train,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Encoding VAL with SBERT...\")\n",
        "    X_val_sbert = sbert_model.encode(\n",
        "        texts_val,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Encoding TEST with SBERT...\")\n",
        "    X_test_sbert = sbert_model.encode(\n",
        "        texts_test,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUIo1YywMy1c"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "  other_embeddings[\"train\"][\"sbert\"] = X_train_sbert\n",
        "  other_embeddings[\"val\"][\"sbert\"]   = X_val_sbert\n",
        "  other_embeddings[\"test\"][\"sbert\"]  = X_test_sbert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDS1o6I6M1P2"
      },
      "outputs": [],
      "source": [
        "if not loading_ok:\n",
        "  print(\"Train SBERT:\", X_train_sbert.shape)\n",
        "  print(\"Val SBERT:\",   X_val_sbert.shape)\n",
        "  print(\"Test SBERT:\",  X_test_sbert.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGU4a3PONXo6"
      },
      "source": [
        "## Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Mc1jbZNca1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Normalization: Standardize each embedding set separately\n",
        "# -------------------------------------------------------------------\n",
        "if not loading_ok:\n",
        "    print(\"\\n--- Normalizing all embeddings ---\")\n",
        "\n",
        "    # Iterate over all embedding names stored in the dictionary\n",
        "    # (e.g., w2v, glove_twitter, glove_wiki, fasttext, sbert, etc.)\n",
        "    for emb_name in other_embeddings[\"train\"].keys():\n",
        "        print(f\"🔹 Normalizing embedding: {emb_name}\")\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Fit ONLY on the training set (compute mean and std)\n",
        "        X_train_norm = scaler.fit_transform(other_embeddings[\"train\"][emb_name])\n",
        "\n",
        "        # Transform validation and test sets using train statistics\n",
        "        X_val_norm  = scaler.transform(other_embeddings[\"val\"][emb_name])\n",
        "        X_test_norm = scaler.transform(other_embeddings[\"test\"][emb_name])\n",
        "\n",
        "        # Overwrite the dictionary with the normalized versions\n",
        "        other_embeddings[\"train\"][emb_name] = X_train_norm\n",
        "        other_embeddings[\"val\"][emb_name]   = X_val_norm\n",
        "        other_embeddings[\"test\"][emb_name]  = X_test_norm\n",
        "\n",
        "    print(\"\\n✅ Normalization completed and dictionary updated!\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ Data loaded from Drive (assumed already normalized before saving).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocQRqfbhkYJ"
      },
      "source": [
        "## Save on drive (delete the file on drive if you want to overwrite save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwipr63thp_Q"
      },
      "outputs": [],
      "source": [
        "# Base path on Drive\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/AI_for_security\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(DRIVE_BASE_PATH):\n",
        "    print(f\"Creating directory: {DRIVE_BASE_PATH}\")\n",
        "    os.makedirs(DRIVE_BASE_PATH)\n",
        "\n",
        "# Output filename (we use .pkl, standard for pickle)\n",
        "FILENAME = \"other_embeddings_data.pkl\"\n",
        "SAVE_PATH = os.path.join(DRIVE_BASE_PATH, FILENAME)\n",
        "\n",
        "print(f\"The dictionary will be saved to: {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czyiNN2phq31"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os  # Import os to check whether the file already exists\n",
        "\n",
        "# 1. Check if the file already exists before saving\n",
        "if os.path.exists(SAVE_PATH):\n",
        "    # If the file exists, print a warning and skip saving\n",
        "    print(f\"\\n⚠️ WARNING: File already exists at: {SAVE_PATH}\")\n",
        "    print(\"Save skipped to avoid accidental overwrite.\")\n",
        "else:\n",
        "    # 2. Save only if the file does NOT exist\n",
        "    try:\n",
        "        with open(SAVE_PATH, 'wb') as f:\n",
        "            pickle.dump(other_embeddings, f)\n",
        "\n",
        "        print(\"\\n✅ Save completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Critical error while saving: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QwBULsPXoM"
      },
      "source": [
        "# other data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snzryBDD22_W"
      },
      "source": [
        "In this “other data visualization” section we explore how different embedding methods (Word2Vec, FastText and SBERT) represent our prompts and how those representations behave. We look at predicted class distributions from simple classifiers built on top of each embedding, compare the norms of the embedding vectors across classes, inspect which words are closest to class centroids in embedding space, and visualize how “semantic blacklist” scores differ between benign and jailbreak prompts. This helps us understand not just the raw text, but how each representation captures jailbreak-like patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QUpL73wrJlM"
      },
      "source": [
        "##**other data visualization for Sentence Bert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPt0g3KMPXoN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 0. Encode with SBERT if embeddings are missing\n",
        "try:\n",
        "    X_train_sbert\n",
        "    X_val_sbert\n",
        "except NameError:\n",
        "    print(\"SBERT embeddings not found in memory: encoding from scratch...\")\n",
        "\n",
        "    # load SBERT model\n",
        "    sbert_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "\n",
        "    # texts for train/val (assumes df, idx_train, idx_val already defined)\n",
        "    texts_train = df.loc[idx_train, \"text\"].astype(str).tolist()\n",
        "    texts_val   = df.loc[idx_val,   \"text\"].astype(str).tolist()\n",
        "\n",
        "    X_train_sbert = sbert_model.encode(\n",
        "        texts_train,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "    X_val_sbert = sbert_model.encode(\n",
        "        texts_val,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # optional: save back into other_embeddings if you use it elsewhere\n",
        "    if \"train\" in other_embeddings and \"val\" in other_embeddings:\n",
        "        other_embeddings[\"train\"][\"sbert\"] = X_train_sbert\n",
        "        other_embeddings[\"val\"][\"sbert\"]   = X_val_sbert\n",
        "\n",
        "print(\"X_train_sbert shape:\", X_train_sbert.shape)\n",
        "print(\"X_val_sbert shape:\",   X_val_sbert.shape)\n",
        "print(\"len(y_train):\", len(y_train))\n",
        "\n",
        "# 1. Use SBERT embeddings\n",
        "X_train = X_train_sbert\n",
        "X_val   = X_val_sbert\n",
        "\n",
        "# 2. Train classifier on SBERT embeddings\n",
        "clf_sbert = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "clf_sbert.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predictions on validation set\n",
        "y_val_pred_sbert = clf_sbert.predict(X_val)\n",
        "\n",
        "# 4. Build a DataFrame just for plotting\n",
        "df_sbert_pred = pd.DataFrame({\n",
        "    \"label\": y_val_pred_sbert\n",
        "})\n",
        "\n",
        "# 5. SAME TYPE OF PLOT: class distribution (predicted)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", data=df_sbert_pred, palette=\"YlOrBr\")\n",
        "plt.title(\"Predicted class distribution (SBERT + Logistic Regression)\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#This countplot shows how often the SBERT + Logistic Regression classifier predicts each class on the validation set.\n",
        "#If the bars are roughly the same height, SBERT is predicting jailbreak and benign with similar frequency;\n",
        "#if one bar is much taller, the model is biased towards that class. Comparing this to the true class distribution (your earlier plot on df[\"label\"])\n",
        "#tells you whether SBERT is overpredicting the majority class or actually correcting some class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35SQ0nvTsx1H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Robust retrieval of SBERT embeddings\n",
        "try:\n",
        "    X_train_sbert\n",
        "except NameError:\n",
        "    X_train_sbert = other_embeddings[\"train\"][\"sbert\"]\n",
        "\n",
        "try:\n",
        "    X_val_sbert\n",
        "except NameError:\n",
        "    X_val_sbert = other_embeddings[\"val\"][\"sbert\"]\n",
        "\n",
        "# The test split may not exist: handle it safely\n",
        "try:\n",
        "    X_test_sbert\n",
        "except NameError:\n",
        "    X_test_sbert = None  # treat as optional\n",
        "\n",
        "print(\"X_train_sbert shape:\", X_train_sbert.shape)\n",
        "print(\"X_val_sbert shape:\",   X_val_sbert.shape)\n",
        "if X_test_sbert is not None:\n",
        "    print(\"X_test_sbert shape:\",  X_test_sbert.shape)\n",
        "\n",
        "# 1) Column for SBERT norm\n",
        "df[\"sbert_norm\"] = np.nan\n",
        "\n",
        "# Fill train and val (which we definitely have)\n",
        "df.loc[idx_train, \"sbert_norm\"] = np.linalg.norm(X_train_sbert, axis=1)\n",
        "df.loc[idx_val,   \"sbert_norm\"] = np.linalg.norm(X_val_sbert,   axis=1)\n",
        "\n",
        "# If the test split exists, add it as well\n",
        "if X_test_sbert is not None:\n",
        "    df.loc[idx_test, \"sbert_norm\"] = np.linalg.norm(X_test_sbert, axis=1)\n",
        "\n",
        "# 2) Same type of plot: boxplot by class\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=\"label\", y=\"sbert_norm\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Distribution of SBERT norms for prompts\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"‖SBERT embedding‖ (L2)\")\n",
        "plt.show()\n",
        "\n",
        "# Here we are comparing the L2 norm of the SBERT embeddings between jailbreak and benign prompts.\n",
        "# If one class has consistently higher norms (higher median and upper quartiles), it suggests SBERT represents those prompts as\n",
        "# “larger” vectors in embedding space, which can indicate differences in semantic richness or how the model encodes that type of content.\n",
        "# If the distributions overlap a lot, SBERT norm alone is not a strong separator; if they diverge, this scalar feature already\n",
        "# carries some class signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKQ3F1_rtltt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 0. Ensure we have the SBERT model loaded\n",
        "try:\n",
        "    sbert_model\n",
        "except NameError:\n",
        "    sbert_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "\n",
        "# 1. Assumption: X_train_sbert and y_train already exist.\n",
        "#    If they are not in RAM, recover them from the dictionary:\n",
        "try:\n",
        "    X_train_sbert\n",
        "    y_train\n",
        "except NameError:\n",
        "    X_train_sbert = other_embeddings[\"train\"][\"sbert\"]\n",
        "    # y_train must already exist in the notebook (e.g., created by the train/val/test split)\n",
        "\n",
        "y_train_arr = np.array(y_train)\n",
        "\n",
        "mask_jail = (y_train_arr == \"jailbreak\")\n",
        "mask_ben  = (y_train_arr == \"benign\")\n",
        "\n",
        "jail_centroid   = X_train_sbert[mask_jail].mean(axis=0)\n",
        "benign_centroid = X_train_sbert[mask_ben].mean(axis=0)\n",
        "\n",
        "# 2. \"Light\" version: use only words already present in jail_freq / benign_freq,\n",
        "#    and limit the number of candidates to avoid heavy RAM usage.\n",
        "MAX_CANDIDATES = 300  # you can increase/decrease this\n",
        "\n",
        "jail_words   = [w for w, _ in jail_freq][:MAX_CANDIDATES]\n",
        "benign_words = [w for w, _ in benign_freq][:MAX_CANDIDATES]\n",
        "\n",
        "def top_words_for_centroid_sbert(centroid, candidate_words, topn=15):\n",
        "    \"\"\"\n",
        "    Compute the words closest to a SBERT centroid among a list\n",
        "    of candidate words (light version, no huge vocabularies).\n",
        "    \"\"\"\n",
        "    if len(candidate_words) == 0:\n",
        "        return pd.DataFrame(columns=[\"word\", \"count\"])\n",
        "\n",
        "    # Encode candidate words with SBERT\n",
        "    word_embs = sbert_model.encode(\n",
        "        candidate_words,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "\n",
        "    # Normalize for cosine similarity\n",
        "    c = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
        "    w_norm = word_embs / (np.linalg.norm(word_embs, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    sims = w_norm @ c  # dot product = cosine similarity\n",
        "    sims = np.asarray(sims)\n",
        "\n",
        "    idx = np.argsort(sims)[::-1][:topn]\n",
        "    top_words = [candidate_words[i] for i in idx]\n",
        "    top_sims  = sims[idx]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"word\": top_words,\n",
        "        \"count\": top_sims  # use 'count' to reuse your plotting code\n",
        "    })\n",
        "\n",
        "# 3. DataFrames for jailbreak / benign (light SBERT version)\n",
        "j_df = top_words_for_centroid_sbert(jail_centroid,   jail_words,   topn=15)\n",
        "b_df = top_words_for_centroid_sbert(benign_centroid, benign_words, topn=15)\n",
        "\n",
        "# 4. Same type of plot: two horizontal barplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "sns.barplot(y='word', x='count', data=j_df, ax=axes[0], palette='YlOrBr')\n",
        "sns.barplot(y='word', x='count', data=b_df, ax=axes[1], palette='YlOrBr')\n",
        "\n",
        "axes[0].set_title('Top SBERT words near jailbreak centroid (light)')\n",
        "axes[1].set_title('Top SBERT words near benign centroid (light)')\n",
        "\n",
        "axes[0].set_xlabel('Cosine similarity')\n",
        "axes[1].set_xlabel('Cosine similarity')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_facecolor('#fff8dc')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# These two horizontal barplots show which words (from your dataset vocabulary) are closest in SBERT space\n",
        "# to the jailbreak centroid and to the benign centroid.\n",
        "# The jailbreak side highlights what SBERT considers “typical” jailbreak-like terms; the benign side shows words\n",
        "# more characteristic of benign prompts. Comparing the two plots tells you whether SBERT is capturing intuitive differences:\n",
        "# for example, jailbreak might lean towards instruction-breaking, system-manipulation, or sensitive-content words,\n",
        "# while benign might show more neutral or task-oriented vocabulary.\n",
        "# This gives a more interpretable view of what SBERT has learned about the two classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9vS5okIuEkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 0) Ensure we have the SBERT model\n",
        "try:\n",
        "    sbert_model\n",
        "except NameError:\n",
        "    sbert_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "\n",
        "# 1) First blacklist\n",
        "blacklist = ['ignore', 'ethical', 'data', 'information', 'write', 'create']\n",
        "\n",
        "# 2) SBERT centroid of the blacklist\n",
        "bl_embs = sbert_model.encode(blacklist, convert_to_numpy=True, show_progress_bar=False)\n",
        "blacklist_centroid = bl_embs.mean(axis=0)\n",
        "\n",
        "# 3) Robust retrieval of SBERT embeddings for the three splits\n",
        "try:\n",
        "    X_train_sbert\n",
        "except NameError:\n",
        "    X_train_sbert = other_embeddings[\"train\"][\"sbert\"]\n",
        "\n",
        "try:\n",
        "    X_val_sbert\n",
        "except NameError:\n",
        "    X_val_sbert   = other_embeddings[\"val\"][\"sbert\"]\n",
        "\n",
        "# Test split may not exist or may not have SBERT; handle with .get\n",
        "X_test_sbert = None\n",
        "if \"test\" in other_embeddings:\n",
        "    X_test_sbert = other_embeddings[\"test\"].get(\"sbert\", None)\n",
        "\n",
        "print(\"X_train_sbert:\", X_train_sbert.shape)\n",
        "print(\"X_val_sbert:\",   X_val_sbert.shape)\n",
        "print(\"X_test_sbert is None?\", X_test_sbert is None)\n",
        "\n",
        "# 4) Function for cosine similarity to centroid\n",
        "def sim_to_centroid(matrix, centroid):\n",
        "    c = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
        "    norms = np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12\n",
        "    m_norm = matrix / norms\n",
        "    return m_norm @ c  # dot product = cosine similarity\n",
        "\n",
        "# 5) Column for SBERT similarity to the blacklist centroid\n",
        "df[\"sbert_sim_blacklist\"] = np.nan\n",
        "\n",
        "df.loc[idx_train, \"sbert_sim_blacklist\"] = sim_to_centroid(X_train_sbert, blacklist_centroid)\n",
        "df.loc[idx_val,   \"sbert_sim_blacklist\"] = sim_to_centroid(X_val_sbert,   blacklist_centroid)\n",
        "\n",
        "# Use the test split only if it really exists\n",
        "if (X_test_sbert is not None) and (len(idx_test) > 0):\n",
        "    df.loc[idx_test, \"sbert_sim_blacklist\"] = sim_to_centroid(X_test_sbert, blacklist_centroid)\n",
        "\n",
        "# 6) Data-driven threshold: e.g. top 20% most similar\n",
        "sims = df[\"sbert_sim_blacklist\"].dropna()\n",
        "threshold = sims.quantile(0.8)\n",
        "\n",
        "df[\"sbert_blacklisted\"] = df[\"sbert_sim_blacklist\"] > threshold\n",
        "print(df[\"sbert_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 7) Same type of plot: countplot per class with hue on SBERT flag\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", hue=\"sbert_blacklisted\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Presence of 'semantic blacklist' (SBERT) per class\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# In this plot, we take a first blacklist (ignore, ethical, data, information, write, create),\n",
        "# compute a SBERT centroid for those words, and then measure how similar each prompt’s embedding is to that centroid.\n",
        "# Prompts above the chosen similarity threshold are marked sbert_blacklisted = True.\n",
        "# The countplot shows, for each class, how many prompts are flagged by this semantic blacklist.\n",
        "# If jailbreak has a much higher proportion of True than benign, this blacklist and SBERT similarity are good at capturing jailbreak-like prompts.\n",
        "# If both bars look similar, this blacklist does not discriminate well in SBERT space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFMfqEBgvCNn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 0) Ensure we have the SBERT model\n",
        "try:\n",
        "    sbert_model\n",
        "except NameError:\n",
        "    sbert_model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
        "\n",
        "# 1) Updated blacklist\n",
        "blacklist = ['ignore', 'previous', 'override', 'forget', 'write', 'create']\n",
        "\n",
        "# 2) SBERT centroid of the blacklist\n",
        "bl_embs = sbert_model.encode(blacklist, convert_to_numpy=True, show_progress_bar=False)\n",
        "blacklist_centroid = bl_embs.mean(axis=0)\n",
        "\n",
        "# 3) Robust retrieval of SBERT embeddings for the three splits\n",
        "try:\n",
        "    X_train_sbert\n",
        "except NameError:\n",
        "    X_train_sbert = other_embeddings[\"train\"][\"sbert\"]\n",
        "\n",
        "try:\n",
        "    X_val_sbert\n",
        "except NameError:\n",
        "    X_val_sbert = other_embeddings[\"val\"][\"sbert\"]\n",
        "\n",
        "# Test split may not exist / may not have SBERT\n",
        "X_test_sbert = None\n",
        "if \"test\" in other_embeddings:\n",
        "    X_test_sbert = other_embeddings[\"test\"].get(\"sbert\", None)\n",
        "\n",
        "print(\"X_train_sbert:\", X_train_sbert.shape)\n",
        "print(\"X_val_sbert:\",   X_val_sbert.shape)\n",
        "print(\"X_test_sbert is None?\", X_test_sbert is None)\n",
        "\n",
        "# 4) Function for cosine similarity to centroid\n",
        "def sim_to_centroid(matrix, centroid):\n",
        "    c = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
        "    norms = np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12\n",
        "    m_norm = matrix / norms\n",
        "    return m_norm @ c  # dot product = cosine similarity\n",
        "\n",
        "# 5) Column for SBERT similarity to the blacklist\n",
        "df[\"sbert_sim_blacklist\"] = np.nan\n",
        "\n",
        "df.loc[idx_train, \"sbert_sim_blacklist\"] = sim_to_centroid(X_train_sbert, blacklist_centroid)\n",
        "df.loc[idx_val,   \"sbert_sim_blacklist\"] = sim_to_centroid(X_val_sbert,   blacklist_centroid)\n",
        "\n",
        "if (X_test_sbert is not None) and (len(idx_test) > 0):\n",
        "    df.loc[idx_test, \"sbert_sim_blacklist\"] = sim_to_centroid(X_test_sbert, blacklist_centroid)\n",
        "\n",
        "# 6) Data-driven threshold: e.g. top 20% most similar\n",
        "sims = df[\"sbert_sim_blacklist\"].dropna()\n",
        "threshold = sims.quantile(0.8)   # you can change 0.8 to 0.7 / 0.9 if you want more/less \"True\"\n",
        "\n",
        "df[\"sbert_blacklisted\"] = df[\"sbert_sim_blacklist\"] > threshold\n",
        "print(df[\"sbert_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 7) Same plot: countplot per class with hue on SBERT flag\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", hue=\"sbert_blacklisted\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Presence of 'semantic blacklist' SBERT per class\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# In this plot, we take an updated blacklist (ignore, previous, override, forget, write, create),\n",
        "# compute a SBERT centroid for those words, and then measure how similar each prompt’s embedding is to that centroid.\n",
        "# Prompts above the chosen similarity threshold are marked sbert_blacklisted = True.\n",
        "# The countplot shows, for each class, how many prompts are flagged by this semantic blacklist.\n",
        "# If jailbreak has a much higher proportion of True than benign,\n",
        "# this blacklist and SBERT similarity are good at capturing jailbreak-like prompts.\n",
        "# If both bars look similar, this blacklist does not discriminate well in SBERT space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8wmT-rVJU88"
      },
      "source": [
        "##**other data visualization for Fast Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvoJvgYdKRIF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Train a classifier on FastText embeddings\n",
        "X_train = X_train_ft\n",
        "X_val   = X_val_ft\n",
        "\n",
        "clf_ft = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "clf_ft.fit(X_train, y_train)\n",
        "\n",
        "# 2. Predictions on validation (or test) set\n",
        "y_val_pred_ft = clf_ft.predict(X_val)\n",
        "\n",
        "# 3. Build a small DataFrame just for plotting\n",
        "df_ft_pred = pd.DataFrame({\n",
        "    \"label\": y_val_pred_ft\n",
        "})\n",
        "\n",
        "# 4. SAME TYPE OF PLOT: countplot of classes\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", data=df_ft_pred, palette=\"YlOrBr\")\n",
        "plt.title(\"Predicted class distribution (FastText + Logistic Regression)\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#This countplot shows how often the FastText + Logistic Regression classifier predicts each class on the validation set,\n",
        "#letting you see if the model tends to favor jailbreak or benign."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9tp7tvXKZ9Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "\n",
        "# 0. Ensure FastText file is present (download if missing)\n",
        "fasttext_path = \"cc.en.300.bin\"\n",
        "\n",
        "if not os.path.exists(fasttext_path):\n",
        "    # scarica e decomprimi il modello pre-addestrato\n",
        "    print(\"FastText file not found, downloading...\")\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"wget\",\n",
        "            \"http://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\"\n",
        "        ],\n",
        "        check=True\n",
        "    )\n",
        "    subprocess.run([\"gzip\", \"-d\", \"cc.en.300.bin.gz\"], check=True)\n",
        "\n",
        "    assert os.path.exists(fasttext_path), \"Download/decompression failed, file still missing.\"\n",
        "\n",
        "# 1. Load FastText model (if not already loaded)\n",
        "try:\n",
        "    ft_model\n",
        "except NameError:\n",
        "    ft_model = fasttext.load_model(fasttext_path)\n",
        "    print(\"FastText model loaded from:\", fasttext_path)\n",
        "\n",
        "# 2. Function to get FastText sentence embedding (no newlines)\n",
        "def text_to_vector_fast(text, embedding_model=None):\n",
        "    if embedding_model is None:\n",
        "        embedding_model = ft_model\n",
        "\n",
        "    if pd.isna(text):\n",
        "        return np.zeros(embedding_model.get_dimension())\n",
        "\n",
        "    cleaned = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
        "    return embedding_model.get_sentence_vector(cleaned)\n",
        "\n",
        "# 3. Compute L2 norm of FastText embedding for each text\n",
        "df[\"ft_norm\"] = df[\"text\"].apply(\n",
        "    lambda txt: np.linalg.norm(text_to_vector_fast(txt))\n",
        ")\n",
        "\n",
        "# 4. Same boxplot as before, but with FastText\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=\"label\", y=\"ft_norm\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Distribuzione norme FastText dei prompt\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"‖FastText embedding‖ (L2)\")\n",
        "plt.show()\n",
        "\n",
        "#This boxplot compares the L2 norm of FastText sentence embeddings between benign and jailbreak prompts,\n",
        "#showing whether one class tends to have systematically larger or smaller embedding magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsHxJnijpvLq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OPTIONAL: stopword list to remove very generic words\n",
        "stopwords = {\n",
        "    \"the\",\"a\",\"an\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"but\",\n",
        "    \"it\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\n",
        "    \"there\",\"here\",\"any\",\"some\",\"many\",\"much\",\n",
        "    \"how\",\"about\",\"way\",\"have\",\"has\",\"had\",\"not\",\"no\",\n",
        "    \"do\",\"does\",\"did\",\"can\",\"could\",\"would\",\"should\",\n",
        "    \"i\",\"you\",\"he\",\"she\",\"we\",\"they\",\"them\",\"me\",\"my\",\"your\",\n",
        "}\n",
        "\n",
        "# 1. Class labels array\n",
        "y_train_arr = np.array(y_train)\n",
        "\n",
        "mask_jail = (y_train_arr == \"jailbreak\")\n",
        "mask_ben  = (y_train_arr == \"benign\")\n",
        "\n",
        "# Centroids in FastText space\n",
        "jail_centroid   = X_train_ft[mask_jail].mean(axis=0)\n",
        "benign_centroid = X_train_ft[mask_ben].mean(axis=0)\n",
        "\n",
        "# 2. Helper: rank ONLY given words by FastText similarity to centroid\n",
        "def top_words_for_centroid_ft(centroid, ft_model, candidate_words, topn=15):\n",
        "    c = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
        "\n",
        "    words = []\n",
        "    sims  = []\n",
        "\n",
        "    for w in candidate_words:\n",
        "        # filter stopwords and very short tokens\n",
        "        if len(w) < 3:\n",
        "            continue\n",
        "        if w.lower() in stopwords:\n",
        "            continue\n",
        "\n",
        "        # FastText handles OOV via subword, so this is safe\n",
        "        v = ft_model.get_word_vector(w)\n",
        "        v_norm = v / (np.linalg.norm(v) + 1e-12)\n",
        "        sim = np.dot(c, v_norm)  # cosine similarity (both normalized)\n",
        "        words.append(w)\n",
        "        sims.append(sim)\n",
        "\n",
        "    if not words:\n",
        "        return pd.DataFrame(columns=[\"word\", \"count\"])\n",
        "\n",
        "    sims = np.array(sims)\n",
        "    words = np.array(words)\n",
        "\n",
        "    idx = np.argsort(sims)[::-1][:topn]\n",
        "    top_words = words[idx]\n",
        "    top_sims  = sims[idx]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"word\": top_words,\n",
        "        \"count\": top_sims   # keep 'count' to reuse your plotting code\n",
        "    })\n",
        "\n",
        "# 3. Build candidate word lists from your freq structures\n",
        "#    (assuming jail_freq / benign_freq = list of (word, count))\n",
        "jail_words   = [w for w, _ in jail_freq]\n",
        "benign_words = [w for w, _ in benign_freq]\n",
        "\n",
        "j_df = top_words_for_centroid_ft(jail_centroid, ft_model, jail_words,   topn=15)\n",
        "b_df = top_words_for_centroid_ft(benign_centroid, ft_model, benign_words, topn=15)\n",
        "\n",
        "# 4. SAME TYPE OF PLOT: two horizontal barplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "sns.barplot(y='word', x='count', data=j_df, ax=axes[0], palette='YlOrBr')\n",
        "sns.barplot(y='word', x='count', data=b_df, ax=axes[1], palette='YlOrBr')\n",
        "\n",
        "axes[0].set_title('Top parole FastText vicine a jailbreak (dataset vocab)')\n",
        "axes[1].set_title('Top parole FastText vicine a benign (dataset vocab)')\n",
        "\n",
        "axes[0].set_xlabel('Cosine similarity')\n",
        "axes[1].set_xlabel('Cosine similarity')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_facecolor('#fff8dc')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#This boxplot compares the L2 norm of FastText sentence embeddings between benign and jailbreak prompts,\n",
        "#showing whether one class tends to have systematically larger or smaller embedding magnitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB3WWsb3LcGW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Initial blacklist\n",
        "blacklist = ['ignore', 'ethical', 'data', 'information', 'write', 'create']\n",
        "\n",
        "# 2) Build the FastText centroid of the blacklist\n",
        "#    (assuming ft_model is already loaded)\n",
        "vocab = set(ft_model.get_words())\n",
        "blacklist_vecs = [\n",
        "    ft_model.get_word_vector(w) for w in blacklist\n",
        "    if w in vocab\n",
        "]\n",
        "\n",
        "blacklist_centroid = np.mean(blacklist_vecs, axis=0)\n",
        "\n",
        "# 3) Functions for sentence embedding and similarity\n",
        "def ft_sentence_vector(text, model=ft_model):\n",
        "    if pd.isna(text):\n",
        "        return np.zeros(model.get_dimension())\n",
        "    cleaned = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
        "    return model.get_sentence_vector(cleaned)\n",
        "\n",
        "def ft_blacklist_similarity(text):\n",
        "    v = ft_sentence_vector(text)\n",
        "    if np.linalg.norm(v) == 0:\n",
        "        return np.nan\n",
        "    return np.dot(v, blacklist_centroid) / (\n",
        "        np.linalg.norm(v) * np.linalg.norm(blacklist_centroid) + 1e-12\n",
        "    )\n",
        "\n",
        "# 4) Compute similarity for each prompt\n",
        "df[\"ft_sim_blacklist\"] = df[\"text\"].apply(ft_blacklist_similarity)\n",
        "\n",
        "# 5) Data-driven threshold: e.g. top 20% most similar\n",
        "sims = df[\"ft_sim_blacklist\"].dropna()\n",
        "threshold = sims.quantile(0.8)   # you can change 0.8 → 0.7 / 0.9 etc.\n",
        "\n",
        "df[\"ft_blacklisted\"] = df[\"ft_sim_blacklist\"] > threshold\n",
        "\n",
        "print(df[\"ft_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 6) Same plot: countplot per class with hue on FastText flag\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', hue='ft_blacklisted', data=df, palette='YlOrBr')\n",
        "plt.title('Presence of \"semantic blacklist\" FastText per class')\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Here each prompt gets a similarity score to the centroid of the blacklist words and those above a threshold are flagged.\n",
        "# The countplot shows, per class, how many prompts are semantically close to this blacklist,\n",
        "# indicating how discriminative it is for jailbreak vs benign.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNCDzdTIL56s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Updated blacklist\n",
        "blacklist = ['ignore', 'previous', 'override', 'forget', 'write', 'create']\n",
        "\n",
        "# 2) Build the FastText centroid of the blacklist\n",
        "#    (assuming ft_model is already loaded)\n",
        "vocab = set(ft_model.get_words())\n",
        "blacklist_vecs = [\n",
        "    ft_model.get_word_vector(w) for w in blacklist\n",
        "    if w in vocab\n",
        "]\n",
        "\n",
        "# Safety: if none of the blacklist words are in the vocabulary\n",
        "if len(blacklist_vecs) == 0:\n",
        "    raise ValueError(\"None of the blacklist words are present in the FastText vocabulary.\")\n",
        "\n",
        "blacklist_centroid = np.mean(blacklist_vecs, axis=0)\n",
        "\n",
        "# 3) Helper functions: sentence embedding + similarity\n",
        "def ft_sentence_vector(text, model=ft_model):\n",
        "    if pd.isna(text):\n",
        "        return np.zeros(model.get_dimension())\n",
        "    cleaned = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
        "    return model.get_sentence_vector(cleaned)\n",
        "\n",
        "def ft_blacklist_similarity(text):\n",
        "    v = ft_sentence_vector(text)\n",
        "    if np.linalg.norm(v) == 0:\n",
        "        return np.nan\n",
        "    return np.dot(v, blacklist_centroid) / (\n",
        "        np.linalg.norm(v) * np.linalg.norm(blacklist_centroid) + 1e-12\n",
        "    )\n",
        "\n",
        "# 4) Similarity for each prompt\n",
        "df[\"ft_sim_blacklist\"] = df[\"text\"].apply(ft_blacklist_similarity)\n",
        "\n",
        "# 5) Data-driven threshold: top 20% most similar (you can change 0.8)\n",
        "sims = df[\"ft_sim_blacklist\"].dropna()\n",
        "threshold = sims.quantile(0.8)\n",
        "\n",
        "df[\"ft_blacklisted\"] = df[\"ft_sim_blacklist\"] > threshold\n",
        "print(df[\"ft_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 6) Same plot: countplot per class with hue on FastText flag\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', hue='ft_blacklisted', data=df, palette='YlOrBr')\n",
        "plt.title(\"Presence of 'semantic blacklist' FastText per class\")\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Here each prompt gets a similarity score to the centroid of the updated blacklist words,\n",
        "# and those above a fixed percentile (e.g., 80%) are flagged as ft_blacklisted.\n",
        "# The countplot shows, per class, how many prompts are semantically close to this blacklist,\n",
        "# indicating how discriminative it is for jailbreak vs benign.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mt9cLzh6Y-P"
      },
      "source": [
        "##**other data visualization for w2v:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqO7CqPD6o6e"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Train a classifier on W2V embeddings\n",
        "X_train = X_train_w2v\n",
        "X_val   = X_val_w2v\n",
        "\n",
        "clf_w2v = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "clf_w2v.fit(X_train, y_train)\n",
        "\n",
        "# 2. Predictions on validation (or test) set\n",
        "y_val_pred_w2v = clf_w2v.predict(X_val)\n",
        "\n",
        "# 3. Build a small DataFrame just for plotting\n",
        "df_w2v_pred = pd.DataFrame({\n",
        "    \"label\": y_val_pred_w2v\n",
        "})\n",
        "\n",
        "# 4. SAME TYPE OF PLOT: countplot of classes\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", data=df_w2v_pred, palette=\"YlOrBr\")\n",
        "plt.title(\"Predicted class distribution (W2V + Logistic Regression)\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "#This countplot shows how often the W2V + Logistic Regression classifier predicts each class on the validation set,\n",
        "#revealing whether the model tends to favor jailbreak or benign prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsj_iK7u7gxz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Compute a scalar W2V-based feature for each text: the L2 norm of its embedding\n",
        "df[\"w2v_norm\"] = df[\"text\"].apply(\n",
        "    lambda txt: np.linalg.norm(text_to_vector(txt, model))\n",
        ")\n",
        "\n",
        "# 2. Same type of plot: boxplot per class, but using w2v_norm instead of length\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(x=\"label\", y=\"w2v_norm\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Distribuzione norme W2V dei prompt\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"‖W2V embedding‖ (L2)\")\n",
        "plt.show()\n",
        "\n",
        "#This boxplot compares the L2 norm of the Word2Vec-based embeddings between benign and jailbreak prompts,\n",
        "#showing whether one class tends to have systematically larger or smaller W2V representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeRMKJr17qW_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- stopword list (same as before, extend if you like) ----\n",
        "stopwords = {\n",
        "    \"the\",\"a\",\"an\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"but\",\n",
        "    \"it\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\n",
        "    \"there\",\"here\",\"any\",\"some\",\"many\",\"much\",\n",
        "    \"how\",\"about\",\"way\",\"have\",\"has\",\"had\",\"not\",\"no\",\n",
        "    \"do\",\"does\",\"did\",\"can\",\"could\",\"would\",\"should\",\n",
        "    \"i\",\"you\",\"he\",\"she\",\"we\",\"they\",\"them\",\"me\",\"my\",\"your\",\n",
        "}\n",
        "\n",
        "# ---- 1. Class centroids ----\n",
        "y_train_arr = np.array(y_train)\n",
        "\n",
        "mask_jail = (y_train_arr == \"jailbreak\")\n",
        "mask_ben  = (y_train_arr == \"benign\")\n",
        "\n",
        "jail_centroid   = X_train_w2v[mask_jail].mean(axis=0)\n",
        "benign_centroid = X_train_w2v[mask_ben].mean(axis=0)\n",
        "\n",
        "# ---- 2. Helper: top-N words (here N=100) ----\n",
        "def top_words_for_centroid(centroid, w2v_model, topn=100):\n",
        "    c = centroid / (np.linalg.norm(centroid) + 1e-12)\n",
        "\n",
        "    words = []\n",
        "    sims = []\n",
        "\n",
        "    for w in w2v_model.key_to_index.keys():\n",
        "        if len(w) < 3:\n",
        "            continue\n",
        "        if w.lower() in stopwords:\n",
        "            continue\n",
        "\n",
        "        v = w2v_model[w]\n",
        "        v_norm = v / (np.linalg.norm(v) + 1e-12)\n",
        "        sim = np.dot(c, v_norm)\n",
        "        words.append(w)\n",
        "        sims.append(sim)\n",
        "\n",
        "    sims = np.array(sims)\n",
        "    words = np.array(words)\n",
        "\n",
        "    idx = np.argsort(sims)[::-1][:topn]\n",
        "    top_words = words[idx]\n",
        "    top_sims  = sims[idx]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"word\": top_words,\n",
        "        \"similarity\": top_sims\n",
        "    })\n",
        "\n",
        "# ---- 3. DataFrames for jailbreak / benign (top 100) ----\n",
        "j_df = top_words_for_centroid(jail_centroid, model, topn=100)\n",
        "b_df = top_words_for_centroid(benign_centroid, model, topn=100)\n",
        "\n",
        "# ---- 4. Plot: two horizontal barplots side by side ----\n",
        "# height increased to fit 100 labels\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 30))\n",
        "\n",
        "sns.barplot(\n",
        "    y=\"word\",\n",
        "    x=\"similarity\",\n",
        "    data=j_df,\n",
        "    ax=axes[0],\n",
        "    palette=\"YlOrBr\"\n",
        ")\n",
        "sns.barplot(\n",
        "    y=\"word\",\n",
        "    x=\"similarity\",\n",
        "    data=b_df,\n",
        "    ax=axes[1],\n",
        "    palette=\"YlOrBr\"\n",
        ")\n",
        "\n",
        "axes[0].set_title(\"Top 100 parole W2V vicine a jailbreak (no stopwords)\")\n",
        "axes[1].set_title(\"Top 100 parole W2V vicine a benign (no stopwords)\")\n",
        "\n",
        "axes[0].set_xlabel(\"Cosine similarity\")\n",
        "axes[1].set_xlabel(\"Cosine similarity\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_facecolor(\"#fff8dc\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#These two tall barplots list the 100 words whose W2V vectors are closest to the jailbreak and benign centroids, respectively.\n",
        "#They highlight which terms Word2Vec associates most strongly with each class once generic stopwords are removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy79NJpp96ma"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Blacklist words\n",
        "blacklist = ['ignore', 'previous', 'override', 'forget', 'write', 'create']\n",
        "\n",
        "# 2) W2V centroid of the blacklist\n",
        "blacklist_vecs = [model[w] for w in blacklist if w in model.key_to_index]\n",
        "blacklist_centroid = np.mean(blacklist_vecs, axis=0)\n",
        "\n",
        "def w2v_similarity(text, w2v_model, centroid):\n",
        "    \"\"\"Cosine similarity between text embedding and blacklist centroid.\"\"\"\n",
        "    v = text_to_vector(text, w2v_model)\n",
        "    if np.linalg.norm(v) == 0:\n",
        "        return np.nan\n",
        "    return np.dot(v, centroid) / (np.linalg.norm(v) * np.linalg.norm(centroid) + 1e-12)\n",
        "\n",
        "# 3) W2V similarity of each prompt to the blacklist centroid\n",
        "df[\"w2v_sim\"] = df[\"text\"].apply(lambda t: w2v_similarity(t, model, blacklist_centroid))\n",
        "\n",
        "# 4) Data-driven threshold: e.g. top 20% most similar\n",
        "threshold = df[\"w2v_sim\"].quantile(0.8)   # you can try 0.7, 0.9, etc.\n",
        "\n",
        "df[\"w2v_blacklisted\"] = df[\"w2v_sim\"] > threshold\n",
        "\n",
        "print(df[\"w2v_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 5) Same plot as before\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"label\", hue=\"w2v_blacklisted\", data=df, palette=\"YlOrBr\")\n",
        "plt.title(\"Presence of 'semantic blacklist' W2V per class\")\n",
        "plt.gca().set_facecolor(\"#fff8dc\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Here each prompt receives a cosine similarity score to the W2V centroid of the blacklist words,\n",
        "# and those above a fixed percentile (e.g., 80%) are flagged as w2v_blacklisted.\n",
        "# The countplot shows, per class, how many prompts are semantically close to this blacklist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzRW82df-co2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Blacklist words\n",
        "blacklist = ['ignore', 'previous', 'override', 'forget', 'write', 'create']\n",
        "\n",
        "# 2) W2V centroid of the blacklist\n",
        "blacklist_vecs = [model[w] for w in blacklist if w in model.key_to_index]\n",
        "blacklist_centroid = np.mean(blacklist_vecs, axis=0)\n",
        "\n",
        "def w2v_similarity(text, w2v_model, centroid):\n",
        "    \"\"\"Cosine similarity between text embedding and blacklist centroid.\"\"\"\n",
        "    v = text_to_vector(text, w2v_model)\n",
        "    if np.linalg.norm(v) == 0:\n",
        "        return np.nan\n",
        "    return np.dot(v, centroid) / (np.linalg.norm(v) * np.linalg.norm(centroid) + 1e-12)\n",
        "\n",
        "# 3) W2V similarity of each prompt with the blacklist\n",
        "df[\"w2v_sim\"] = df[\"text\"].apply(lambda t: w2v_similarity(t, model, blacklist_centroid))\n",
        "\n",
        "# 4) Automatic threshold choice: search for a percentile that yields both True and False\n",
        "sims = df[\"w2v_sim\"].dropna()\n",
        "\n",
        "threshold = None\n",
        "for q in [0.9, 0.8, 0.7, 0.6, 0.5]:\n",
        "    t = sims.quantile(q)\n",
        "    flags = sims > t\n",
        "    if flags.nunique() == 2:   # we get both 0 and 1\n",
        "        threshold = t\n",
        "        break\n",
        "\n",
        "# If for some reason we do not find anything, fall back to the mean\n",
        "if threshold is None:\n",
        "    threshold = sims.mean()\n",
        "\n",
        "print(\"Chosen threshold:\", threshold)\n",
        "\n",
        "df[\"w2v_blacklisted\"] = df[\"w2v_sim\"] > threshold\n",
        "print(df[\"w2v_blacklisted\"].value_counts(dropna=False))\n",
        "\n",
        "# 5) Same plot as before\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', hue='w2v_blacklisted', data=df, palette='YlOrBr')\n",
        "plt.title(\"Presence of semantic blacklist (W2V) per class\")\n",
        "plt.gca().set_facecolor('#fff8dc')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# This is similar to the previous graph but uses an adaptive threshold chosen to ensure both True and False values appear.\n",
        "# The resulting countplot shows how this more robust thresholding affects the distribution of “blacklisted” prompts across\n",
        "# benign and jailbreak classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5734Yis2Qhsu"
      },
      "source": [
        "# Logistic regression, SVC, Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUYrsx_CquP"
      },
      "source": [
        "In this chapter we build and compare classic machine-learning classifiers for the jailbreak vs benign task. Starting from the preprocessed texts, we vectorize them (bag-of-words and various embeddings), then train and evaluate Multinomial Naive Bayes, Logistic Regression, and Support Vector Machines. We also tune key hyperparameters (like C and alpha) and inspect metrics and confusion matrices to understand model performance and possible under- or overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvQRXky6h21B"
      },
      "source": [
        "### Classifiers fitting and evaluation\n",
        "\n",
        "We can see how different classifiers behave on the same data set and compare their performances.\n",
        "\n",
        "As for now we will work with simple linear classifiers:\n",
        "- *Naive Bayes*\n",
        "- *Logistic Regression*\n",
        "- *Support Vector Machines*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxafZSfwfN_-"
      },
      "outputs": [],
      "source": [
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvswi1iVcUuA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_classifier(\n",
        "    clf,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    show_plots=True,\n",
        "):\n",
        "\n",
        "    # 1) Predictions\n",
        "    y_pred = clf.predict(X_val)\n",
        "\n",
        "    # 2) Base Metrics\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    prec = precision_score(y_val, y_pred, pos_label=pos_label)\n",
        "    rec = recall_score(y_val, y_pred, pos_label=pos_label)\n",
        "    f1 = f1_score(y_val, y_pred, pos_label=pos_label)\n",
        "\n",
        "    spec = recall_score(y_val, y_pred, pos_label=neg_label)\n",
        "\n",
        "    print(f\"Accuracy:     {acc:.5f}\")\n",
        "    print(f\"Precision:    {prec:.5f}   (positive class: {pos_label})\")\n",
        "    print(f\"Recall:       {rec:.5f}   (sensibility, {pos_label})\")\n",
        "    print(f\"Specificity:  {spec:.5f}   (recall of {neg_label})\")\n",
        "    print(f\"F1-score:     {f1:.5f}\")\n",
        "\n",
        "    # 3) Confusion matrix\n",
        "    labels_order = [neg_label, pos_label]\n",
        "    cm = confusion_matrix(y_val, y_pred, labels=labels_order)\n",
        "\n",
        "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "\n",
        "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
        "    print(f\"\\t\\t       Pred: {neg_label:>8} | {pos_label:>8}\")\n",
        "    print(f\"\\tTrue {neg_label:>8} | {tn:8d} | {fp:8d}\")\n",
        "    print(f\"\\tTrue {pos_label:>8} | {fn:8d} | {tp:8d}\")\n",
        "\n",
        "    # 4) Confusion matrix plot\n",
        "    if show_plots:\n",
        "        # Not normalized\n",
        "        ConfusionMatrixDisplay.from_predictions(\n",
        "            y_val,\n",
        "            y_pred,\n",
        "            display_labels=labels_order,\n",
        "            cmap=\"Blues\",\n",
        "        )\n",
        "        plt.title(\"Confusion Matrix (counts)\")\n",
        "        plt.show()\n",
        "\n",
        "        # Normalized\n",
        "        ConfusionMatrixDisplay.from_predictions(\n",
        "            y_val,\n",
        "            y_pred,\n",
        "            display_labels=labels_order,\n",
        "            cmap=\"Blues\",\n",
        "            normalize=\"true\",\n",
        "        )\n",
        "        plt.title(\"Confusion Matrix (normalized)\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoA-CYmZjSCg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sweep_model_C(\n",
        "    model_factory,\n",
        "    X_train_vector,\n",
        "    y_train,\n",
        "    X_val_vector,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    max_iter=200,\n",
        "    C_values=None,\n",
        "    model_name=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Sweep different values of the regularization parameter C for a given model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_factory : callable\n",
        "        Function that takes a value of C and returns an untrained model\n",
        "        instance (e.g., lambda C: LogisticRegression(C=C, ...)).\n",
        "    X_train_vector : array-like\n",
        "        Training feature matrix.\n",
        "    y_train : array-like\n",
        "        Training labels.\n",
        "    X_val_vector : array-like\n",
        "        Validation feature matrix.\n",
        "    y_val : array-like\n",
        "        Validation labels.\n",
        "    pos_label : hashable, default=\"jailbreak\"\n",
        "        Label considered as the positive class.\n",
        "    neg_label : hashable, default=\"benign\"\n",
        "        Label considered as the negative class (for specificity).\n",
        "    max_iter : int, default=200\n",
        "        Maximum number of iterations for the model (if applicable).\n",
        "    C_values : iterable of float, optional\n",
        "        Values of C (inverse of regularization strength) to sweep.\n",
        "        If None, defaults to np.linspace(0.1, 1.0, 19).\n",
        "    model_name : str, optional\n",
        "        Name of the model for plot titles. If None, it is inferred from the\n",
        "        first model created by model_factory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary with lists of metrics for each C:\n",
        "        {\n",
        "            \"C\": list of C values,\n",
        "            \"accuracy\": [...],\n",
        "            \"precision\": [...],\n",
        "            \"recall\": [...],\n",
        "            \"specificity\": [...],\n",
        "            \"f1\": [...],\n",
        "            \"training_time\": [...]\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    if C_values is None:\n",
        "        C_values = np.linspace(0.1, 1.0, 19)\n",
        "\n",
        "    list_c = list(C_values)\n",
        "\n",
        "    list_accuracy = []\n",
        "    list_precision = []\n",
        "    list_recall = []\n",
        "    list_specificity = []\n",
        "    list_f1 = []\n",
        "    list_training_time = []\n",
        "\n",
        "    # Infer model name if not provided\n",
        "    if model_name is None:\n",
        "        tmp_model = model_factory(list_c[0])\n",
        "        model_name = tmp_model.__class__.__name__\n",
        "\n",
        "    for c in list_c:\n",
        "        clf = model_factory(c)\n",
        "\n",
        "        t_start = time.time()\n",
        "\n",
        "        # Train on the training set\n",
        "        clf.fit(X_train_vector, y_train)\n",
        "\n",
        "        t_stop = time.time()\n",
        "\n",
        "        # Predict on the validation set\n",
        "        y_pred = clf.predict(X_val_vector)\n",
        "\n",
        "        # Metrics computed on the validation set\n",
        "        list_accuracy.append(accuracy_score(y_val, y_pred))\n",
        "        list_precision.append(\n",
        "            precision_score(y_val, y_pred, pos_label=pos_label)\n",
        "        )\n",
        "        list_recall.append(\n",
        "            recall_score(y_val, y_pred, pos_label=pos_label)\n",
        "        )\n",
        "        list_f1.append(\n",
        "            f1_score(y_val, y_pred, pos_label=pos_label)\n",
        "        )\n",
        "        # Specificity = recall on the negative class\n",
        "        list_specificity.append(\n",
        "            recall_score(y_val, y_pred, pos_label=neg_label)\n",
        "        )\n",
        "        list_training_time.append(t_stop - t_start)\n",
        "\n",
        "    # Plot results\n",
        "    fig, ax = plt.subplots(3, 2, figsize=(12, 8))\n",
        "\n",
        "    ax[0][0].plot(list_c, list_accuracy, c=\"tab:blue\")\n",
        "    ax[0][0].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[0][0].set_title(f\"Accuracy ({model_name})\")\n",
        "    ax[0][0].set_ylabel(\"Accuracy (Validation)\")\n",
        "    ax[0][0].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    ax[0][1].plot(list_c, list_precision, c=\"tab:orange\")\n",
        "    ax[0][1].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[0][1].set_title(f\"Precision ({model_name})\")\n",
        "    ax[0][1].set_ylabel(\"Precision (Validation)\")\n",
        "    ax[0][1].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    ax[1][0].plot(list_c, list_recall, c=\"tab:green\")\n",
        "    ax[1][0].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[1][0].set_title(f\"Recall ({model_name})\")\n",
        "    ax[1][0].set_ylabel(\"Recall (Validation)\")\n",
        "    ax[1][0].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    ax[1][1].plot(list_c, list_specificity, c=\"tab:olive\")\n",
        "    ax[1][1].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[1][1].set_title(f\"Specificity ({model_name})\")\n",
        "    ax[1][1].set_ylabel(\"Specificity (Validation)\")\n",
        "    ax[1][1].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    ax[2][0].plot(list_c, list_f1, c=\"tab:cyan\")\n",
        "    ax[2][0].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[2][0].set_title(f\"F1 ({model_name})\")\n",
        "    ax[2][0].set_ylabel(\"F1 (Validation)\")\n",
        "    ax[2][0].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    ax[2][1].plot(list_c, list_training_time, c=\"tab:red\")\n",
        "    ax[2][1].set_xlim([min(list_c), max(list_c) * 1.05])\n",
        "    ax[2][1].set_title(f\"Training time ({model_name})\")\n",
        "    ax[2][1].set_ylabel(\"Training time [s]\")\n",
        "    ax[2][1].set_xlabel(\"C (inverse regularization)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"C\": list_c,\n",
        "        \"accuracy\": list_accuracy,\n",
        "        \"precision\": list_precision,\n",
        "        \"recall\": list_recall,\n",
        "        \"specificity\": list_specificity,\n",
        "        \"f1\": list_f1,\n",
        "        \"training_time\": list_training_time,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDzqmZzcFxm7"
      },
      "source": [
        "we are gonna use the following plots:\n",
        "\n",
        "**Confusion matrix (counts)**\n",
        "Shows, in absolute numbers, how many benign and jailbreak prompts were correctly classified (diagonal cells)\n",
        "and how many were misclassified (off-diagonal) by Multinomial Naive Bayes on the validation set.\n",
        "\n",
        "**Confusion matrix (normalized)**\n",
        "Same matrix but row-normalized, so each row sums to 1 and the diagonal values directly show the recall for benign and for jailbreak.\n",
        "\n",
        "**Accuracy vs alpha**\n",
        "Shows how overall validation accuracy changes as you vary the Naive Bayes smoothing parameter alpha.\n",
        "\n",
        "**Precision vs alpha**\n",
        "Shows how the precision for the positive class (jailbreak) changes with alpha, i.e. how many predicted jailbreaks are actually jailbreaks.\n",
        "\n",
        "**Recall vs alpha**\n",
        "Shows how often true jailbreaks are correctly detected as jailbreaks for different values of alpha.\n",
        "\n",
        "**Specificity vs alpha**\n",
        "Shows recall of the negative class (benign) as alpha varies, i.e. how often benign prompts are correctly classified as benign.\n",
        "\n",
        "**F1 vs alpha**\n",
        "Shows the F1-score (harmonic mean of precision and recall) for jailbreak across different alpha values, summarizing the trade-off between missing and falsely flagging jailbreaks.\n",
        "\n",
        "**Training time vs alpha**\n",
        "Shows the training time of Naive Bayes for each alpha; usually fairly flat, it just confirms that sweeping alpha is computationally cheap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ErYxHIh21B"
      },
      "source": [
        "#### Naive Bayes\n",
        "\n",
        "First step is to import the class of the desired classifier and create and object with an instance of that class.\n",
        "\n",
        "[Here](https://scikit-learn.org/stable/modules/naive_bayes.html) you can take a look of the different verisions of Naive Bayes implemented in Scikit-Learn.\n",
        "We will be working with *Multinomial Naive Bayes*, [here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) find the documentation of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMH7yUGAh21C"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucbnf64Sh21D"
      },
      "source": [
        "We train the instance of the classifier on the training data using the `fit`method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dJFqYOyh21D"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEHMWS3h21G"
      },
      "source": [
        "We compute the predictions of the trained classifier on the validation data using the `fit`method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pZVt6--h21H"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_val_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YYak3Byh21H"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZnZGZehh21H"
      },
      "source": [
        "Finally we can use Scikit-Learn built-in fucntions to evaluate the model starting from the target labels and the predictions.\n",
        "We will compute the same metrics as before: accuracy, precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxEUrx4-bV2y"
      },
      "outputs": [],
      "source": [
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhEPP42gTRgS"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "nb_factory = lambda alpha: MultinomialNB(alpha=alpha)\n",
        "\n",
        "alpha_values = np.linspace(0.1, 1.0, 19)\n",
        "\n",
        "results_nb = sweep_model_C(\n",
        "    model_factory=nb_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    C_values=alpha_values,          # C_values = alpha_values\n",
        "    model_name=\"MultinomialNB\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsTt8C1DpGQe"
      },
      "source": [
        "Unfortunately the model doesn't work with negative numbers,so we can't test it on the other word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej0TVQkrh21I"
      },
      "source": [
        "#### Logistic Regression\n",
        "\n",
        "Let's try now with a different model, logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO6dlWRxh21I"
      },
      "source": [
        "Let's import the model and create an instance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxL5FEZSh21I"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX49kTNZh21I"
      },
      "source": [
        "Now we can train the instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTy0MgCfh21J"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6eCPIFh21J"
      },
      "source": [
        "and check its predictions and performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mngsF0lydys_"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_vector, y_train)\n",
        "\n",
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n",
        "\n",
        "#The normalized matrix shows about 90% recall for benign and 81% recall for jailbreak, so the model is strong on both classes,\n",
        "#slightly better at recognizing benign prompts while still catching most jailbreaks with relatively few false alarms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-OYJNvOh21K"
      },
      "source": [
        "You can play with the hyperparameters of the model, looking for the best configuration.\n",
        "\n",
        "The complete list of parameters can be found on the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_jHQsCnh21K"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-YTXcBph21L"
      },
      "source": [
        "How do performances change with the new parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wai0hVRIb_A_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n",
        "\n",
        "#The normalized matrix shows about 89% recall for benign and 82% recall for jailbreak, very similar balance to before but with higher overall accuracy,\n",
        "#precision, and F1. So tuning the hyperparameters gives a small but consistent performance gain without changing the error pattern much.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnO3ViCM3JI-"
      },
      "source": [
        "#####continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCs5y8Ahe5_l"
      },
      "outputs": [],
      "source": [
        "list_c = np.linspace(0.1, 1.0, 19)\n",
        "list_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvGK_zrZe_gM"
      },
      "source": [
        "At each iteration we are going to append the newest result in these lists, at the end we will plot all the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1Go_yhe_gM"
      },
      "source": [
        "Iterate over the values of `C` and fit each time a new classifier on the training set, then register the results on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhqhsC_5e_gM"
      },
      "source": [
        "Plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Mv5Ftbj06x"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg_factory = lambda C: LogisticRegression(\n",
        "    max_iter=200,\n",
        "    penalty=\"l2\",\n",
        "    C=C,\n",
        "    solver=\"liblinear\",\n",
        ")\n",
        "\n",
        "results_logreg = sweep_model_C(\n",
        "    model_factory=logreg_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "#Here C is the inverse regularization for logistic regression on the BoW features:\n",
        "\n",
        "#Accuracy – Gently increases with C, peaking around 0.86–0.865, so a bit less regularization helps.\n",
        "#Precision – Also rises with C, meaning higher C reduces false positives on jailbreak.\n",
        "#Recall – Gradually improves as C grows, so the model catches slightly more jailbreaks too.\n",
        "#Specificity – Already high and creeps up to ~0.90, so benign prompts are very well preserved across C.\n",
        "#F1 – Follows precision/recall and steadily increases, with the best values at the largest C tried.\n",
        "#Training time – Stays low overall, with some small bumps as C increases, but nothing problematic computationally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exj2gvWdTZkR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XABkBveV296W"
      },
      "source": [
        "#####different word embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3kQDQSLm1WO"
      },
      "outputs": [],
      "source": [
        "logreg_models_default = {}  # per salvare i modelli logreg (default) per ogni embedding\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 LogisticRegression (parametri default) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # 1) standard model\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    logreg_models_default[emb_name] = clf\n",
        "\n",
        "    # 2) evaluation\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "    )\n",
        "\n",
        "#For w2v, logistic regression reaches about 0.72 accuracy with moderate precision and recall on jailbreak; it misses many jailbreaks (109) and also mislabels 100 benign prompts.\n",
        "\n",
        "#With glove_twitter, accuracy and specificity improve slightly, but recall on jailbreak drops a bit, so the model is a bit more conservative and prefers to call things benign.\n",
        "\n",
        "#With glove_wiki, performance is very close to glove_twitter, with a small gain in recall and F1; overall behaviour is almost the same as Twitter GloVe.\n",
        "\n",
        "#With fasttext, there is a clear improvement: accuracy, precision, recall and F1 all increase, and missed jailbreaks fall to 76 while benign errors stay similar, giving a better balance.\n",
        "\n",
        "#With sbert, results are the best: highest accuracy (0.83), higher precision and recall on jailbreak, and better specificity on benign, meaning it detects more jailbreaks while making fewer false alarms on benign prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXGU5K2im4W1"
      },
      "outputs": [],
      "source": [
        "C_values = list_c\n",
        "\n",
        "results_logreg_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep C per LogisticRegression su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # Factory for LogisticRegression\n",
        "    logreg_factory = lambda C: LogisticRegression(\n",
        "        C=C,\n",
        "        max_iter=1000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    results_lr = sweep_model_C(\n",
        "        model_factory=logreg_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=C_values,\n",
        "        model_name=f\"LogisticRegression_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_logreg_sweep[emb_name] = results_lr\n",
        "\n",
        "#For w2v, changing C only slightly moves the curves: metrics stay relatively low and fairly flat, so tuning regularization cannot fix the weaker embedding.\n",
        "\n",
        "#For glove_twitter, the scores are a bit higher than w2v, but again the curves are smooth with only small bumps; a wide range of C values gives similar mid-level performance.\n",
        "\n",
        "#For glove_wiki, behaviour is very similar to glove_twitter: modest improvements over w2v, no sharp optimum in C, and only mild variation in accuracy, recall and F1.\n",
        "\n",
        "#For fasttext, all the metric curves sit clearly above the previous ones; there is a broad region of C where accuracy,\n",
        "#recall and F1 are high and stable, so FastText plus logistic regression works well without needing very precise tuning.\n",
        "\n",
        "#For sbert, the curves are highest and quite flat across C, showing that SBERT embeddings give the best performance and that logistic regression remains robust even if C is not perfectly optimized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meoz9otNQNdE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Q-1oYsh21M"
      },
      "source": [
        "#### Support Vector Machine\n",
        "\n",
        "Let's try now with a different type of classifier, a support vector machine.\n",
        "As before, we import the corresponding class and we create an instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lx5P8Beh21M",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(C=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS_o741Dh21M"
      },
      "source": [
        "Once we have the instance of our SVM we can fit it on the training data.\n",
        "\n",
        "**NOTE**: we must limit to a subset of the words otherwise the algorithm won't fit the model in a reasonable amount of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6r-26ldh21N"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZfz2Gbxh21N"
      },
      "source": [
        "And we can evaluate it on the validation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplUmJxnh21N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n",
        "\n",
        "#This model shows solid, fairly balanced performance:\n",
        "#It correctly classifies most benign prompts (377) and mislabels 73 as jailbreak.\n",
        "#For jailbreak, it correctly detects 243 and misses 57.\n",
        "#The normalized matrix shows about 84% recall for benign and 81% recall for jailbreak, so the classifier is slightly better\n",
        "#at recognizing benign prompts but still identifies the majority of jailbreak attempts with relatively few false alarms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfDTUc1kh21O"
      },
      "outputs": [],
      "source": [
        "clf = SVC(C=0.1)\n",
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEGpY_GLh21O"
      },
      "source": [
        "As usual we compute the predictions on the validation set and we use them to evaluate the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGWY3_Wnh21O"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n",
        "\n",
        "#Accuracy is about 0.74 with F1 around 0.68, so overall performance is moderate.\n",
        "#For benign prompts, 348 are correctly classified while 102 are wrongly flagged as jailbreak (about 77% recall on benign).\n",
        "#For jailbreak, 206 are correctly detected but 94 are missed (about 69% recall), so almost one third of jailbreaks are still predicted as benign.\n",
        "#The normalized matrix highlights this imbalance: the model is noticeably less reliable on jailbreak than on benign,\n",
        "#and generates more false negatives than in the better-performing setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfJGDWWJUglj"
      },
      "outputs": [],
      "source": [
        "clf = SVC(C=0.9)\n",
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2hXfOriUglj"
      },
      "source": [
        "As usual we compute the predictions on the validation set and we use them to evaluate the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnTLrynHUglj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n",
        "\n",
        "#Here the SVM with C = 0.9 is performing quite well:\n",
        "#Accuracy is about 0.84 and F1 around 0.80, so overall performance is strong.\n",
        "#For benign prompts, 388 are correctly classified and 62 are wrongly flagged as jailbreak (about 86% recall on benign).\n",
        "#For jailbreak prompts, 241 are correctly detected and 59 are missed (about 80% recall), which is better balance than in the previous SVM runs.\n",
        "#The normalized confusion matrix shows both classes are handled fairly symmetrically, with only a modest fraction of false positives and false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NYMOYJXlzq3"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc_factory = lambda C: SVC(\n",
        "    C=C,\n",
        "    kernel=\"linear\",\n",
        "    probability=False\n",
        ")\n",
        "\n",
        "\n",
        "results_svc = sweep_model_C(\n",
        "    model_factory=svc_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "#This sweep shows how the linear SVC behaves as C changes:\n",
        "\n",
        "#Accuracy gently fluctuates around 0.85 and peaks for moderate C (around 0.25–0.35); larger C values do not bring clear gains.\n",
        "#Precision also reaches its highest point in the same C range, then slightly decreases as C increases, meaning more false positives for high C.\n",
        "#Recall is quite stable, with a small improvement at medium C and then a plateau, so sensitivity to jailbreaks doesn’t change much.\n",
        "#Specificity slowly decreases with larger C, so the model becomes a bit worse at correctly recognising benign prompts when C is high.\n",
        "#F1 follows the same pattern as accuracy and precision, suggesting that moderate regularisation (C ≈ 0.25–0.35) gives the best overall balance.\n",
        "#Training time is low but somewhat erratic across C, with a few spikes; there is no strong reason to choose a very large C from a computational point of view either.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdnZEH5Uh21O"
      },
      "source": [
        "How did the model performed on the training data?\n",
        "We can compute the predictions on the training set too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26QuOaHuh21P"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_train_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HapOJzMGh21P"
      },
      "source": [
        "Then we compute the metrics on the new predictions using the target labels of the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDEGAJUgh21P"
      },
      "outputs": [],
      "source": [
        "y_pred_train = clf.predict(X_train_vector)\n",
        "\n",
        "print(f\"Accuracy (train):  {accuracy_score(y_train, y_pred_train):.5f}\")\n",
        "print(f\"Precision (train): {precision_score(y_train, y_pred_train, pos_label='jailbreak'):.5f}\")\n",
        "print(f\"Recall (train):    {recall_score(y_train, y_pred_train, pos_label='jailbreak'):.5f}\")\n",
        "\n",
        "\n",
        "# Specificity = recall of the negative class ('benign')\n",
        "specificity = recall_score(y_train, y_pred, pos_label='benign')\n",
        "print(f\"Specificity: {specificity:.5f}\")\n",
        "\n",
        "print(f\"F1-score:  {f1_score(y_train, y_pred, pos_label='jailbreak'):.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSEJs6zeh21P"
      },
      "source": [
        "- if the performance is poor both on the training set and the test set, it is a case of **UNDERFITTING**.\n",
        "- if the performance is poor on the test set but good on the training set, it is a case of **OVERFITTING**. (Basically, the model is not able to generalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq2PsMyO8rNC"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc_models_default = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 SVC (parametri default) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # 1) standard SVC\n",
        "    clf = SVC(\n",
        "        C=0.5,\n",
        "        kernel=\"rbf\",\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    svc_models_default[emb_name] = clf\n",
        "\n",
        "    # 2) evaluation\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n",
        "\n",
        "    #For SVC with w2v embeddings, performance is reasonably strong: around 0.81 accuracy and F1≈0.77. Recall for jailbreaks is slightly\n",
        "#higher than precision, so the model catches most jailbreaks but still mislabels some benign prompts as malicious.\n",
        "\n",
        "#With glove_twitter, metrics are very similar but a bit worse than w2v: slightly lower accuracy, precision, recall, and F1.\n",
        "#It still behaves in a balanced way, but w2v has a small edge.\n",
        "\n",
        "#With glove_wiki, accuracy and F1 drop further. Recall on jailbreaks falls to 0.72, even if specificity stays good,\n",
        "#meaning the model is more conservative and misses more jailbreaks than the previous two.\n",
        "\n",
        "#FastText improves over all the GloVe variants and w2v: accuracy rises to about 0.83 and F1 to ~0.79. Recall on jailbreaks (0.83)\n",
        "#is high while keeping similar specificity, so it is better at detecting attacks without sacrificing too many benign examples.\n",
        "\n",
        "#SBERT again is the best representation: accuracy reaches about 0.88 with the highest precision, recall, and F1 (~0.85). The confusion matrix shows\n",
        "#few mistakes on both classes, with high recall for jailbreaks and very strong specificity for benign prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nx1FiTP8rNJ"
      },
      "outputs": [],
      "source": [
        "# C_values viene da list_c, come prima\n",
        "C_values = list_c\n",
        "\n",
        "results_svc_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep C per SVC su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # Factory for SVC\n",
        "    svc_factory = lambda C: SVC(\n",
        "        C=C,\n",
        "        kernel=\"rbf\",\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    results_svc = sweep_model_C(\n",
        "        model_factory=svc_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=C_values,\n",
        "        model_name=f\"SVC_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_svc_sweep[emb_name] = results_svc\n",
        "\n",
        "    #For the SVC sweeps, each block explores how the regularization parameter C affects performance for a different embedding (w2v, GloVe Twitter, GloVe Wiki, FastText, SBERT).\n",
        "\n",
        "#Across embeddings, the curves show that changing C slightly shifts the trade-off between precision, recall, and specificity, but there is no dramatic instability:\n",
        "#performance usually stays in a fairly narrow band. Medium C values tend to give a good balance, while very small C underfits and very large C bring only marginal gains or small drops.\n",
        "#As with the default runs, the SBERT and FastText sweeps reach the highest overall metric levels,\n",
        "#confirming that these semantic embeddings are the most effective feature representations for the SVC classifier in this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drcZXzmCE1ej"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwJcwJ7pkwOi"
      },
      "source": [
        "Let's try to see if we can discover different meaninful structures of the data by using clustering alogorithms.\n",
        "We will:\n",
        "- Apply different clustering algorithms.\n",
        "- Visualize clusters after dimensionality reduction (PCA, UMAP, t-SNE).\n",
        "- Evaluate the quality of the clusters with internal and external metrics.\n",
        "- Explore how the choice of **number of clusters** or **eps** might affects the metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkJFPAODbrql"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oBtTgLwbrql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from scipy.stats import uniform, randint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgDjv1Fibrqm"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqwf3vGobrqm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJlVfWwNbrqn"
      },
      "source": [
        "Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la6TH24qbrqo"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import (\n",
        "    KMeans,\n",
        "    AgglomerativeClustering,\n",
        "    DBSCAN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCN3AGrlbrqo"
      },
      "source": [
        "Clustering metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "521tvcRpbrqo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score,\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYK9q4uCbrqx"
      },
      "source": [
        "### k-means Clustering\n",
        "\n",
        "Now we apply the actual clustering and observe the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRMF2Z37brqx"
      },
      "source": [
        "#### K-Means\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZTsPihpbrqx"
      },
      "source": [
        "Fit the clustering algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POdFIdLObrqx"
      },
      "outputs": [],
      "source": [
        "# Register start time\n",
        "t_start = time.time()\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=random_seed)\n",
        "\n",
        "kmeans.fit(X_train_vector)\n",
        "labels_kmeans = kmeans.labels_\n",
        "\n",
        "\n",
        "# Register end time\n",
        "t_stop = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnw_FTl1brqy"
      },
      "outputs": [],
      "source": [
        "print(f\"Elapsed time: {t_stop - t_start:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MCoRPpbrqy"
      },
      "source": [
        "We can print the centers of the clusters (which are, in this case, two high-dimensional points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9wiTBJUbrqy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0qgNaaqbrqy"
      },
      "source": [
        "We can see the cluster labels.\n",
        "These labels represent the ID of the clusters found with K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBHhjGIUbrqy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FybFkWPfbrqy"
      },
      "outputs": [],
      "source": [
        "len(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWe0H-skbrqy"
      },
      "source": [
        "We can store these labels in a new column of the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x1b53gobrqy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "text_train_strings = df.loc[idx_train, \"text\"]\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    \"text\": text_train_strings,\n",
        "    \"true_label\": y_train,\n",
        "    \"cluster\": kmeans.labels_\n",
        "})\n",
        "\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI-d_3MBbrqy"
      },
      "source": [
        "#### Clustering results visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0pc8Taqbrqy"
      },
      "source": [
        "##### Scatter plot with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtlcJ7XDUBxh"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA requires a dense matrix\n",
        "X_train_dense = X_train_vector.toarray()\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_train_2D = pca.fit_transform(X_train_dense)\n",
        "\n",
        "print(\"Shape after PCA:\", X_train_2D.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW4UYQVvUKmx"
      },
      "outputs": [],
      "source": [
        "colors = ['red', 'blue']  #a different color for each cluster\n",
        "cluster_colors = [colors[c] for c in train_df[\"cluster\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1QfW6x_H-ud"
      },
      "outputs": [],
      "source": [
        "#plot\n",
        "markers = {'benign': 'o', 'jailbreak': 'x'}\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for t in ['benign', 'jailbreak']:\n",
        "    idx = [i for i, lbl in enumerate(y_train) if lbl == t]\n",
        "    plt.scatter(\n",
        "        X_train_2D[idx, 0],\n",
        "        X_train_2D[idx, 1],\n",
        "        s=20,\n",
        "        alpha=0.7,\n",
        "        c=[cluster_colors[i] for i in idx],\n",
        "        marker=markers[t],\n",
        "        label=t\n",
        "    )\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(\"Clusters as Colors, True Labels as Markers\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49tQk0N-KHcT"
      },
      "source": [
        "the clusters don't look great with PCA, let's try a different technique to reduce dimensions: UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uOc_KrvKL4z"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn\n",
        "\n",
        "import umap.umap_ as umap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOIK7LJJKhTA"
      },
      "outputs": [],
      "source": [
        "import umap.umap_ as umap\n",
        "\n",
        "umap_2d = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_2D = umap_2d.fit_transform(X_train_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5BYCybPKWtR"
      },
      "outputs": [],
      "source": [
        "#we will use the same graphical conventions\n",
        "markers = {'benign': 'o', 'jailbreak': 'x'}\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for t in ['benign', 'jailbreak']:\n",
        "    idx = [i for i, lbl in enumerate(y_train) if lbl == t]\n",
        "    plt.scatter(\n",
        "        X_train_2D[idx, 0],\n",
        "        X_train_2D[idx, 1],\n",
        "        s=20,\n",
        "        alpha=0.7,\n",
        "        c=[cluster_colors[i] for i in idx],\n",
        "        marker=markers[t],\n",
        "        label=t\n",
        "    )\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.title(\"Clusters as Colors, True Labels as Markers\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOz-Q7ernzQo"
      },
      "source": [
        "here the 2 clusters are more distinct, but they don't seem to be related to the \"jailbreak\" or \"benign\" label at all!\n",
        "\n",
        "perhaps our data representation is just too high-dimensional to be understood with a simple graph in the Cartesian Plane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1tHMLUhbrqz"
      },
      "source": [
        "##### t-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz_jlJIlbrqz"
      },
      "source": [
        "A possible solution to the high-dimensionality problem is **t-SNE** (t-distributed Stochastic Neighbor Embedding) and algorithm for dimensionality reduction.\n",
        "We can use it to better visualize the clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJgszyUbrqz"
      },
      "source": [
        "Import the `TSNE` object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sneqeo0brqz"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n0CYLWEbrqz"
      },
      "source": [
        "Fit the embedding model on the data and transform them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRayGcmkbrq0"
      },
      "outputs": [],
      "source": [
        "X_dense = X_train_vector.toarray()\n",
        "\n",
        "X_embedded = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=20,\n",
        "    random_state=42\n",
        ").fit_transform(X_dense)\n",
        "\n",
        "print(X_embedded.shape)  # (n_samples, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkaRUPabbrq0"
      },
      "source": [
        "Now look at the shape of the transformed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGeY9pDHbrq0"
      },
      "outputs": [],
      "source": [
        "X_embedded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3mukk_brq0"
      },
      "source": [
        "Now we can try visualising the results of the clustering process using the downprojected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKVp72uebrq0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# colors for K-Means clusters\n",
        "colors = ['red', 'blue']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
        "\n",
        "# ---- Plot 1: K-Means clustering ----\n",
        "ax[0].set_title(\"K-Means clustering (k = 2)\")\n",
        "ax[0].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=[colors[idx] for idx in train_df['cluster']]\n",
        ")\n",
        "ax[0].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[0].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "# ---- Plot 2: True labels (benign vs jailbreak) ----\n",
        "ax[1].set_title(\"Benign vs. jailbreak samples\")\n",
        "ax[1].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=['g' if lbl == 'benign' else 'r' for lbl in train_df['true_label']]\n",
        ")\n",
        "ax[1].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[1].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHwFz-DprSm"
      },
      "source": [
        "let's look at some of the points in the red cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7J8_SkKpRfg"
      },
      "outputs": [],
      "source": [
        "# Filter only the samples that belong to cluster 0 (red cluster in the t-SNE plot)\n",
        "cluster_red_df = train_df[train_df[\"cluster\"] == 0]\n",
        "\n",
        "print(f\"Number of samples in the red cluster (cluster 0): {len(cluster_red_df)}\\n\")\n",
        "\n",
        "# Select up to 20 examples from this cluster\n",
        "# You can use .head(20) to take the first ones, or .sample(...) to pick them randomly\n",
        "subset = cluster_red_df.head(20)   # or: cluster_red_df.sample(n=min(20, len(cluster_red_df)), random_state=42)\n",
        "\n",
        "# Loop over the selected samples and print some useful information\n",
        "for i, row in subset.iterrows():\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Index:      {i}\")                 # original index in the training set\n",
        "    print(f\"Cluster:    {row['cluster']}\")    # should be 0 (red cluster)\n",
        "    print(f\"True label: {row['true_label']}\") # 'benign' or 'jailbreak'\n",
        "    print(\"Text:\")\n",
        "    print(row[\"text\"])                        # the original text associated with this sample\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrjuVRZsHZsF"
      },
      "source": [
        "**very interesting!** it seems that using a \"multiple choice quiz\" approach is a good way of jailbreaking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-xDDgc2brq0"
      },
      "source": [
        "#### Evaluation\n",
        "\n",
        "now we will evaluate the clustering using some commonly used metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7tToED8MDqY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "\n",
        "# Metrics\n",
        "homogeneity = homogeneity_score(train_df['true_label'], train_df['cluster'])\n",
        "completeness = completeness_score(train_df['true_label'], train_df['cluster'])\n",
        "vmeasure = v_measure_score(train_df['true_label'], train_df['cluster'])\n",
        "\n",
        "print(\"Homogeneity:\", homogeneity)\n",
        "print(\"Completeness:\", completeness)\n",
        "print(\"V-measure:\", vmeasure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsXVcx0RN6n6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score,\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score\n",
        ")\n",
        "\n",
        "# Metrics: homogeneity, completeness, v-measure\n",
        "homogeneity = homogeneity_score(train_df['true_label'], train_df['cluster'])\n",
        "completeness = completeness_score(train_df['true_label'], train_df['cluster'])\n",
        "vmeasure = v_measure_score(train_df['true_label'], train_df['cluster'])\n",
        "\n",
        "print(\"Homogeneity:\", homogeneity)\n",
        "print(\"Completeness:\", completeness)\n",
        "print(\"V-measure:\", vmeasure)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Extra metrics: Silhouette & Calinski-Harabasz\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "print(f\"Silhouette score      = {silhouette_score(X_train_vector, train_df['cluster']):.4f}\")\n",
        "print(f\"Calinski-Harabasz     = {calinski_harabasz_score(X_dense, train_df['cluster']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdYEaKoUbrq5"
      },
      "source": [
        "**Elbow method** with the **homogeneity score**\n",
        "\n",
        "We iterate over different values of the `n_clusters` parameter to collect the corresponding homogeneity score values, we plot these value pairs and we look for the elbow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urqWFkR-brq5"
      },
      "outputs": [],
      "source": [
        "n_clusters = list(range(2, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dimRGoVmbrq5"
      },
      "source": [
        "Accumulator for the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYpVnq4vbrq5"
      },
      "outputs": [],
      "source": [
        "homogeneity_values = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_JCUrXbrq5"
      },
      "source": [
        "Iteratively compute the scores for the different `n_clusters` values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQa5wZ5bbrq5"
      },
      "outputs": [],
      "source": [
        "for k in n_clusters:\n",
        "    # Register start time\n",
        "    t_start = time.time()\n",
        "\n",
        "    # Fit K-Means on the vectorized data\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        random_state=random_seed\n",
        "    ).fit(X_train_vector)          # <-- usa X_train_vector, non df\n",
        "\n",
        "    # Compute homogeneity between true labels and clusters\n",
        "    homogeneity_values.append(\n",
        "        homogeneity_score(y_train, kmeans.labels_)\n",
        "    )\n",
        "\n",
        "    # Register end time\n",
        "    t_stop = time.time()\n",
        "\n",
        "    # Print elapsed time for this k\n",
        "    print(f\"Elapsed time: {t_stop - t_start:.5f} seconds (k = {k})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7322r6Ncbrq5"
      },
      "source": [
        "Plot the collected scores as function of the `n_clusters` hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU-MS4s6brq6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(n_clusters, homogeneity_values, marker='o')\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Homogeneity\")\n",
        "plt.title(\"Homogeneity vs k for K-Means on text vectors\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHXkJYVEbrq7"
      },
      "source": [
        "**Elbow method** with the **Calinski-Harabasz index score**\n",
        "\n",
        "We iterate over different values of the `n_clusters` parameter to collect the corresponding C-H index score values, we plot these value pairs and we look for the elbow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL5oYMQIbrq7"
      },
      "outputs": [],
      "source": [
        "n_clusters = list(range(2, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0LJvIfFbrq7"
      },
      "source": [
        "Accumulator for the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joX1ospcbrq7"
      },
      "outputs": [],
      "source": [
        "ch_values = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3UenF6vbrq7"
      },
      "source": [
        "Iteratively compute the scores for the different `n_clusters` values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kH3cQLfbrq7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "\n",
        "# Convert sparse → dense\n",
        "#X_dense = X_train_vector.toarray()\n",
        "\n",
        "\n",
        "\n",
        "for k in n_clusters:\n",
        "    t_start = time.time()\n",
        "\n",
        "    # Fit KMeans su vettori densi\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        random_state=random_seed\n",
        "    ).fit(X_dense)\n",
        "\n",
        "    # Calinski-Harabasz score\n",
        "    ch_values.append(calinski_harabasz_score(X_dense, kmeans.labels_))\n",
        "\n",
        "    t_stop = time.time()\n",
        "    print(f\"Elapsed time: {t_stop - t_start:.5f} seconds (k = {k})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b8-TIC8brq7"
      },
      "source": [
        "Plot the collected scores as function of the `n_clusters` hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqqPV3MAbrq7"
      },
      "outputs": [],
      "source": [
        "plt.plot(n_clusters, ch_values, marker='o')\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Calinski-Harabasz score\")\n",
        "plt.title(\"CH score vs k for K-Means\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suupMKzqrju9"
      },
      "outputs": [],
      "source": [
        "# Best k according to CH\n",
        "best_k = n_clusters[int(np.argmax(ch_values))]\n",
        "print(\"➡️ Best k according to CH:\", best_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CHVDgfTsKuN"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 2) Final KMeans with best_k\n",
        "# ============================\n",
        "kmeans_best = KMeans(\n",
        "    n_clusters=best_k,\n",
        "    random_state=random_seed\n",
        ").fit(X_dense)\n",
        "\n",
        "df_plot = train_df.copy()\n",
        "df_plot[\"cluster\"] = kmeans_best.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NxzU6TNsTwm"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 3) PCA 2D visualization\n",
        "# ============================\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_train_2D_pca = pca.fit_transform(X_dense)\n",
        "\n",
        "print(\"Shape after PCA:\", X_train_2D_pca.shape)\n",
        "\n",
        "# One color per cluster\n",
        "# (make sure length of 'colors' is >= best_k)\n",
        "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "cluster_colors = [colors[c] for c in df_plot[\"cluster\"]]\n",
        "\n",
        "# Markers for true labels\n",
        "markers = {'benign': 'o', 'jailbreak': 'x'}\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for t in ['benign', 'jailbreak']:\n",
        "    # indices where true label == t\n",
        "    idx = [i for i, lbl in enumerate(y_train) if lbl == t]\n",
        "    plt.scatter(\n",
        "        X_train_2D_pca[idx, 0],\n",
        "        X_train_2D_pca[idx, 1],\n",
        "        s=20,\n",
        "        alpha=0.7,\n",
        "        c=[cluster_colors[i] for i in idx],\n",
        "        marker=markers[t],\n",
        "        label=t\n",
        "    )\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.title(f\"PCA – K-Means (k={best_k}), Clusters as Colors, True Labels as Markers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXVrY0gqsZJl"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 4) UMAP 2D visualization\n",
        "# ============================\n",
        "umap_2d = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# UMAP can work directly with the sparse matrix\n",
        "X_train_2D_umap = umap_2d.fit_transform(X_train_vector)\n",
        "\n",
        "markers = {'benign': 'o', 'jailbreak': 'x'}\n",
        "# Reuse same color mapping\n",
        "cluster_colors = [colors[c] for c in df_plot[\"cluster\"]]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for t in ['benign', 'jailbreak']:\n",
        "    idx = [i for i, lbl in enumerate(y_train) if lbl == t]\n",
        "    plt.scatter(\n",
        "        X_train_2D_umap[idx, 0],\n",
        "        X_train_2D_umap[idx, 1],\n",
        "        s=20,\n",
        "        alpha=0.7,\n",
        "        c=[cluster_colors[i] for i in idx],\n",
        "        marker=markers[t],\n",
        "        label=t\n",
        "    )\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.title(f\"UMAP – K-Means (k={best_k}), Clusters as Colors, True Labels as Markers\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1v9J68lsaBE"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 5) t-SNE 2D visualization\n",
        "# ============================\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_embedded = tsne.fit_transform(X_dense)\n",
        "print(\"Shape after t-SNE:\", X_embedded.shape)\n",
        "\n",
        "colors_tsne = ['red', 'blue', 'green', 'purple', 'orange']\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
        "\n",
        "# ---- Plot 1: K-Means clustering in t-SNE space ----\n",
        "ax[0].set_title(f\"K-Means clustering (k = {best_k}) – t-SNE\")\n",
        "ax[0].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=[colors_tsne[idx] for idx in df_plot['cluster']]\n",
        ")\n",
        "ax[0].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[0].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "# ---- Plot 2: True labels in t-SNE space ----\n",
        "ax[1].set_title(\"Benign vs. jailbreak samples – t-SNE\")\n",
        "ax[1].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=['g' if lbl == 'benign' else 'r' for lbl in df_plot['true_label']]\n",
        ")\n",
        "ax[1].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[1].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFv4oY11brq7"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "<b>Q: Repeat the analysis with the elbow method using the silhouette score</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjo-8qlDbrq7"
      },
      "source": [
        "We iterate over different values of the `n_clusters` parameter to collect the corresponding silhouette score values, we plot these value pairs and we look for the elbow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORbZBrbJbrq7"
      },
      "outputs": [],
      "source": [
        "n_clusters = list(range(2, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdN9PdtTbrq7"
      },
      "source": [
        "Accumulator for the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6mAXLoRbrq7"
      },
      "outputs": [],
      "source": [
        "silhouette_values = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CiURIiIbrq7"
      },
      "source": [
        "Iteratively compute the scores for the different `n_clusters` values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ8RZMnebrq7"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for k in n_clusters:\n",
        "    # Register start time\n",
        "    t_start = time.time()\n",
        "\n",
        "    # Fit clustering algorithm on your vectorized text data\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        random_state=random_seed\n",
        "    ).fit(X_train_vector)\n",
        "\n",
        "    # Compute Silhouette score for the current number of clusters\n",
        "    score = silhouette_score(X_train_vector, kmeans.labels_)\n",
        "    silhouette_values.append(score)\n",
        "\n",
        "    # Register end time\n",
        "    t_stop = time.time()\n",
        "\n",
        "    # Print elapsed time\n",
        "    print(f\"Elapsed time: {t_stop - t_start:.5f} seconds (k: {k:2d}), Silhouette={score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DPfT2fBbrq7"
      },
      "source": [
        "Plot the collected scores as function of the `n_clusters` hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_AYKGfkbrq7"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "\n",
        "plt.plot(n_clusters, silhouette_values, marker='o')  # ← QUI\n",
        "\n",
        "plt.title('K-Means Silhouette score for varying number of clusters')\n",
        "plt.xlabel('No. of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bBDDtqHJS3i"
      },
      "source": [
        "let's plot with 24 and 25 clusters since the graph changes its trend there dramatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VllnXDEYJRLB"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Make sure X_dense and df_plot (with column 'true_label') already exist\n",
        "\n",
        "# ============================\n",
        "# 0) t-SNE 2D visualization (computed once)\n",
        "# ============================\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_embedded = tsne.fit_transform(X_dense)\n",
        "print(\"Shape after t-SNE:\", X_embedded.shape)\n",
        "\n",
        "# ============================\n",
        "# 1) Loop over k = 24 and k = 25\n",
        "# ============================\n",
        "for k_clusters in [24, 25]:\n",
        "    print(f\"\\n=== K-Means with k = {k_clusters} ===\")\n",
        "\n",
        "    kmeans_k = KMeans(\n",
        "        n_clusters=k_clusters,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "    cluster_labels_k = kmeans_k.fit_predict(X_dense)\n",
        "\n",
        "    # Save (overwrite) cluster assignment for plotting\n",
        "    df_plot[\"cluster\"] = cluster_labels_k\n",
        "\n",
        "    print(f\"Cluster counts (k={k_clusters}):\", np.bincount(cluster_labels_k))\n",
        "\n",
        "    # Build a colormap with k distinct colors\n",
        "    cmap = plt.cm.get_cmap(\"tab20\", k_clusters)\n",
        "    colors_tsne = [cmap(i) for i in range(k_clusters)]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
        "\n",
        "    # ---- Plot 1: K-Means clustering in t-SNE space ----\n",
        "    ax[0].set_title(f\"K-Means clustering (k = {k_clusters}) – t-SNE\")\n",
        "    ax[0].scatter(\n",
        "        X_embedded[:, 0],\n",
        "        X_embedded[:, 1],\n",
        "        s=5,\n",
        "        alpha=0.7,\n",
        "        c=[colors_tsne[idx] for idx in df_plot[\"cluster\"]]\n",
        "    )\n",
        "    ax[0].set_xlabel(\"t-SNE dim 1\")\n",
        "    ax[0].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "    # ---- Plot 2: True labels in t-SNE space ----\n",
        "    ax[1].set_title(\"Benign vs. jailbreak samples – t-SNE\")\n",
        "    ax[1].scatter(\n",
        "        X_embedded[:, 0],\n",
        "        X_embedded[:, 1],\n",
        "        s=5,\n",
        "        alpha=0.7,\n",
        "        c=[\"g\" if lbl == \"benign\" else \"r\" for lbl in df_plot[\"true_label\"]]\n",
        "    )\n",
        "    ax[1].set_xlabel(\"t-SNE dim 1\")\n",
        "    ax[1].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Z5pPTkrju5"
      },
      "source": [
        "Unfortunately we didn't manage to identify any other meaningful clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0mbxJDNVJzv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score,\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score\n",
        ")\n",
        "\n",
        "def evaluate_clustering(\n",
        "    X,\n",
        "    labels_pred,\n",
        "    y_true=None,\n",
        "    compute_ch=True,\n",
        "    name=\"Clustering Evaluation\"\n",
        "):\n",
        "    \"\"\"\n",
        "    General evaluation function for clustering algorithms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like or sparse matrix\n",
        "        Feature matrix (vectorized text in your case).\n",
        "    labels_pred : array-like\n",
        "        Cluster labels predicted by the clustering algorithm.\n",
        "    y_true : array-like or None\n",
        "        Ground truth labels. If None, external metrics are skipped.\n",
        "    compute_ch : bool\n",
        "        Whether to compute Calinski-Harabasz score (requires dense array).\n",
        "    name : str\n",
        "        Title for printing results.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # External metrics (require ground truth)\n",
        "    # -----------------------------------------------------------\n",
        "    if y_true is not None:\n",
        "        h = homogeneity_score(y_true, labels_pred)\n",
        "        c = completeness_score(y_true, labels_pred)\n",
        "        v = v_measure_score(y_true, labels_pred)\n",
        "\n",
        "        print(f\"Homogeneity:        {h:.5f}\")\n",
        "        print(f\"Completeness:       {c:.5f}\")\n",
        "        print(f\"V-measure:          {v:.5f}\")\n",
        "    else:\n",
        "        print(\"Ground truth not provided: skipping homogeneity/completeness/v-measure.\\n\")\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # Internal metrics\n",
        "    # -----------------------------------------------------------\n",
        "    try:\n",
        "        sil = silhouette_score(X, labels_pred)\n",
        "        print(f\"Silhouette score:   {sil:.5f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Silhouette score failed: {e}\")\n",
        "\n",
        "    if compute_ch:\n",
        "        try:\n",
        "            # convert to dense\n",
        "            X_dense = X.toarray() if hasattr(X, \"toarray\") else X\n",
        "            ch = calinski_harabasz_score(X_dense, labels_pred)\n",
        "            print(f\"Calinski-Harabasz: {ch:.5f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"CH score failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug0a4dsFsad2"
      },
      "source": [
        "let's define the function elbow_method, a general-purpose tool designed to evaluate clustering quality across a range of possible numbers of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3mGfih1XjM9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score,\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score\n",
        ")\n",
        "\n",
        "def elbow_method(\n",
        "    X,\n",
        "    clusterer_factory,\n",
        "    k_values,\n",
        "    metric=\"silhouette\",\n",
        "    y_true=None,\n",
        "    title=None,\n",
        "    to_dense_for_ch=True,\n",
        "    x_label=\"Number of clusters (k)\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generic elbow-method helper for clustering.\n",
        "    \"\"\"\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    # Precompute dense if necessary for CH\n",
        "    if metric == \"calinski_harabasz\":\n",
        "        if to_dense_for_ch and hasattr(X, \"toarray\"):\n",
        "            X_for_ch = X.toarray()\n",
        "        else:\n",
        "            X_for_ch = X\n",
        "    else:\n",
        "        X_for_ch = None  # not used\n",
        "\n",
        "    print(f\"\\n=== Elbow method using metric: {metric} ===\")\n",
        "\n",
        "    for k in k_values:\n",
        "        t_start = time.time()\n",
        "\n",
        "        # create and fit the clusterer\n",
        "        clusterer = clusterer_factory(k)\n",
        "        clusterer.fit(X)\n",
        "\n",
        "        labels_pred = clusterer.labels_\n",
        "        t_stop = time.time()\n",
        "\n",
        "        # choose metric\n",
        "        if metric == \"silhouette\":\n",
        "            try:\n",
        "                score = silhouette_score(X, labels_pred)\n",
        "            except Exception as e:\n",
        "                print(f\"k={k}: silhouette failed ({e}), setting score=NaN\")\n",
        "                score = np.nan\n",
        "\n",
        "        elif metric == \"homogeneity\":\n",
        "            if y_true is None:\n",
        "                raise ValueError(\"y_true is required for homogeneity.\")\n",
        "            score = homogeneity_score(y_true, labels_pred)\n",
        "\n",
        "        elif metric == \"completeness\":\n",
        "            if y_true is None:\n",
        "                raise ValueError(\"y_true is required for completeness.\")\n",
        "            score = completeness_score(y_true, labels_pred)\n",
        "\n",
        "        elif metric == \"v_measure\":\n",
        "            if y_true is None:\n",
        "                raise ValueError(\"y_true is required for v_measure.\")\n",
        "            score = v_measure_score(y_true, labels_pred)\n",
        "\n",
        "        elif metric == \"calinski_harabasz\":\n",
        "            try:\n",
        "                score = calinski_harabasz_score(X_for_ch, labels_pred)\n",
        "            except Exception as e:\n",
        "                print(f\"k={k}: CH failed ({e}), setting score=NaN\")\n",
        "                score = np.nan\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f\"k = {k} | {metric} = {score:.5f} | elapsed = {t_stop - t_start:.4f} s\")\n",
        "\n",
        "    # ---- Plot ----\n",
        "    if title is None:\n",
        "        title = f\"Elbow method ({metric})\"\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(k_values, scores, marker='o')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(metric)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"k_values\": k_values,\n",
        "        \"scores\": scores,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wE00mtosqPt"
      },
      "source": [
        "let's evaluate the algorithm with 2 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxrkBBntVJ9_"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(X_train_vector)\n",
        "\n",
        "evaluate_clustering(\n",
        "    X=X_train_vector,\n",
        "    labels_pred=kmeans.labels_,\n",
        "    y_true=y_train,\n",
        "    name=\"K-Means (k=2)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLd6E9vzYC-j"
      },
      "outputs": [],
      "source": [
        "k_values = list(range(2, 10))\n",
        "\n",
        "kmeans_factory = lambda k: KMeans(\n",
        "    n_clusters=k,\n",
        "    random_state=random_seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_jb41iaX7wt"
      },
      "outputs": [],
      "source": [
        "results_hom = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=kmeans_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"homogeneity\",\n",
        "    y_true=y_train,\n",
        "    title=\"K-Means Homogeneity vs k\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iecy5CiXtrL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "results_sil = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=kmeans_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"silhouette\",\n",
        "    y_true=None,\n",
        "    title=\"K-Means Silhouette vs k\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoeQAwZtVKId"
      },
      "outputs": [],
      "source": [
        "results_ch = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=kmeans_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"calinski_harabasz\",\n",
        "    y_true=None,\n",
        "    title=\"K-Means Calinski–Harabasz vs k\",\n",
        "    to_dense_for_ch=True  # fa .toarray() dentro\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZVxeJ3tC9v"
      },
      "source": [
        "2-3 seems to be a good number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33hxhtKHbYkA"
      },
      "source": [
        "###Agglomerative clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmttBzNmtKZN"
      },
      "source": [
        "now we will evaluate a different algorithm using the same techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRRnEZpJPipD"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# PCA / Agglomerative / silhouette ecc. require dense array\n",
        "X_train_dense = X_train_vector.toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OTEnWanbn64"
      },
      "outputs": [],
      "source": [
        "agg = AgglomerativeClustering(\n",
        "    n_clusters=2,\n",
        "    linkage=\"ward\"\n",
        ").fit(X_train_dense)\n",
        "\n",
        "evaluate_clustering(\n",
        "    X=X_train_dense,\n",
        "    labels_pred=agg.labels_,\n",
        "    y_true=y_train,\n",
        "    name=\"Agglomerative Clustering (k=2, ward)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRNEWnbYucUE"
      },
      "source": [
        "let's plot the evaluation metrics with different numbers of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmrtL3E0bqoK"
      },
      "outputs": [],
      "source": [
        "k_values = list(range(2, 8))\n",
        "\n",
        "agg_factory = lambda k: AgglomerativeClustering(\n",
        "    n_clusters=k,\n",
        "    linkage=\"ward\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR_hTBGmbsnf"
      },
      "outputs": [],
      "source": [
        "results_hom_agg = elbow_method(\n",
        "    X=X_train_dense,               # DENSO\n",
        "    clusterer_factory=agg_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"homogeneity\",\n",
        "    y_true=y_train,\n",
        "    title=\"Agglomerative Homogeneity vs k\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA4JdQjObu57"
      },
      "outputs": [],
      "source": [
        "results_sil_agg = elbow_method(\n",
        "    X=X_train_dense,\n",
        "    clusterer_factory=agg_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"silhouette\",\n",
        "    y_true=None,\n",
        "    title=\"Agglomerative Silhouette vs k\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPrg75Vsbxjx"
      },
      "outputs": [],
      "source": [
        "results_ch_agg = elbow_method(\n",
        "    X=X_train_dense,\n",
        "    clusterer_factory=agg_factory,\n",
        "    k_values=k_values,\n",
        "    metric=\"calinski_harabasz\",\n",
        "    y_true=None,\n",
        "    title=\"Agglomerative Calinski–Harabasz vs k\",\n",
        "    to_dense_for_ch=False   # opzionale, è già denso\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf_IptRMtXju"
      },
      "source": [
        "2-3 seems to be a good number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSuL05GJbzw_"
      },
      "source": [
        "###DBScan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIlCiV5Wtakj"
      },
      "source": [
        "let's do the same with DBScan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJwksnIgb64-"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#eps: 0.1, 0.2, ..., 2.0\n",
        "eps_values = [(e + 1) / 10 for e in range(20)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTGyjKKscV-_"
      },
      "outputs": [],
      "source": [
        "# Factory for DBSCAN: k = eps\n",
        "dbscan_factory = lambda eps: DBSCAN(\n",
        "    eps=eps,\n",
        "    min_samples=5,\n",
        "    metric=\"cosine\",\n",
        "    n_jobs=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpdHg6mIcYxA"
      },
      "outputs": [],
      "source": [
        "dbscan_clust = dbscan_factory(0.5)\n",
        "dbscan_clust.fit(X_train_vector)\n",
        "\n",
        "evaluate_clustering(\n",
        "    X=X_train_vector,\n",
        "    labels_pred=dbscan_clust.labels_,\n",
        "    y_true=y_train,\n",
        "    name=\"DBSCAN (eps=0.5)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VByyjUDSu8LT"
      },
      "source": [
        "let's see how the evaluation metrics change when eps changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30FFqG1LcazO"
      },
      "outputs": [],
      "source": [
        "results_dbscan_sil = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=dbscan_factory,\n",
        "    k_values=eps_values,\n",
        "    metric=\"silhouette\",\n",
        "    y_true=None,\n",
        "    title=\"DBSCAN Silhouette vs eps\",\n",
        "    x_label=\"eps\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXyKYPRwcc_i"
      },
      "outputs": [],
      "source": [
        "results_dbscan_hom = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=dbscan_factory,\n",
        "    k_values=eps_values,\n",
        "    metric=\"homogeneity\",\n",
        "    y_true=y_train,\n",
        "    title=\"DBSCAN Homogeneity vs eps\",\n",
        "    x_label=\"eps\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlxAhKgSdXp-"
      },
      "outputs": [],
      "source": [
        "results_dbscan_ch = elbow_method(\n",
        "    X=X_train_vector,\n",
        "    clusterer_factory=dbscan_factory,\n",
        "    k_values=eps_values,              # qui eps_values = [0.1, 0.2, ..., 2.0]\n",
        "    metric=\"calinski_harabasz\",\n",
        "    y_true=None,\n",
        "    title=\"DBSCAN Calinski–Harabasz vs eps\",\n",
        "    to_dense_for_ch=True,             # fa .toarray() dentro\n",
        "    x_label=\"eps\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVtBD1tfkwj"
      },
      "source": [
        "lets try to fit DBScan with eps=0.3 and count how many clusters it generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbTw_CxCfoML"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "eps=0.3\n",
        "\n",
        "# Fit DBSCAN\n",
        "dbscan = DBSCAN(\n",
        "    eps,\n",
        "    min_samples=5,\n",
        "    metric=\"cosine\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dbscan.fit(X_train_vector)\n",
        "\n",
        "labels = dbscan.labels_\n",
        "\n",
        "# Count clusters (ignore label -1 which means noise)\n",
        "unique_labels = set(labels)\n",
        "clusters = [lbl for lbl in unique_labels if lbl != -1]\n",
        "n_clusters_generated = len(clusters)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(\"Cluster labels:\", unique_labels)\n",
        "print(f\"Number of clusters found (excluding noise): {n_clusters_generated}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5NH3UUuE52"
      },
      "source": [
        "let's plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5qKCWpouDn8"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 2) t-SNE 2D visualization\n",
        "# ============================\n",
        "\n",
        "# If X_train_vector is sparse, convert to dense\n",
        "try:\n",
        "    X_dense = X_train_vector.toarray()\n",
        "except AttributeError:\n",
        "    X_dense = X_train_vector  # already dense\n",
        "\n",
        "tsne = TSNE(\n",
        "    n_components=2,\n",
        "    perplexity=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_embedded = tsne.fit_transform(X_dense)\n",
        "print(\"Shape after t-SNE:\", X_embedded.shape)\n",
        "\n",
        "# ----------------------------\n",
        "# Color mapping for DBSCAN:\n",
        "# - each cluster gets a color\n",
        "# - noise points (label = -1) are light gray\n",
        "# ----------------------------\n",
        "unique_labels_sorted = sorted(unique_labels)\n",
        "cluster_labels = [lbl for lbl in unique_labels_sorted if lbl != -1]\n",
        "\n",
        "n_clusters = len(cluster_labels)\n",
        "\n",
        "# Use a matplotlib colormap for clusters\n",
        "cmap = plt.cm.get_cmap(\"tab20\", n_clusters)\n",
        "\n",
        "cluster_colors = []\n",
        "for lbl in labels:\n",
        "    if lbl == -1:\n",
        "        # noise points\n",
        "        cluster_colors.append(\"lightgray\")\n",
        "    else:\n",
        "        idx = cluster_labels.index(lbl)\n",
        "        cluster_colors.append(cmap(idx))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
        "\n",
        "# ---- Plot 1: DBSCAN clustering in t-SNE space ----\n",
        "ax[0].set_title(f\"DBSCAN clustering (eps={eps}) – t-SNE\")\n",
        "ax[0].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=cluster_colors\n",
        ")\n",
        "ax[0].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[0].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "# ---- Plot 2: True labels (benign vs jailbreak) in t-SNE space ----\n",
        "ax[1].set_title(\"Benign vs. jailbreak samples – t-SNE\")\n",
        "ax[1].scatter(\n",
        "    X_embedded[:, 0],\n",
        "    X_embedded[:, 1],\n",
        "    s=5,\n",
        "    alpha=0.7,\n",
        "    c=['g' if lbl == 'benign' else 'r' for lbl in y_train]\n",
        ")\n",
        "ax[1].set_xlabel(\"t-SNE dim 1\")\n",
        "ax[1].set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-gT6iZTuo5U"
      },
      "source": [
        "we didn't manage to locate any new meaningful clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr6Ohf8BjKtY"
      },
      "source": [
        "### final comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o9mjWkjuOsm"
      },
      "source": [
        "let's compare clusters made by different clustering algorithms by plotting them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xilkj9GjNUk"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "X_for_agg = X_train_dense\n",
        "\n",
        "agg = AgglomerativeClustering(\n",
        "    n_clusters=2,\n",
        "    linkage=\"ward\"\n",
        ")\n",
        "agg_labels = agg.fit_predict(X_for_agg)\n",
        "\n",
        "print(\"Agglomerative - cluster unici:\", np.unique(agg_labels))\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(\n",
        "    eps=0.3,\n",
        "    min_samples=5,\n",
        "    metric=\"cosine\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "db_labels = dbscan.fit_predict(X_train_vector)\n",
        "\n",
        "print(\"DBSCAN - labels:\", np.unique(db_labels))\n",
        "print(\"DBSCAN - n° cluster (escluding noise):\", len([c for c in set(db_labels) if c != -1]))\n",
        "print(\"DBSCAN - n° noise:\", list(db_labels).count(-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QJ8eZz0jQ7l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "markers = {'benign': 'o', 'jailbreak': 'x'}\n",
        "\n",
        "def plot_clusters_2d(X_2D, labels, y_true, title):\n",
        "\n",
        "    unique_clusters = np.unique(labels)\n",
        "    cmap = plt.cm.get_cmap(\"tab10\", len(unique_clusters))\n",
        "    cluster2color = {c: cmap(i) for i, c in enumerate(unique_clusters)}\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    for t in ['benign', 'jailbreak']:\n",
        "        idx = [i for i, lbl in enumerate(y_true) if lbl == t]\n",
        "\n",
        "        plt.scatter(\n",
        "            X_2D[idx, 0],\n",
        "            X_2D[idx, 1],\n",
        "            s=20,\n",
        "            alpha=0.7,\n",
        "            c=[cluster2color[labels[i]] for i in idx],\n",
        "            marker=markers[t],\n",
        "            label=t\n",
        "        )\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SisrUWEhjVHB"
      },
      "outputs": [],
      "source": [
        "# Hierarchical\n",
        "plot_clusters_2d(\n",
        "    X_2D=X_train_2D,\n",
        "    labels=agg_labels,\n",
        "    y_true=y_train,\n",
        "    title=\"Agglomerative clustering – clusters as colors, true labels as markers\"\n",
        ")\n",
        "\n",
        "# DBSCAN\n",
        "plot_clusters_2d(\n",
        "    X_2D=X_train_2D,\n",
        "    labels=db_labels,\n",
        "    y_true=y_train,\n",
        "    title=\"DBSCAN clustering – clusters as colors, true labels as markers\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1aIqWyanT6x"
      },
      "source": [
        "### other embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8Lpm9pv_YP"
      },
      "source": [
        "let's repeat the previous evaluation on the other embeddings we extracted from our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp0VuHeLnWka"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        print(f\"⚠️ Embedding '{emb_name}' not present in other_embeddings['train'], skip.\")\n",
        "        continue\n",
        "\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 K-Means (k=2) on embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, random_state=random_seed).fit(X_emb)\n",
        "    labels_pred = kmeans.labels_\n",
        "\n",
        "    evaluate_clustering(\n",
        "        X=X_emb,\n",
        "        labels_pred=labels_pred,\n",
        "        y_true=y_train,\n",
        "        name=f\"K-Means (k=2) on {emb_name}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkSFraQnnZEa"
      },
      "outputs": [],
      "source": [
        "k_values = list(range(2, 10))\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def kmeans_factory(k):\n",
        "    return KMeans(\n",
        "        n_clusters=k,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        print(f\"⚠️ Embedding '{emb_name}' not present, skip.\")\n",
        "        continue\n",
        "\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    print(\"\\n\" + \"#\"*80)\n",
        "    print(f\"📈 Elbow K-Means su embedding: {emb_name}\")\n",
        "    print(\"#\"*80)\n",
        "\n",
        "    # Homogeneity\n",
        "    _ = elbow_method(\n",
        "        X=X_emb,\n",
        "        clusterer_factory=kmeans_factory,\n",
        "        k_values=k_values,\n",
        "        metric=\"homogeneity\",\n",
        "        y_true=y_train,\n",
        "        title=f\"K-Means Homogeneity vs k ({emb_name})\"\n",
        "    )\n",
        "\n",
        "    # Silhouette\n",
        "    _ = elbow_method(\n",
        "        X=X_emb,\n",
        "        clusterer_factory=kmeans_factory,\n",
        "        k_values=k_values,\n",
        "        metric=\"silhouette\",\n",
        "        y_true=None,\n",
        "        title=f\"K-Means Silhouette vs k ({emb_name})\"\n",
        "    )\n",
        "\n",
        "    # Calinski–Harabasz\n",
        "    _ = elbow_method(\n",
        "        X=X_emb,\n",
        "        clusterer_factory=kmeans_factory,\n",
        "        k_values=k_values,\n",
        "        metric=\"calinski_harabasz\",\n",
        "        y_true=None,\n",
        "        title=f\"K-Means Calinski–Harabasz vs k ({emb_name})\",\n",
        "        to_dense_for_ch=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9paLPripwbeq"
      },
      "source": [
        "recap on the clustering algorithms metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFe8Um64nbTB"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        continue\n",
        "\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]  # già denso (np.array)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 Agglomerative (k=2, ward) on embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    agg = AgglomerativeClustering(\n",
        "        n_clusters=2,\n",
        "        linkage=\"ward\"\n",
        "    ).fit(X_emb)\n",
        "\n",
        "    evaluate_clustering(\n",
        "        X=X_emb,\n",
        "        labels_pred=agg.labels_,\n",
        "        y_true=y_train,\n",
        "        name=f\"Agglomerative (k=2, ward) su {emb_name}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmsgcSJMngMS"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "eps = 0.3\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        continue\n",
        "\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 DBSCAN (eps={eps}) su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    dbscan = DBSCAN(\n",
        "        eps=eps,\n",
        "        min_samples=5,\n",
        "        metric=\"cosine\",\n",
        "        n_jobs=-1\n",
        "    ).fit(X_emb)\n",
        "\n",
        "    labels = dbscan.labels_\n",
        "    unique_labels = set(labels)\n",
        "    clusters = [lbl for lbl in unique_labels if lbl != -1]\n",
        "    n_clusters_generated = len(clusters)\n",
        "    n_noise = list(labels).count(-1)\n",
        "\n",
        "    print(\"Cluster labels:\", unique_labels)\n",
        "    print(f\"Number of clusters found (excluding noise): {n_clusters_generated}\")\n",
        "    print(f\"Number of noise points: {n_noise}\")\n",
        "\n",
        "    evaluate_clustering(\n",
        "        X=X_emb,\n",
        "        labels_pred=labels,\n",
        "        y_true=y_train,\n",
        "        name=f\"DBSCAN (eps={eps}) su {emb_name}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy9vElkVnkvQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import umap.umap_ as umap\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        continue\n",
        "\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🎨 PCA + UMAP + KMeans per embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # KMeans\n",
        "    kmeans = KMeans(n_clusters=2, random_state=random_seed).fit(X_emb)\n",
        "    km_labels = kmeans.labels_\n",
        "\n",
        "    # PCA 2D\n",
        "    pca = PCA(n_components=2, random_state=random_seed)\n",
        "    X_pca_2D = pca.fit_transform(X_emb)\n",
        "\n",
        "    plot_clusters_2d(\n",
        "        X_2D=X_pca_2D,\n",
        "        labels=km_labels,\n",
        "        y_true=y_train,\n",
        "        title=f\"PCA – KMeans clusters vs true labels ({emb_name})\"\n",
        "    )\n",
        "\n",
        "    # UMAP 2D\n",
        "    umap_2d = umap.UMAP(\n",
        "        n_components=2,\n",
        "        n_neighbors=15,\n",
        "        min_dist=0.1,\n",
        "        metric='cosine',\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    X_umap_2D = umap_2d.fit_transform(X_emb)\n",
        "\n",
        "    plot_clusters_2d(\n",
        "        X_2D=X_umap_2D,\n",
        "        labels=km_labels,\n",
        "        y_true=y_train,\n",
        "        title=f\"UMAP – KMeans clusters vs true labels ({emb_name})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQkjAaaoDdB"
      },
      "source": [
        "# Anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0ACSWRioGUa"
      },
      "outputs": [],
      "source": [
        "# Many anomaly detection algorithms require dense matrices (NumPy arrays),\n",
        "# so this helper converts sparse matrices (like from CountVectorizer) to dense.\n",
        "def to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk2zgDuIrs-h"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def run_model(name, fn):\n",
        "  \"\"\"\n",
        "    Utility to:\n",
        "      - measure runtime of an anomaly detection model\n",
        "      - run the model via a function `fn()`\n",
        "      - print how long it took and how many points were labeled as each class\n",
        "\n",
        "    `fn` must return an array of predictions (usually +1 for normal, -1 for anomaly).\n",
        "    \"\"\"\n",
        "  t_start = time.time()\n",
        "  y_pred = fn()\n",
        "  t_stop = time.time()\n",
        "\n",
        "  vals, cnts = np.unique(y_pred, return_counts=True)\n",
        "  print(f\"[{name:<16}] time={t_stop - t_start:6.2f}s  labels={dict(zip(vals, cnts))}\")\n",
        "  return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JijDhuCmry7f"
      },
      "outputs": [],
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "def anomaly_detection(\n",
        "    X,\n",
        "    rep_name,\n",
        "    outlier_fraction=0.01,\n",
        "    random_seed=42,\n",
        "    do_tsne=False,\n",
        "    light_mode=False\n",
        "):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"🔍 ANOMALY DETECTION on representation: {rep_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    def to_dense_local(X_):\n",
        "        return X_.toarray() if hasattr(X_, \"toarray\") else X_\n",
        "\n",
        "    X_dense = to_dense_local(X)\n",
        "    X_scaled = X_dense  # here you could add scaling/normalization if needed\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 0) One-Class SVM\n",
        "    # ---------------------------------------------------------\n",
        "    results[\"svm\"] = run_model(\n",
        "        \"OneClassSVM\",\n",
        "        lambda: OneClassSVM(\n",
        "            kernel=\"rbf\",\n",
        "            gamma=\"scale\",\n",
        "            nu=outlier_fraction\n",
        "        ).fit(X_scaled).predict(X_scaled)\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1) Isolation Forest\n",
        "    # ---------------------------------------------------------\n",
        "    results[\"isolation\"] = run_model(\n",
        "        \"IsolationForest\",\n",
        "        lambda: IsolationForest(\n",
        "            contamination=outlier_fraction,\n",
        "            random_state=random_seed,\n",
        "            n_jobs=-1\n",
        "        ).fit(X_scaled).predict(X_scaled)\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2) Local Outlier Factor (LOF)\n",
        "    # ---------------------------------------------------------\n",
        "    results[\"lof\"] = run_model(\n",
        "        \"LocalOutlierFactor\",\n",
        "        lambda: LocalOutlierFactor(\n",
        "            n_neighbors=35,\n",
        "            contamination=outlier_fraction,\n",
        "            n_jobs=-1\n",
        "        ).fit_predict(X_scaled)\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3) t-SNE visualization\n",
        "    # ---------------------------------------------------------\n",
        "    if do_tsne:\n",
        "        plot_tsne(X_scaled, results, rep_name)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HibREFzkr9au"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = np.array([\"r\", \"g\"])  # -1 = red, +1 = green\n",
        "\n",
        "def plot_tsne(X, preds, rep_name):\n",
        "    print(\"Calcolo t-SNE...\")\n",
        "    X_2d = TSNE(n_components=2, perplexity=20, random_state=42).fit_transform(X)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    methods = [\"svm\", \"isolation\", \"lof\"]\n",
        "    titles  = [\"One-Class SVM\", \"Isolation Forest\", \"Local Outlier Factor\"]\n",
        "\n",
        "    for i, (m, t) in enumerate(zip(methods, titles)):\n",
        "        y = preds[m]\n",
        "        ax[i].scatter(X_2d[:, 0], X_2d[:, 1],\n",
        "                      s=5, alpha=0.5,\n",
        "                      c=colors[(y + 1) // 2])\n",
        "        ax[i].set_title(f\"{rep_name} – {t}\")\n",
        "        ax[i].set_xticks([])\n",
        "        ax[i].set_yticks([])\n",
        "\n",
        "    plt.suptitle(f\"t-SNE – {rep_name}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7qo7yhb2SDd"
      },
      "outputs": [],
      "source": [
        "results_bow = anomaly_detection(\n",
        "    X_train_vector,\n",
        "    rep_name=\"bow\",\n",
        "    outlier_fraction=0.01,\n",
        "    random_seed=random_seed,\n",
        "    do_tsne=False,\n",
        "    #light_mode=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot7BqfoE-kpr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Isolation Forest predictions\n",
        "y_pred_is = results_bow[\"isolation\"]\n",
        "\n",
        "X_bow_dense = X_train_vector.toarray()\n",
        "\n",
        "\n",
        "max_features = 50\n",
        "X_bow_dense_small = X_bow_dense[:, :max_features]\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(24, 8), sharey=True)\n",
        "\n",
        "# Non-anomalies (label = 1)\n",
        "ax[0].boxplot(X_bow_dense_small[y_pred_is == 1])\n",
        "ax[0].set_title(\"predicted non-anomalies (Isolation Forest)\")\n",
        "\n",
        "# anomalies (label = -1)\n",
        "ax[1].boxplot(X_bow_dense_small[y_pred_is == -1])\n",
        "ax[1].set_title(\"predicted anomalies (Isolation Forest)\")\n",
        "\n",
        "feature_labels = [f\"f{i}\" for i in range(max_features)]\n",
        "\n",
        "for idx in [0, 1]:\n",
        "    ax[idx].set_xticklabels(feature_labels, rotation=90)\n",
        "    ax[idx].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jygujKPW-vyE"
      },
      "outputs": [],
      "source": [
        "y_pred_lof = results_bow[\"lof\"]\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(24, 8), sharey=True)\n",
        "\n",
        "ax[0].boxplot(X_bow_dense_small[y_pred_lof == 1])\n",
        "ax[0].set_title(\"predicted non-anomalies (LOF)\")\n",
        "\n",
        "ax[1].boxplot(X_bow_dense_small[y_pred_lof == -1])\n",
        "ax[1].set_title(\"predicted anomalies (LOF)\")\n",
        "\n",
        "for idx in [0, 1]:\n",
        "    ax[idx].set_xticklabels(feature_labels, rotation=90)\n",
        "    ax[idx].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76JOdwixsDUe"
      },
      "outputs": [],
      "source": [
        "anomaly_results = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    X_emb = other_embeddings[\"train\"][emb_name]\n",
        "    anomaly_results[emb_name] = anomaly_detection(\n",
        "        X_emb,\n",
        "        rep_name=emb_name,\n",
        "        outlier_fraction=0.01,\n",
        "        do_tsne=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "no matter the word embedding, the anomalies don't seem to be describing useful additional patterns in the data"
      ],
      "metadata": {
        "id": "AqT-CY_BiVVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's see if they can be used to improve performance on other models:"
      ],
      "metadata": {
        "id": "N6QAU-gTjTDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# 1) Logistic Regression on FULL BoW\n",
        "logreg_bow_full = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver=\"liblinear\"\n",
        ")\n",
        "\n",
        "logreg_bow_full.fit(X_train_vector, y_train)\n",
        "\n",
        "print(\"=== LogReg BoW – full training data (VALIDATION) ===\")\n",
        "results_logreg_full_val = evaluate_classifier(\n",
        "    logreg_bow_full,\n",
        "    X_val_vector,\n",
        "    y_val\n",
        ")\n"
      ],
      "metadata": {
        "id": "pmMcd2_6i1LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Logistic Regression on CLEAN BoW (anomalies removed)\n",
        "\n",
        "# Isolation Forest predictions on BoW features\n",
        "y_pred_is = results_bow[\"isolation\"]   # +1 = normal, -1 = anomaly\n",
        "\n",
        "# Align y_train with X_train_vector indices\n",
        "y_train_arr = y_train.reset_index(drop=True).to_numpy()\n",
        "\n",
        "# Keep only inliers\n",
        "mask_inliers = (y_pred_is == 1)\n",
        "\n",
        "X_train_bow_clean = X_train_vector[mask_inliers]\n",
        "y_train_clean     = y_train_arr[mask_inliers]\n",
        "\n",
        "print(f\"Train size (full):  {X_train_vector.shape[0]}\")\n",
        "print(f\"Train size (clean): {X_train_bow_clean.shape[0]}  \"\n",
        "      f\"(removed {X_train_vector.shape[0] - X_train_bow_clean.shape[0]} anomalies)\")\n",
        "\n",
        "logreg_bow_clean = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver=\"liblinear\"\n",
        ")\n",
        "\n",
        "logreg_bow_clean.fit(X_train_bow_clean, y_train_clean)\n",
        "\n",
        "print(\"=== LogReg BoW – anomalies removed (VALIDATION) ===\")\n",
        "results_logreg_clean_val = evaluate_classifier(\n",
        "    logreg_bow_clean,\n",
        "    X_val_vector,\n",
        "    y_val\n",
        ")"
      ],
      "metadata": {
        "id": "-H8ogeA9i21D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can notice that removing the anomalies didn't affect much the performance of the classifier."
      ],
      "metadata": {
        "id": "9OvIPzRejYLL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SawR1-_e8_iH"
      },
      "source": [
        "# Non linear classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVBUiLeZwV0g"
      },
      "source": [
        "In the following section, we train several supervised machine learning models—including k-Nearest Neighbors (KNN), Decision Trees, and a Multi-Layer Perceptron (MLP) neural network—directly on these vectorized features. For each model, we evaluate performance using standard classification metrics such as accuracy, precision, recall, F1-score, and the confusion matrix. The goal is to compare how different algorithms behave on the same representation of the data and to identify which models are most effective at distinguishing jailbreak prompts from benign ones, without applying any hyperparameter search procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPvfptCBzJrX"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMbeH6S6zJrX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from scipy.stats import uniform, randint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0nGIe6czJrY"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOqeW25xzJrY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# Show plots inline in the notebook\n",
        "# Plotting libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7aR72jgzJrY"
      },
      "source": [
        "Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtbL3QgbzJrY"
      },
      "outputs": [],
      "source": [
        "# Feature scaling (for models that are sensitive to feature magnitude)\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXcTaQozJrZ"
      },
      "source": [
        "Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIo2_5KczJrZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier # k-Nearest Neighbors\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree # Decision Trees\n",
        "from sklearn.neural_network import MLPClassifier # Multi-Layer Perceptron (Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z12w4t5azJra"
      },
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrx9d3E2zJra"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    roc_curve,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.calibration import CalibrationDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cutcPcOUzJrb"
      },
      "source": [
        "Model selection and cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCqEVPKEzJrb"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter search utilities\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpQXx9Y7zJrl"
      },
      "source": [
        "### Non-linear classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wo2N4QUzJrm"
      },
      "source": [
        "#### k-Nearest Neighbors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1B5Sq1izJrm"
      },
      "source": [
        "We create our list of values to try and we fit and evaluate the model iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJxhWVnFzJrm"
      },
      "outputs": [],
      "source": [
        "list_h_param_values = [1, 2, 3, 4, 5]\n",
        "list_h_param_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSQdbAuAOfnE"
      },
      "outputs": [],
      "source": [
        "list_h_param_values = [1, 2, 3, 4, 5]\n",
        "\n",
        "knn_factory = lambda C: KNeighborsClassifier(n_neighbors=int(C))\n",
        "\n",
        "results_knn = sweep_model_C(\n",
        "    model_factory=knn_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    C_values=list_h_param_values,\n",
        "    model_name=\"k-Nearest Neighbors\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all metrics, the hyperparameter sweep shows that KNN does not significantly benefit from changing k.\n",
        "Some values (like k = 2 and k=4) provide better precision and specificity, meaning the classifier becomes more selective and avoids false alarms.\n",
        "However, recall remains critically low for every k, preventing the model from detecting jailbreak prompts reliably.\n",
        "\n",
        "The combined trends indicate that:\n",
        "\n",
        "the model is systematically biased toward the benign class,\n",
        "\n",
        "increasing or decreasing k does not correct this imbalance,\n",
        "\n",
        "KNN cannot form meaningful neighborhoods for the jailbreak samples in this embedding space,\n",
        "\n",
        "and no value of k produces a balanced or robust classifier."
      ],
      "metadata": {
        "id": "h_oKHSQwRoK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMzwnGL9zcIs"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_models_default = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 k-Nearest Neighbors (default, k=5) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # KNN with fixed k\n",
        "    clf = KNeighborsClassifier(n_neighbors=5)\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    knn_models_default[emb_name] = clf\n",
        "\n",
        "    # evalaution on validation set\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment clearly shows that the embedding quality dominates KNN performance.\n",
        "With classical embeddings (w2v, GloVe), KNN shows moderate accuracy and high recall but poor specificity. FastText improves recall dramatically but at the cost of excessive false alarms.\n",
        "\n",
        "SBERT stands out by a large margin, offering:\n",
        "\n",
        "- high recall similar to FastText,\n",
        "\n",
        "- much higher precision and specificity,\n",
        "\n",
        "- the best accuracy (0.833) and F1-score (0.814),\n",
        "\n",
        "- and a more balanced confusion matrix.\n",
        "\n",
        "Despite this, even with SBERT, KNN remains inferior to more advanced models like SVM and MLP in overall robustness and precision-recall balance."
      ],
      "metadata": {
        "id": "Zni9Yl-B1LX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibF82sGWzdVm"
      },
      "outputs": [],
      "source": [
        "# possibile values of k\n",
        "list_h_param_values = [1, 2, 3, 4, 5]\n",
        "\n",
        "results_knn_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep k per k-Nearest Neighbors su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # Factory for KNN, C used as \"k\"\n",
        "    knn_factory = lambda C: KNeighborsClassifier(n_neighbors=int(C))\n",
        "\n",
        "    results_knn = sweep_model_C(\n",
        "        model_factory=knn_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=list_h_param_values,\n",
        "        model_name=f\"k-Nearest Neighbors_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_knn_sweep[emb_name] = results_knn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all embeddings, the k-sweep shows that changing k slightly alters the balance between precision and recall, but does not dramatically change performance.\n",
        "The dominant factor is the embedding representation, not the neighborhood size.\n",
        "\n",
        "w2v, GloVe Twitter, and GloVe Wiki produce similar and limited performance.\n",
        "\n",
        "FastText shows meaningful improvements, especially in recall.\n",
        "\n",
        "SBERT is clearly the best embedding, providing consistent, stable, and significantly better results for every metric."
      ],
      "metadata": {
        "id": "vBgPaOjo2KeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all experiments, KNN shows clear limitations as a non-linear classifier for jailbreak detection.\n",
        "The model’s performance is dominated by the embedding space, and varying k does not fundamentally alter its behaviour. While FastText improves sensitivity and SBERT dramatically improves the overall balance between classes, KNN remains:\n",
        "\n",
        "- unstable,\n",
        "\n",
        "- heavily biased toward the benign class,\n",
        "\n",
        "- inconsistent across embeddings,\n",
        "\n",
        "- and incapable of achieving strong F1-scores except with SBERT.\n",
        "\n",
        "Therefore, although SBERT + KNN yields acceptable performance, KNN in general is not a reliable classifier for this task. More advanced models (Decision Tree, SVM, MLP) are significantly more robust and accurate."
      ],
      "metadata": {
        "id": "TfEelgGh2T5c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkUxvU1HzJrm"
      },
      "source": [
        "#### Decision Tree\n",
        "\n",
        "You can find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZYPjffdzJrm"
      },
      "source": [
        "We create our list of values to try and we fit and evaluate the model iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnIjaWcCzJrm"
      },
      "outputs": [],
      "source": [
        "list_h_param_values = [x + 1 for x in range(10)]\n",
        "list_h_param_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V91mie4YPWI6"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_factory = lambda C: DecisionTreeClassifier(\n",
        "    max_depth=int(C),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "results_tree = sweep_model_C(\n",
        "    model_factory=tree_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    C_values=list_h_param_values,\n",
        "    model_name=\"Decision Tree\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Tree improves noticeably as depth increases from 1 to around 5–7, where it achieves its best balance between sensitivity and precision. Beyond that point, deeper trees degrade the ability to correctly classify benign prompts and begin to overfit without offering clear advantages.\n",
        "\n",
        "Thus, intermediate-depth trees represent the optimal configuration, while very shallow and very deep trees are suboptimal.\n",
        "This analysis confirms the classic behaviour of decision trees: too simple → underfit; too complex → overfit."
      ],
      "metadata": {
        "id": "jhWKUWZg3dKK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q6_EuGqzJrq"
      },
      "source": [
        "Visualise the tree (example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myQzsTRfTXRT"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) create model\n",
        "tree_clf = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2) Fit the model\n",
        "tree_clf.fit(X_train_vector, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwJ_D4x-zJrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. train witn max_depth=5\n",
        "clf_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "clf_tree.fit(X_train_vector, y_train)\n",
        "\n",
        "# 2. Visualizza tree\n",
        "plt.figure(figsize=(20, 12))\n",
        "plot_tree(\n",
        "    clf_tree,\n",
        "    filled=True,\n",
        "    feature_names=vectorizer.get_feature_names_out(),   # nomi delle features\n",
        "    class_names=['benign', 'jailbreak'],\n",
        "    max_depth=5,  # limita la profondità del plot (opzionale ma utile)\n",
        "    fontsize=8\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2y7Fw_zyfRP"
      },
      "source": [
        "now we repeat for the other embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7EHDDXop0_o"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_models_default = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 DecisionTreeClassifier (default, max_depth=5) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    clf = DecisionTreeClassifier(\n",
        "        max_depth=5,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    dt_models_default[emb_name] = clf\n",
        "\n",
        "    # evaluation on validation set\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across embeddings, the tree with max_depth = 5 shows:\n",
        "\n",
        "- moderate performance for classical embeddings (w2v, GloVe),\n",
        "\n",
        "- high sensitivity but lower precision with FastText,\n",
        "\n",
        "- the best and most stable performance with SBERT.\n",
        "\n",
        "This confirms that embedding quality strongly influences Decision Tree performance, and that more semantically expressive embeddings — like SBERT — allow even simple models to achieve significantly better classification results."
      ],
      "metadata": {
        "id": "MHwooZH35ONn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nESwtK4Ep5dP"
      },
      "outputs": [],
      "source": [
        "#max_depth possible values\n",
        "list_max_depth = [2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "results_dt_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep max_depth per DecisionTreeClassifier su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # factory for DecisionTree: C as max_depth\n",
        "    dt_factory = lambda C: DecisionTreeClassifier(\n",
        "        max_depth=int(C),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    results_dt = sweep_model_C(\n",
        "        model_factory=dt_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=list_max_depth,\n",
        "        model_name=f\"DecisionTree_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_dt_sweep[emb_name] = results_dt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all embeddings, the Decision Tree shows:\n",
        "\n",
        "- moderate performance with w2v and both GloVe variants,\n",
        "\n",
        "- higher sensitivity but reduced precision with FastText,\n",
        "\n",
        "- the strongest and most stable results with SBERT.\n",
        "\n",
        "These observations reinforce the notion that embedding quality strongly influences tree-based models.\n",
        "SBERT remains the most effective representation, enabling the Decision Tree to form clearer, more discriminative partitions of the data, even under depth constraints."
      ],
      "metadata": {
        "id": "GN2NxwTg5lBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all results together, the Decision Tree:\n",
        "\n",
        "- performs best at intermediate depths (5–7),\n",
        "\n",
        "- suffers from underfitting when shallow,\n",
        "\n",
        "- suffers from overfitting when too deep,\n",
        "\n",
        "- reaches only moderate performance with classic embeddings (w2v/GloVe),\n",
        "\n",
        "- becomes aggressive but imprecise with FastText,\n",
        "\n",
        "- and achieves the strongest and most stable results with SBERT.\n",
        "\n",
        "The Decision Tree is highly dependent on embedding quality. While tuning max_depth improves performance, only semantically expressive embeddings—especially SBERT—enable the model to reach its full potential. Without high-quality embeddings, the tree remains limited regardless of depth."
      ],
      "metadata": {
        "id": "cDFaZStG6IRg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNwGsgpozJrn"
      },
      "source": [
        "#### Kernel SVM\n",
        "we repeat our analysis on this other algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7bherWpqGBR"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_models_default = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Kernel SVM (SVC RBF, default) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # modello SVM with kernel RBF\n",
        "    clf = SVC(\n",
        "        kernel=\"rbf\",\n",
        "        C=1.0,\n",
        "\n",
        "    )\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    svm_models_default[emb_name] = clf\n",
        "\n",
        "    # evaluation on validation set\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all embeddings, several patterns emerge:\n",
        "\n",
        "Embedding quality is the dominant factor.\n",
        "\n",
        "Even with the same hyperparameters, performance varies dramatically depending on the embedding.\n",
        "\n",
        "SBERT > FastText > (w2v ≈ GloVe)\n",
        "\n",
        "- SBERT consistently provides the highest accuracy, precision, recall, and F1.\n",
        "\n",
        "- FastText is a strong second, improving recall and overall robustness.\n",
        "\n",
        "- w2v and GloVe provide similar but weaker results.\n"
      ],
      "metadata": {
        "id": "6yOjzDisKZYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuQXqTtYqIif"
      },
      "outputs": [],
      "source": [
        "#same list as before\n",
        "C_values = list_c\n",
        "\n",
        "results_svm_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep C per Kernel SVM (RBF) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # factory for SVM RBF (parameter called C)\n",
        "    svm_factory = lambda C: SVC(\n",
        "        kernel=\"rbf\",\n",
        "        C=C\n",
        "    )\n",
        "\n",
        "    results_svm = sweep_model_C(\n",
        "        model_factory=svm_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=C_values,\n",
        "        model_name=f\"KernelSVM_RBF_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_svm_sweep[emb_name] = results_svm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all embeddings, the C-sweep shows that:\n",
        "\n",
        "- The RBF SVM is relatively robust: performance does not collapse for “wrong” C; it mostly gently improves then saturates.\n",
        "\n",
        "- For simpler embeddings (w2v, GloVe), tuning C only brings small incremental gains.\n",
        "\n",
        "- For FastText and especially SBERT, increasing C leads to consistent, meaningful improvements in accuracy, F1, and specificity, with recall staying high.\n",
        "\n",
        "- SBERT + RBF SVM with moderately large C is clearly the best configuration, combining high accuracy, high recall, and high precision, at the cost of slightly higher training time.\n",
        "\n",
        "In other words:\n",
        "\n",
        "Tuning C refines an already strong model; the real performance leap comes from using richer embeddings (FastText, SBERT), and C mainly helps these embeddings reach their full potential."
      ],
      "metadata": {
        "id": "dBrQsrmxLi8g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ScA66AizJrn"
      },
      "source": [
        "We create our list of values to try and we fit and evaluate the model iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_17bYSKzJrn"
      },
      "outputs": [],
      "source": [
        "list_h_param_values = np.linspace(0.1, 3, 30)\n",
        "list_h_param_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi7uiYKpP-x-"
      },
      "outputs": [],
      "source": [
        "svc_factory = lambda C: SVC(C=C, kernel=\"linear\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "list_h_param_values = np.linspace(0.1, 3, 30)\n",
        "\n",
        "svc_factory = lambda C: SVC(\n",
        "    C=C,\n",
        "    kernel=\"linear\",\n",
        ")\n",
        "\n",
        "results_svc = sweep_model_C(\n",
        "    model_factory=svc_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    C_values=list_h_param_values,\n",
        "    model_name=\"Linear SVC\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important conclusion from the C-sweep is that the Linear SVM is extremely insensitive to the choice of C. All metrics remain nearly constant as C changes:\n",
        "\n",
        "- No significant overfitting at high C\n",
        "\n",
        "- No strong underfitting at low C\n",
        "\n",
        "- Very small oscillations in performance\n",
        "\n",
        "- Decision boundary is largely stable\n",
        "\n",
        "This strongly indicates that:\n",
        "\n",
        "The dataset is almost linearly separable\n",
        "and the linear SVM finds a good separating hyperplane regardless of regularization strength"
      ],
      "metadata": {
        "id": "6fGQdoSVMOKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel SVM emerges as a highly effective classifier for jailbreak detection, but its true performance is realized only when using expressive, semantically rich embeddings. SBERT stands out decisively, enabling the SVM to form clearer and more reliable decision boundaries than any other embedding. FastText serves as a strong intermediate option, outperforming classical embeddings but still falling short of SBERT in terms of precision–recall balance. Traditional embeddings like w2v and GloVe consistently yield moderate and quickly saturating performance, confirming that embedding quality—not C tuning—is the primary driver of model effectiveness"
      ],
      "metadata": {
        "id": "MIm8UAJmNJsW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_5sQOg6zJro"
      },
      "source": [
        "#### (Deep) Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWB9mF9ZzXco"
      },
      "source": [
        "we try to do the same with a neural network. We didn't choose the best parameters to make sure the model wouldn't be too hefty to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9svIfDuMzJro"
      },
      "source": [
        "We create our list of values to try and we fit and evaluate the model iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDYydcr3zJro"
      },
      "outputs": [],
      "source": [
        "list_h_param_values = np.linspace(1e-4, 1, 10)\n",
        "list_h_param_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dLsU5SWST7x"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_factory = lambda C: MLPClassifier(\n",
        "    hidden_layer_sizes=(50,),\n",
        "    alpha=C,\n",
        "    max_iter=200,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "results_mlp = sweep_model_C(\n",
        "    model_factory=mlp_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    C_values=list_h_param_values,\n",
        "    model_name=\"MLPClassifier (alpha sweep)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The α-sweep shows that the MLPClassifier is a highly robust and consistently well-performing classifier for jailbreak detection.\n",
        "Regularization has only a moderate impact on its metrics, confirming that the network forms a strong internal representation early and maintains it regardless of α.\n",
        "\n",
        "- Accuracy, precision, recall, and F1 all remain high and stable.\n",
        "\n",
        "- Specificity stays consistently strong, indicating reliable handling of benign prompts.\n",
        "\n",
        "- Improvements or degradations due to α are minor compared to the impact of embeddings seen in other models."
      ],
      "metadata": {
        "id": "GX84geU6SYdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5YImtX9qQoK"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_models_default = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 MLPClassifier (default) su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # modello MLP \"base\"\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(50,),\n",
        "        alpha=1e-3,\n",
        "        max_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    mlp_models_default[emb_name] = clf\n",
        "\n",
        "    # evaluation on validation set\n",
        "    evaluate_classifier(\n",
        "        clf,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all embeddings, the default MLPClassifier demonstrates robust and stable performance, but its effectiveness grows dramatically with embedding quality.\n",
        "\n",
        "- With w2v and GloVe, the model performs moderately well but remains limited by the embeddings’ lack of semantic richness.\n",
        "\n",
        "- FastText significantly improves both recall and precision, allowing the neural network to build more informative decision boundaries.\n",
        "\n",
        "- SBERT achieves the best results across all metrics, confirming that deep semantic representations help even a shallow neural network reach high accuracy, strong class separation, and excellent F1-scores.\n",
        "In conclusion, the deep neural network is a strong model whose performance is primarily driven by embedding quality. SBERT allows the MLP to operate at its full potential, making SBERT + MLP one of the best-performing combinations in the entire classifier suite—second only to SBERT + SVM"
      ],
      "metadata": {
        "id": "7fkkmRRCSb9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96S--FuQqQbh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#list of values to try\n",
        "list_h_param_values = np.linspace(1e-4, 1, 10)\n",
        "\n",
        "results_mlp_sweep = {}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔹 Sweep alpha per MLPClassifier su embedding: {emb_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # factory for MLP: C as alpha\n",
        "    mlp_factory = lambda C: MLPClassifier(\n",
        "        hidden_layer_sizes=(50,),\n",
        "        alpha=C,\n",
        "        max_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    results_mlp = sweep_model_C(\n",
        "        model_factory=mlp_factory,\n",
        "        X_train_vector=X_train_vec,\n",
        "        y_train=y_train,\n",
        "        X_val_vector=X_val_vec,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        C_values=list_h_param_values,\n",
        "        model_name=f\"MLPClassifier_alpha_{emb_name}\"\n",
        "    )\n",
        "\n",
        "    results_mlp_sweep[emb_name] = results_mlp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all embeddings together:\n",
        "\n",
        "- SBERT still delivers the highest absolute performance for almost every α; its best results are obtained with low regularization (small α).\n",
        "\n",
        "- FastText is the second-best option and actually benefits from relatively large α, combining high recall with improved specificity and lower training time.\n",
        "\n",
        "- GloVe Wiki is clearly better than GloVe Twitter, and both gain something from moderate α but never reach SBERT/FastText levels.\n",
        "\n",
        "- w2v is the least sensitive to α: its metrics change very little, confirming that regularization cannot compensate for its limited representational power"
      ],
      "metadata": {
        "id": "2YEYmBPsPWul"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-5PPz3AXCYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The deep neural network is a strong and stable classifier, but its true effectiveness is determined by the richness of the embedding.\n",
        "SBERT enables the MLP to function at its full potential, producing results that are competitive with the best-performing models in the entire study—second only to the combination SBERT + RBF SVM.\n",
        "FastText provides a solid alternative, while w2v and GloVe embeddings limit the model by constraining the expressive power of its learned representations.\n",
        "\n",
        "In summary, the MLPClassifier excels when paired with semantically meaningful embeddings, confirming that embedding quality is the key driver of neural network performance in jailbreak detection."
      ],
      "metadata": {
        "id": "2_X1eYv1P5lA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all models and embeddings tested, SBERT consistently delivers the highest semantic quality, enabling every classifier to perform at significantly higher levels.\n",
        "When coupled with SBERT, both Kernel SVM (RBF) and Linear SVM achieve near-optimal performance, making them the best choices for jailbreak detection.\n",
        "\n",
        "The MLPClassifier provides a strong alternative with similarly robust results and minimal sensitivity to hyperparameters.\n",
        "Decision Trees provide interpretability but fall behind in accuracy and F1.\n",
        "KNN, finally, shows clear limitations and is not competitive for this task"
      ],
      "metadata": {
        "id": "FZS7-Is4QCDb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o33oo_LnW3fv"
      },
      "source": [
        "# XGBoost and Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcpL7L0bW3fw"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJZYarpwW3fw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "from scipy.stats import uniform, randint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3v75GJzW3fw"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d27LQI5WW3fw"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azs1IZVrW3fw"
      },
      "source": [
        "Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnoN6Z2sW3fx"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQr56U_W3fx"
      },
      "source": [
        "Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TwTibtOW3fx"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grO_l0cfW3fx"
      },
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC-Y15TKW3fx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "    roc_curve,\n",
        "    roc_auc_score,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.calibration import CalibrationDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llL9MUrHW3fx"
      },
      "source": [
        "Model selection and cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-PiRUbiW3fx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySOwXgZ30OF3"
      },
      "source": [
        "XGBoost requires numerical values for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9CfV-2LW3fy"
      },
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQMQXMD7W3fy"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_val_enc   = le.transform(y_val)\n",
        "\n",
        "print(\"Classi:\", le.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjmIRL0SW3fy"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store results for each embedding\n",
        "# For each embedding we will save the trained model and its F1-score\n",
        "xgb_results = {}\n",
        "\n",
        "# Loop over all available embedding names\n",
        "for emb_name in embedding_names:\n",
        "\n",
        "    # Skip embeddings that are not available in the dictionary\n",
        "    if emb_name not in other_embeddings[\"train\"]:\n",
        "        print(f\"Embedding {emb_name} not available, skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🚀 XGBoost with embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Retrieve training and validation embeddings\n",
        "    X_train_emb = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_emb   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    # Print shapes for sanity check\n",
        "    print(\"Shapes:\", X_train_emb.shape, X_val_emb.shape)\n",
        "\n",
        "    # Initialize XGBoost classifier with fixed hyperparameters\n",
        "    xgb_emb = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        n_jobs=-1,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model on the training embeddings\n",
        "    xgb_emb.fit(X_train_emb, y_train_enc)\n",
        "\n",
        "    # Predict labels on the validation set\n",
        "    y_val_pred = xgb_emb.predict(X_val_emb)\n",
        "\n",
        "    # Compute F1-score on the validation set\n",
        "    f1 = f1_score(y_val_enc, y_val_pred, pos_label=1)\n",
        "\n",
        "    print(f\"F1-score (validation): {f1:.4f}\")\n",
        "\n",
        "    # Store model and performance for later comparison\n",
        "    xgb_results[emb_name] = {\n",
        "        \"model\": xgb_emb,\n",
        "        \"f1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3FfoqGnjJ4u"
      },
      "source": [
        "The results show that Word2Vec embeddings achieve the best performance with XGBoost,\n",
        "obtaining the highest F1-score on the validation set. GloVe Twitter and GloVe Wikipedia\n",
        "perform slightly worse, while FastText, despite its higher dimensionality, does not\n",
        "provide an improvement in F1-score. This suggests that simpler, domain-aligned\n",
        "embeddings (Word2Vec) are more effective for this classification task than larger or\n",
        "more general-purpose embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrKOyf6WW3fz"
      },
      "outputs": [],
      "source": [
        "# Select the embedding that achieved the highest F1-score on the validation set\n",
        "best_emb_name = max(\n",
        "    xgb_results,\n",
        "    key=lambda k: xgb_results[k][\"f1\"]\n",
        ")\n",
        "\n",
        "# Retrieve the corresponding trained XGBoost model\n",
        "best_xgb_emb = xgb_results[best_emb_name][\"model\"]\n",
        "\n",
        "# Retrieve the corresponding F1-score\n",
        "best_f1 = xgb_results[best_emb_name][\"f1\"]\n",
        "\n",
        "# Print summary of the best embedding configuration\n",
        "print(\"\\nBEST XGBOOST EMBEDDING\")\n",
        "print(\"Embedding:\", best_emb_name)\n",
        "print(\"F1-score:\", best_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7p1Scnvjn35"
      },
      "source": [
        "FastText outperforms the other embeddings, likely due to its ability to model subword information and rare tokens, which are common in jailbreak prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IT_gNsVW3fz"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(\n",
        "    clf,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    pos_label=1,\n",
        "    neg_label=0,\n",
        "    show_plots=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate a trained classifier on a validation set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    clf : trained classifier\n",
        "        Model implementing predict() and optionally predict_proba() or decision_function()\n",
        "    X_val : array-like\n",
        "        Validation features\n",
        "    y_val : array-like\n",
        "        True labels (numeric: 0/1)\n",
        "    pos_label : int\n",
        "        Label of the positive class (default: 1)\n",
        "    neg_label : int\n",
        "        Label of the negative class (default: 0)\n",
        "    show_plots : bool\n",
        "        Whether to show confusion matrix, ROC and Precision-Recall curves\n",
        "    \"\"\"\n",
        "\n",
        "    from sklearn.metrics import (\n",
        "        accuracy_score,\n",
        "        precision_score,\n",
        "        recall_score,\n",
        "        f1_score,\n",
        "        ConfusionMatrixDisplay,\n",
        "        RocCurveDisplay,\n",
        "        PrecisionRecallDisplay\n",
        "    )\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = clf.predict(X_val)\n",
        "\n",
        "    # Scores for ROC / PR curves\n",
        "    y_score = None\n",
        "\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        proba = clf.predict_proba(X_val)\n",
        "        pos_idx = list(clf.classes_).index(pos_label)\n",
        "        y_score = proba[:, pos_idx]\n",
        "\n",
        "    elif hasattr(clf, \"decision_function\"):\n",
        "        y_score = clf.decision_function(X_val)\n",
        "\n",
        "    # Metrics\n",
        "    print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_val, y_pred, pos_label=pos_label, zero_division=0))\n",
        "    print(\"Recall   :\", recall_score(y_val, y_pred, pos_label=pos_label, zero_division=0))\n",
        "    print(\"F1-score :\", f1_score(y_val, y_pred, pos_label=pos_label, zero_division=0))\n",
        "\n",
        "    # Plots\n",
        "    if show_plots:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        ConfusionMatrixDisplay.from_predictions(\n",
        "            y_val,\n",
        "            y_pred,\n",
        "            display_labels=[neg_label, pos_label],\n",
        "            ax=axes[0]\n",
        "        )\n",
        "        axes[0].set_title(\"Confusion Matrix\")\n",
        "\n",
        "        # ROC + Precision-Recall (only if scores are available)\n",
        "        if y_score is not None:\n",
        "            RocCurveDisplay.from_predictions(\n",
        "                y_val,\n",
        "                y_score,\n",
        "                pos_label=pos_label,\n",
        "                ax=axes[1]\n",
        "            )\n",
        "            axes[1].set_title(\"ROC Curve\")\n",
        "\n",
        "            PrecisionRecallDisplay.from_predictions(\n",
        "                y_val,\n",
        "                y_score,\n",
        "                pos_label=pos_label,\n",
        "                ax=axes[2]\n",
        "            )\n",
        "            axes[2].set_title(\"Precision-Recall Curve\")\n",
        "        else:\n",
        "            axes[1].set_axis_off()\n",
        "            axes[2].set_axis_off()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLKHKSOKj3P1"
      },
      "outputs": [],
      "source": [
        "X_val_best = other_embeddings[\"val\"][best_emb_name]\n",
        "\n",
        "evaluate_classifier(\n",
        "    clf=best_xgb_emb,\n",
        "    X_val=X_val_best,\n",
        "    y_val=y_val_enc,\n",
        "    pos_label=1,\n",
        "    neg_label=0,\n",
        "    show_plots=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRC6IzpkQxB"
      },
      "source": [
        "The XGBoost model with FastText embeddings achieves strong performance on the validation set, with an F1-score of 0.80. The high ROC-AUC (0.91) and Average Precision (0.87) indicate good discriminative ability, while the confusion matrix shows a balanced trade-off between false positives and false negatives, making this model suitable for jailbreak detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18aJyfHa1Z2W"
      },
      "source": [
        "now let's try some different parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NSm1PGDW3f0"
      },
      "outputs": [],
      "source": [
        "# List of values to test for the number of trees (n_estimators)\n",
        "list_h_param_values = [10, 50, 100, 200]\n",
        "\n",
        "# Factory function that creates an XGBoost classifier\n",
        "# using the provided value as number of estimators\n",
        "xgb_factory = lambda C: XGBClassifier(\n",
        "    n_estimators=int(C),     # number of boosting trees\n",
        "    max_depth=10,            # maximum depth of each tree\n",
        "    n_jobs=6,                # parallel threads\n",
        "    tree_method=\"hist\",      # histogram-based algorithm (faster)\n",
        "    eval_metric=\"logloss\"    # evaluation metric for binary classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BlZISoltxW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def sweep_model_C(\n",
        "    model_factory,\n",
        "    X_train_vector,\n",
        "    y_train,\n",
        "    X_val_vector,\n",
        "    y_val,\n",
        "    pos_label,\n",
        "    C_values,\n",
        "    model_name=\"model sweep\"\n",
        "):\n",
        "    results = {}\n",
        "\n",
        "    for C in C_values:\n",
        "        clf = model_factory(C)\n",
        "        clf.fit(X_train_vector, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_val_vector)\n",
        "\n",
        "        results[C] = {\n",
        "            \"accuracy\": accuracy_score(y_val, y_pred),\n",
        "            \"precision\": precision_score(y_val, y_pred, pos_label=pos_label, zero_division=0),\n",
        "            \"recall\": recall_score(y_val, y_pred, pos_label=pos_label, zero_division=0),\n",
        "            \"f1\": f1_score(y_val, y_pred, pos_label=pos_label, zero_division=0),\n",
        "        }\n",
        "\n",
        "        print(f\"\\n[{model_name}] C={C}\")\n",
        "        print(\" Accuracy :\", results[C]['accuracy'])\n",
        "        print(\" Precision:\", results[C]['precision'])\n",
        "        print(\" Recall   :\", results[C]['recall'])\n",
        "        print(\" F1       :\", results[C]['f1'])\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEGwULbeW3f0"
      },
      "outputs": [],
      "source": [
        "# This block performs a hyperparameter sweep over different data representations\n",
        "# (Bag-of-Words and all available embeddings) using XGBoost.\n",
        "# For each representation, we vary the number of estimators and evaluate\n",
        "#performance on the validation set using F1-score for the positive class.\n",
        "results_xgb_sweep = {}\n",
        "\n",
        "for rep_name in all_representations:\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 sweep_model_C per XGBoost su rappresentazione: {rep_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # --- Recupero i vettori corretti ---\n",
        "    if rep_name == \"bow\":\n",
        "        X_tr = X_train_vector\n",
        "        X_valr = X_val_vector\n",
        "    else:\n",
        "        X_tr = other_embeddings[\"train\"][rep_name]\n",
        "        X_valr = other_embeddings[\"val\"][rep_name]\n",
        "\n",
        "    # --- Sweep ---\n",
        "    res = sweep_model_C(\n",
        "        model_factory=xgb_factory,\n",
        "        X_train_vector=X_tr,\n",
        "        y_train=y_train_enc,\n",
        "        X_val_vector=X_valr,\n",
        "        y_val=y_val_enc,\n",
        "        pos_label=1,\n",
        "        C_values=list_h_param_values,\n",
        "        model_name=f\"XGBoost sweep ({rep_name})\"\n",
        "    )\n",
        "\n",
        "    results_xgb_sweep[rep_name] = res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3T5B3-4m9CU"
      },
      "source": [
        "Bag-of-Words yields the best overall performance for XGBoost, suggesting that explicit lexical cues are more informative than dense semantic embeddings for jailbreak detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xH_BlBjW3f1"
      },
      "outputs": [],
      "source": [
        "all_representations = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmGAYLuMW3f1"
      },
      "outputs": [],
      "source": [
        "# Candidate values for the hyperparameter we want to sweep.\n",
        "# Here we vary n_estimators (number of trees) to see how model capacity affects validation performance.\n",
        "list_h_param_values = [10, 50, 100, 200]\n",
        "\n",
        "# XGBoost expects numeric labels, so we convert string labels to binary:\n",
        "#   \"jailbreak\" -> 1 (positive class), everything else -> 0 (benign)\n",
        "y_train_xgb = (y_train == \"jailbreak\").astype(int)\n",
        "y_test_xgb  = (y_test  == \"jailbreak\").astype(int)\n",
        "y_val_xgb   = (y_val   == \"jailbreak\").astype(int)\n",
        "\n",
        "# Sanity check: should print [0 1] if both classes are present in the training split\n",
        "print(np.unique(y_train_xgb))\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Factory function: given a value (C), build an XGBoost model with that number of trees.\n",
        "# NOTE: the variable name \"C\" is just a placeholder here; it actually controls n_estimators.\n",
        "xgb_factory = lambda C: XGBClassifier(\n",
        "    n_estimators=int(C),   # number of boosting rounds / trees\n",
        "    max_depth=10,          # fixed depth for all sweeps (controls tree complexity)\n",
        "    n_jobs=6,              # parallelism\n",
        "    tree_method=\"hist\",    # faster histogram-based algorithm (CPU-friendly)\n",
        "    eval_metric=\"logloss\"  # required by xgboost; we still evaluate with F1 in sweep_model_C\n",
        ")\n",
        "\n",
        "# Run a 1D hyperparameter sweep on the original BoW/CountVectorizer representation.\n",
        "# For each n_estimators value, we train on X_train_vector and compute metrics on X_val_vector.\n",
        "results_xgb = sweep_model_C(\n",
        "    model_factory=xgb_factory,\n",
        "    X_train_vector=X_train_vector,\n",
        "    y_train=y_train_xgb,\n",
        "    X_val_vector=X_val_vector,\n",
        "    y_val=y_val_xgb,\n",
        "    pos_label=1,                     # positivo = jailbreak\n",
        "    C_values=list_h_param_values,\n",
        "    model_name=\"XGBoost (n_estimators sweep)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWOy_WKnoNB7"
      },
      "source": [
        "The sweep over the number of estimators shows a consistent performance across all configurations. Increasing the number of trees slightly improves the F1-score, with the best result achieved at n_estimators = 200. The improvements are marginal, indicating that the model is already stable with fewer trees and does not strongly benefit from further increases in complexity. This suggests a good bias–variance trade-off and no evident overfitting as the number of estimators grows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "171AKxLpW3f1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "f1_pos = make_scorer(f1_score, pos_label=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJkXBKvDW3f2"
      },
      "source": [
        "A way to measure the \"importance\" of each feature is given by some models. For instance:\n",
        "- the `feature_importances_` attribute of XGBoost classifiers\n",
        "- the `plot_importance` method of XGBoost\n",
        "- the `feature_importances_` attribute of Random Forests\n",
        "Let's try to work on the `plot_importance` from xgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JodzadkW3f2"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzg0fcRdW3f2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "\n",
        "# 1) Train an XGBoost model directly on the BoW features\n",
        "xgb_clf = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    n_jobs=6,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "xgb_clf.fit(X_train_vector, y_train_enc)\n",
        "\n",
        "# 2) Retrieve feature names from the CountVectorizer\n",
        "feature_names = vectorizer.get_feature_names_out().tolist()\n",
        "\n",
        "# 3) Attach feature names to the booster for proper visualization\n",
        "booster = xgb_clf.get_booster()\n",
        "booster.feature_names = feature_names\n",
        "\n",
        "# 4) Plot the top 20 most important features\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "plot_importance(\n",
        "    booster,\n",
        "    ax=ax,\n",
        "    height=0.8,\n",
        "    importance_type=\"weight\",   # how often a feature is used for splits\n",
        "    xlabel=\"Feature importance\",\n",
        "    show_values=False,\n",
        "    max_num_features=20\n",
        ")\n",
        "\n",
        "plt.title(\"XGBoost Feature Importance (Top 20)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPG1XyO3W3f2"
      },
      "outputs": [],
      "source": [
        "# Extract feature importance scores from the trained XGBoost model.\n",
        "# The resulting dictionary maps each feature name to its importance\n",
        "# based on how often it is used to split the data across all trees.\n",
        "feature_importances_dict = xgb_clf.get_booster().get_fscore()\n",
        "feature_importances_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyKCf0NpW3f2"
      },
      "source": [
        "Plot features importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cwttCnXW3f2"
      },
      "outputs": [],
      "source": [
        "def select_features(feature_importances, fraction=0.1, top=True):\n",
        "    if not 0 < fraction <= 1:\n",
        "        raise ValueError(\"fraction must be between 0 (exclusive) and 1 (inclusive)\")\n",
        "\n",
        "    values = np.array(list(feature_importances.values()))\n",
        "    # For top x%, threshold should be (1 - fraction) quantile\n",
        "    threshold = np.quantile(values, 1 - fraction if top else fraction)\n",
        "\n",
        "    if top:\n",
        "        return {k: v for k, v in feature_importances.items() if v >= threshold}\n",
        "    else:\n",
        "        return {k: v for k, v in feature_importances.items() if v <= threshold}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z61yJ8wmW3f2"
      },
      "outputs": [],
      "source": [
        "# Select the bottom 5% least important features according to XGBoost\n",
        "# These features contribute the least to the model decisions\n",
        "irrelevant_columns = select_features(feature_importances_dict, 0.05, top=False)\n",
        "\n",
        "# Display the least important features\n",
        "irrelevant_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv-MI7JWW3f3"
      },
      "outputs": [],
      "source": [
        "list(irrelevant_columns.items())[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhjfdUbTW3f3"
      },
      "outputs": [],
      "source": [
        "len(irrelevant_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNuBB50YW3f3"
      },
      "source": [
        "Analysis of the Least Important Features in XGBoost\n",
        "\n",
        "The analysis of the bottom 5% least important features shows that they are mainly common, semantically neutral English words such as “about”, “always”, “answer”, “both”, and “business”. These terms appear frequently across both classes (benign and jailbreak) and do not carry meaningful discriminative information. As a result, XGBoost correctly assigns them minimal importance, since they do not contribute to separating normal user inputs from jailbreak attempts.\n",
        "\n",
        "The fact that only a relatively small portion of features falls into this “irrelevant” category suggests that the vectorization process produces a reasonably clean and informative feature space. Overall, no anomalies are observed among the low-importance features: the model is effectively filtering out linguistic noise and focusing on the terms that provide real predictive value for jailbreak detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnfh8RbsW3f3"
      },
      "outputs": [],
      "source": [
        "relevant_columns = select_features(feature_importances_dict, 0.05, top=True)\n",
        "relevant_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StO5pnkkW3f3"
      },
      "outputs": [],
      "source": [
        "relevant_columns = list(relevant_columns.keys())\n",
        "relevant_columns# let's take a look at the distribution of values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b71jqcZ9W3f3"
      },
      "source": [
        "let's try to remove all irrelevant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj_h787lW3f3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# 1. Map irrelevant feature names → indices\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "irrelevant_feature_indices = [\n",
        "    np.where(feature_names == feat)[0][0]\n",
        "    for feat in irrelevant_columns.keys()\n",
        "]\n",
        "\n",
        "# 2. Function to drop columns from CSR matrix\n",
        "def drop_columns_csr(mat, cols_to_drop):\n",
        "    mask = np.ones(mat.shape[1], dtype=bool)\n",
        "    mask[cols_to_drop] = False\n",
        "    return mat[:, mask]\n",
        "\n",
        "# 3. Apply reduction\n",
        "X_train_reduced = drop_columns_csr(X_train_vector, irrelevant_feature_indices)\n",
        "X_test_reduced  = drop_columns_csr(X_test_vector, irrelevant_feature_indices)\n",
        "X_val_reduced  = drop_columns_csr(X_val_vector, irrelevant_feature_indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz3A1S8W3f3"
      },
      "source": [
        "Fit the model using the subset of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbPRZmympl58"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels to 0/1 for XGBoost (benign=0, jailbreak=1)\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTJP8QEUqJL1"
      },
      "outputs": [],
      "source": [
        "clf = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    n_jobs=6,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Register start time\n",
        "t_start = time.time()\n",
        "\n",
        "# Train the classifier on the reduced feature set\n",
        "clf.fit(X_train_reduced, y_train_enc)\n",
        "\n",
        "# Register end time\n",
        "t_stop = time.time()\n",
        "\n",
        "print(f\"Training time: {t_stop - t_start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwCm9cmdq8Nd"
      },
      "outputs": [],
      "source": [
        "y_val_enc = le.transform(y_val)\n",
        "\n",
        "evaluate_classifier(\n",
        "    clf=clf,\n",
        "    X_val=X_val_reduced,\n",
        "    y_val=y_val_enc,\n",
        "    pos_label=1,\n",
        "    neg_label=0,\n",
        "    show_plots=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8RFAdturW6y"
      },
      "source": [
        "The XGBoost model trained on the reduced feature set shows strong discriminative performance on the validation set. The high ROC–AUC (0.92) and Average Precision (0.89) indicate good class separability, while the confusion matrix highlights a balanced trade-off between precision (0.81) and recall (0.79) for the jailbreak class. Overall, the model effectively detects jailbreak prompts while maintaining a low false positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pI71cZeW3f5"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode string labels (\"benign\"/\"jailbreak\") into numeric labels (0/1) required by XGBoost\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)   # e.g., ['benign','jailbreak'] -> [0,1]\n",
        "y_test_enc  = le.transform(y_test)        # same mapping applied to test labels\n",
        "\n",
        "# Sanity check: prints the class order used by the encoder (defines which class is 0 and which is 1)\n",
        "print(le.classes_)   # expected: ['benign' 'jailbreak']\n",
        "\n",
        "# Define an XGBoost classifier (baseline configuration) to train on the reduced feature space\n",
        "xgb_clf = XGBClassifier(\n",
        "    n_estimators=10,        # number of trees (kept small here for a fast baseline)\n",
        "    max_depth=10,           # maximum tree depth (controls model complexity)\n",
        "    n_jobs=6,               # CPU parallelism\n",
        "    tree_method=\"hist\",     # faster histogram-based training\n",
        "    eval_metric=\"logloss\"   # evaluation metric for training\n",
        ")\n",
        "\n",
        "# Fit XGBoost on the reduced training matrix (features selected/reduced earlier)\n",
        "xgb_clf.fit(X_train_reduced, y_train_enc)\n",
        "\n",
        "# Evaluate the model on the reduced validation matrix using the numeric labels (pos_label=1 => \"jailbreak\")\n",
        "evaluate_classifier(\n",
        "    clf=xgb_clf,\n",
        "    X_val=X_val_reduced,    # reduced validation features (must match X_train_reduced columns)\n",
        "    y_val=y_val_enc,        # numeric validation labels produced by the same LabelEncoder\n",
        "    pos_label=1,            # positive class index (jailbreak)\n",
        "    neg_label=0,            # negative class index (benign)\n",
        "    show_plots=True,        # show confusion matrix, ROC, and PR curves (if scores are available)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLS2O6kfroul"
      },
      "source": [
        "Model performance on Reduced Features:\n",
        "the model trained on reduced features performs better and requires fewer trees to peak.\n",
        "\n",
        "Peak: The best result is achieved at C=50 with an F1 score of 0.809 (Accuracy ~84.5%), which is higher than the best result of the full feature set.\n",
        "\n",
        "Behavior: At C=10, it underperforms (F1 0.77), suggesting it needs a minimum capacity to grasp the patterns in the reduced space. However, after C=50, performance drops slightly, indicating potential overfitting or noise capture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpREAnXCW3f6"
      },
      "outputs": [],
      "source": [
        "# Define an F1 scorer focused on the positive class (jailbreak = 1)\n",
        "# This is important because the task is asymmetric and jailbreak detection is the priority\n",
        "f1_pos = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "# Initialize a base XGBoost classifier\n",
        "# No hyperparameters related to model capacity are fixed here,\n",
        "# since they will be explored via GridSearch\n",
        "xgb = XGBClassifier(\n",
        "    tree_method=\"hist\",     # histogram-based training for efficiency\n",
        "    eval_metric=\"logloss\",  # standard loss for binary classification\n",
        "    n_jobs=6,               # parallel computation\n",
        "    random_state=42         # reproducibility\n",
        ")\n",
        "\n",
        "# Define the hyperparameter grid to explore\n",
        "# - n_estimators controls the number of trees\n",
        "# - max_depth controls tree complexity\n",
        "param_grid = {\n",
        "    \"n_estimators\": [10, 50],\n",
        "    \"max_depth\": [5, 10],\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV to perform cross-validated hyperparameter tuning\n",
        "# The model is selected based on F1-score on the positive class\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring=f1_pos,   # optimize for jailbreak detection\n",
        "    cv=3,             # 3-fold cross-validation\n",
        "    n_jobs=-1         # use all available cores\n",
        ")\n",
        "\n",
        "# Fit the grid search on the reduced training feature space\n",
        "# Labels are numeric (0 = benign, 1 = jailbreak)\n",
        "grid.fit(X_train_reduced, y_train_enc)\n",
        "\n",
        "# Retrieve the best-performing model according to cross-validation\n",
        "best_xgb_reduced = grid.best_estimator_\n",
        "\n",
        "# Print the best hyperparameter configuration found\n",
        "print(\"Best params (reduced):\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8HSxU5CW3f6"
      },
      "outputs": [],
      "source": [
        "evaluate_classifier(\n",
        "    clf=best_xgb_reduced,\n",
        "    X_val=X_val_reduced,\n",
        "    y_val=y_val_enc,\n",
        "    pos_label=1,\n",
        "    neg_label=0,\n",
        "    show_plots=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBBQB35WW3f6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Hyperparameter values to sweep (number of trees)\n",
        "list_h_param_values = [10, 50, 100, 200]\n",
        "\n",
        "# Convert string labels to numeric labels (0/1)\n",
        "# Required by XGBoost\n",
        "#   1 = jailbreak\n",
        "#   0 = benign\n",
        "y_train_xgb = (y_train == \"jailbreak\").astype(int)\n",
        "y_val_xgb   = (y_val == \"jailbreak\").astype(int)\n",
        "\n",
        "# Sanity check: should print [0 1]\n",
        "print(np.unique(y_train_xgb))\n",
        "\n",
        "# Factory function that creates an XGBoost model\n",
        "# using C as the number of estimators (trees)\n",
        "xgb_factory = lambda C: XGBClassifier(\n",
        "    n_estimators=int(C),   # number of trees\n",
        "    max_depth=10,          # fixed tree depth\n",
        "    n_jobs=6,              # parallel computation\n",
        "    tree_method=\"hist\",    # fast histogram-based training\n",
        "    eval_metric=\"logloss\"  # training loss\n",
        ")\n",
        "\n",
        "# Hyperparameter sweep on the reduced feature space\n",
        "results_xgb = sweep_model_C(\n",
        "    model_factory=xgb_factory,\n",
        "    X_train_vector=X_train_reduced,  # reduced training features\n",
        "    y_train=y_train_xgb,             # numeric training labels\n",
        "    X_val_vector=X_val_reduced,      # reduced validation features\n",
        "    y_val=y_val_xgb,                 # numeric validation labels\n",
        "    pos_label=1,                     # positive class = jailbreak\n",
        "    C_values=list_h_param_values,    # values to sweep\n",
        "    model_name=\"XGBoost (n_estimators sweep)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lMc3O8kW3f6"
      },
      "source": [
        "#### Random Forest\n",
        "\n",
        "You can find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNMPrDEVW3f6"
      },
      "source": [
        "##### Model fitting and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK8drHd3W3f7"
      },
      "source": [
        "Create classifier instance and fit it on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLU5fndSW3f7"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH7KJZtMW3f7"
      },
      "source": [
        "Fit classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK2oR8pZW3f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1) Define a Random Forest classifier\n",
        "# - n_estimators: number of trees in the forest\n",
        "# - max_depth: maximum depth of each tree (controls model complexity)\n",
        "# - n_jobs: use all available CPU cores\n",
        "# - random_state: ensures reproducibility\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2) Train the Random Forest on the vectorized training data (BoW / TF-IDF)\n",
        "rf_clf.fit(X_train_vector, y_train)\n",
        "\n",
        "# 3) Evaluate the model on the validation set\n",
        "# Labels are kept as strings (\"benign\" / \"jailbreak\"),\n",
        "# since RandomForest supports non-numeric labels directly\n",
        "evaluate_classifier(\n",
        "    clf=rf_clf,\n",
        "    X_val=X_val_vector,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",   # positive class\n",
        "    neg_label=\"benign\",      # negative class\n",
        "    show_plots=True          # show confusion matrix, ROC, and PR curves\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ecmyzwes6Zq"
      },
      "source": [
        "The Random Forest model achieves high precision but low recall on the jailbreak class. This indicates a conservative behavior: when the model predicts jailbreak it is usually correct, but it fails to identify a large portion of actual jailbreak prompts. Overall performance is therefore limited by the high number of false negatives, making the model less suitable when recall on jailbreaks is critical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-mmt0b-W3f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "f1_jb = make_scorer(f1_score, pos_label=\"jailbreak\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9no3Y0SW3f8"
      },
      "source": [
        "##### Features importance\n",
        "\n",
        "Similarly to XGBoost we can also visualise features importance of random forests.\n",
        "You can access this information from the same `feature_importances_` attribute, that returns the importance of each feature. You can find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_).\n",
        "\n",
        "**However, it is important not to trust blindly the values returned by such attribute**, as it only represents \"the (normalized) total reduction of the criterion brought by that feature\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZrA1GlW3f9"
      },
      "source": [
        " for now fit with random parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTwmMgJhW3f9"
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfvsQHMW3f9"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "<b>\n",
        "    Q:\n",
        "    Use the following line to create a dict that maps each column to its importance (as computed by the XGBoost model).\n",
        "</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiqOrCbTW3f9"
      },
      "outputs": [],
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_importances_dict = dict(zip(feature_names, clf.feature_importances_))\n",
        "\n",
        "feature_importances_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU8oz5bMW3f9"
      },
      "source": [
        "Plot features importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZusgqfmW3f9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort features by importance score in descending order\n",
        "sorted_features = sorted(\n",
        "    feature_importances_dict.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# Select the top 20 most important features\n",
        "top_features = sorted_features[:20]\n",
        "\n",
        "# Separate feature names and their importance values\n",
        "names = [x[0] for x in top_features]\n",
        "values = [x[1] for x in top_features]\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(names, values)\n",
        "\n",
        "# Use a logarithmic scale to better visualize differences\n",
        "plt.xscale(\"log\")\n",
        "\n",
        "# Invert y-axis so the most important feature appears at the top\n",
        "plt.gca().invert_yaxis()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-WpXFmRW3f9"
      },
      "source": [
        "let's look at the irrelevant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N5VWYYYW3f9"
      },
      "outputs": [],
      "source": [
        "irrelevant_columns = select_features(feature_importances_dict, fraction=0.05, top=False)\n",
        "irrelevant_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's look at the relevant ones"
      ],
      "metadata": {
        "id": "8JQJtAths2MK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sFQjklsW3f-"
      },
      "outputs": [],
      "source": [
        "relevant_columns = select_features(feature_importances_dict, 0.05, top=True)\n",
        "relevant_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5PhIbRW3f-"
      },
      "source": [
        "##### Explaining predictions\n",
        "\n",
        "In addition to making predictions with your models, is very important to explain them, to make you models more intepretable.\n",
        "Today, we are going to see LIME a tool for explainability that works with any model.\n",
        "\n",
        "LIME stands for Local Interpretable Model-agnostic Explanations. LIME focuses on training local surrogate linear models to explain individual predictions. Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models.\n",
        "\n",
        "LIME is model-agnostic, meaning that it can be applied to any machine learning model. The technique attempts to understand the model by perturbing the input of data samples and understanding how the predictions change.\n",
        "\n",
        "Why Lime?\n",
        "\n",
        "The intuition behind LIME is very simple. First, forget the training data and imagine we have only the black box model where we supply the input data. The black box model generates the predictions for the model. We can enquire the box as many times as we like. Our objective is to understand why the machine learning model made a certain prediction.\n",
        "\n",
        "Now, LIME comes into play. LIME tests what happens to the predictions when we provide variations in the data which is being fed into the machine learning model.\n",
        "\n",
        "LIME generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model. It is weighted by the proximity of the sampled instances to the instance of interest. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called local fidelity.\n",
        "\n",
        "The user has to determine the complexity, e.g. by selecting the maximum number of features that the linear regression model may use.\n",
        "\n",
        "The following example is based on the material avaialble at this [link](https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTR7ax7_W3f-"
      },
      "source": [
        "Let's start by installing the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1uRPAP95W3f-"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WCOpmlKW3f-"
      },
      "source": [
        "An then let's import the tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn5YUXULW3f-"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNYKhMkHW3f-"
      },
      "source": [
        "####LIME has one explainer for all the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxa_bRa8W3f-"
      },
      "source": [
        "Select an instance from the test set and use it to explain the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-UYAYR1W3f-"
      },
      "outputs": [],
      "source": [
        "j = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hjWh05FZW3f_"
      },
      "outputs": [],
      "source": [
        "clf.predict(X_val_vector[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQeBtbE5W3f_"
      },
      "outputs": [],
      "source": [
        "y_val.iloc[j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azkHbYcNW3f_"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Initialize a LIME text explainer for binary classification\n",
        "# The class_names order must match the model's label encoding\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=[\"benign\", \"jailbreak\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBZSgzLW3f_"
      },
      "outputs": [],
      "source": [
        "j = 42  # index of the validation instance to explain\n",
        "\n",
        "# Original text instance (raw text, not vectorized)\n",
        "text_instance = df.loc[idx_val[j], \"text\"]\n",
        "\n",
        "# Print the ground-truth label for this instance\n",
        "print(\"True label:\", y_val.iloc[j])\n",
        "\n",
        "# Print a short snippet of the text for inspection\n",
        "print(\"Text snippet:\", text_instance[:200], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d76fv_uW3f_"
      },
      "outputs": [],
      "source": [
        "def rf_predict_proba_text(text_list):\n",
        "    \"\"\"\n",
        "    text_list: list of raw text strings\n",
        "    returns: numpy array of shape (n_samples, n_classes) with class probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Vectorize the input texts using the SAME vectorizer used during training\n",
        "    X_vec = vectorizer.transform(text_list)\n",
        "\n",
        "    # 2. Use the already trained RandomForest model to predict class probabilities\n",
        "    return clf.predict_proba(X_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnnAa_kTW3f_"
      },
      "outputs": [],
      "source": [
        "j = 10  # index of the instance to explain\n",
        "\n",
        "# Retrieve the raw text corresponding to the selected validation sample\n",
        "text_j = df.loc[idx_val[j], \"text\"]\n",
        "\n",
        "# Display true label and text preview\n",
        "print(\"True label:\", y_val.iloc[j])\n",
        "print(\"Text snippet:\", text_j[:200], \"...\\n\")\n",
        "\n",
        "# Generate a LIME explanation for the selected instance\n",
        "exp = explainer.explain_instance(\n",
        "    text_j,                 # raw text input\n",
        "    rf_predict_proba_text,  # prediction function (with internal vectorization)\n",
        "    num_features=6,         # number of most influential words to display\n",
        "    labels=[1]              # class to explain (1 = jailbreak)\n",
        ")\n",
        "\n",
        "# Visualize the explanation directly in the notebook\n",
        "exp.show_in_notebook()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivvDJviTW3gA"
      },
      "source": [
        "Show the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGR5jtB_W3gA"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMIUaYc4W3gA"
      },
      "outputs": [],
      "source": [
        "y_val.values[j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcPFc926W3gA"
      },
      "outputs": [],
      "source": [
        "true_label = y_val.iloc[j]   # \"benign\" o \"jailbreak\"\n",
        "\n",
        "\n",
        "lime_explanation_list = exp.as_list(label=1)\n",
        "lime_explanation_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QMLjHUW3gA"
      },
      "source": [
        "We can also get a prediction on all the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkRzsv6yW3gA"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Initialize the LIME text explainer with the class names\n",
        "explainer = LimeTextExplainer(class_names=[\"benign\", \"jailbreak\"])\n",
        "\n",
        "# Wrapper function: converts raw text into vectors and returns class probabilities\n",
        "def rf_predict_proba_text(text_list):\n",
        "    # Vectorize the input texts using the same vectorizer used for training\n",
        "    X_vec = vectorizer.transform(text_list)\n",
        "    # Return probability predictions from the trained Random Forest\n",
        "    return clf.predict_proba(X_vec)\n",
        "\n",
        "# Index of the validation example to explain\n",
        "j = 30\n",
        "\n",
        "# IMPORTANT: use the original raw text, not vectorized features\n",
        "text_j = df.loc[idx_val[j], \"text\"]\n",
        "\n",
        "# Retrieve the index corresponding to the \"jailbreak\" class\n",
        "class_index = list(clf.classes_).index(\"jailbreak\")\n",
        "\n",
        "# Generate a LIME explanation for the selected instance\n",
        "exp = explainer.explain_instance(\n",
        "    text_j,                 # raw input text\n",
        "    rf_predict_proba_text,  # prediction function (model-agnostic)\n",
        "    num_features=100,       # number of features shown in the explanation\n",
        "    labels=[class_index]    # class to explain (jailbreak)\n",
        ")\n",
        "\n",
        "# Display the explanation inside the notebook\n",
        "exp.show_in_notebook()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_CLIPD3W3gA"
      },
      "outputs": [],
      "source": [
        "lime_explanation_list = exp.as_list(label=class_index)\n",
        "lime_explanation_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D28cmVFUW3gA"
      },
      "source": [
        "How can we interpret the result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlZDNLJmWU2Z"
      },
      "source": [
        "# Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21SeYkmm4FYj"
      },
      "source": [
        "we will try to do zero-shot and few-shot evaluation with a reasonably small pretrained LLM on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWLOAXlphkQM"
      },
      "outputs": [],
      "source": [
        "train_df = df.loc[idx_train].copy()\n",
        "val_df   = df.loc[idx_val].copy()\n",
        "test_df  = df.loc[idx_test].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WShlYvtQThv-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate langchain_community #xformers\n",
        "# !pip install -U \"transformers\" \"accelerate\" \"huggingface_hub\" sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmboCRuqThv_"
      },
      "source": [
        "Now load a few libraries from Huggingface transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRPWPdwDThv_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "# from langchain_community.llms import HuggingFacePipeline\n",
        "# from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cgkarv5Thv_"
      },
      "outputs": [],
      "source": [
        "# torch.random.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDnsLz7SSNIo"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "train_df = df.loc[idx_train]\n",
        "val_df   = df.loc[idx_val]\n",
        "test_df  = df.loc[idx_test]\n",
        "\n",
        "print(\"Dataset sizes:\")\n",
        "print(f\" Train: {len(train_df)}\")\n",
        "print(f\" Val:   {len(val_df)}\")\n",
        "print(f\" Test:  {len(test_df)}\")\n",
        "print(\"\\nLabel distribution (train):\")\n",
        "print(train_df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zVGTF_zeHNV"
      },
      "outputs": [],
      "source": [
        "##!must activate GPU on Colab to run this part\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "print(f\"Loading model {model_name} ...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✅ Model loaded successfully (Phi-3 Mini).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QUnyiAHShtw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_response(prompt, max_new_tokens=4, temperature=0.0):\n",
        "    \"\"\"\n",
        "    Genera a response dall'LLM.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=(temperature > 0),\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = full_output[len(prompt):].strip()\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3NzI4SPSjUQ"
      },
      "outputs": [],
      "source": [
        "# let's extract some prompts and labels to use\n",
        "import numpy as np\n",
        "\n",
        "N_FEWSHOT_EXAMPLES = 2\n",
        "RANDOM_SEED = random_seed\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "benign_examples = train_df[train_df[\"label\"] == \"benign\"].sample(\n",
        "    min(N_FEWSHOT_EXAMPLES, (train_df[\"label\"] == \"benign\").sum()),\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "jailbreak_examples = train_df[train_df[\"label\"] == \"jailbreak\"].sample(\n",
        "    min(N_FEWSHOT_EXAMPLES, (train_df[\"label\"] == \"jailbreak\").sum()),\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "print(\"Few-shot examples extracted:\")\n",
        "print(f\" benign:    {len(benign_examples)}\")\n",
        "print(f\" jailbreak: {len(jailbreak_examples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E9foQxLSmAG"
      },
      "outputs": [],
      "source": [
        "#zero_shot\n",
        "\n",
        "def classify_prompt_zero_shot(prompt_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Classifica prompt come benign o jailbreak usando zero-shot prompting.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a security classifier. Classify the following prompt as either:\n",
        "- benign = safe, harmless\n",
        "- jailbreak = attempts to bypass safety or elicit harmful instructions.\n",
        "\n",
        "Respond with exactly one word: benign or jailbreak.\n",
        "\n",
        "Prompt:\n",
        "\\\"\\\"\\\"{prompt_text[:800]}\\\"\\\"\\\"\n",
        "\n",
        "Classification:\n",
        "\"\"\"\n",
        "    resp = generate_response(prompt).lower()\n",
        "\n",
        "    if \"jailbreak\" in resp:\n",
        "        return \"jailbreak\"\n",
        "    if \"benign\" in resp:\n",
        "        return \"benign\"\n",
        "    return \"benign\"   # fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA2VpDBcSn-T"
      },
      "outputs": [],
      "source": [
        "#few-shot\n",
        "\n",
        "def classify_prompt_few_shot(prompt_text: str, n_examples: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Few-shot classification usando esempi reali del tuo training set.\n",
        "    \"\"\"\n",
        "\n",
        "    n_ben = min(n_examples, len(benign_examples))\n",
        "    n_jb  = min(n_examples, len(jailbreak_examples))\n",
        "\n",
        "    prompt = \"You are an expert security classifier.\\n\"\n",
        "    prompt += \"Classify each prompt as 'benign' or 'jailbreak'.\\n\\n\"\n",
        "\n",
        "    # jailbreak examples\n",
        "    for i in range(n_jb):\n",
        "        ex = str(jailbreak_examples.iloc[i][\"text\"])[:400]\n",
        "        prompt += f\"Prompt: {ex}\\nClassification: jailbreak\\n\\n\"\n",
        "\n",
        "    # benign examples\n",
        "    for i in range(n_ben):\n",
        "        ex = str(benign_examples.iloc[i][\"text\"])[:400]\n",
        "        prompt += f\"Prompt: {ex}\\nClassification: benign\\n\\n\"\n",
        "\n",
        "    # target prompt\n",
        "    prompt += f\"Prompt: {prompt_text[:800]}\\nClassification: \"\n",
        "\n",
        "    resp = generate_response(prompt).lower()\n",
        "\n",
        "    if \"jailbreak\" in resp:\n",
        "        return \"jailbreak\"\n",
        "    if \"benign\" in resp:\n",
        "        return \"benign\"\n",
        "    return \"benign\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDJHaB_kSplI"
      },
      "outputs": [],
      "source": [
        "#evaluation\n",
        "def evaluate_llm_classifier(classifier_func, df_split, desc=\"Eval\"):\n",
        "    y_true = df_split[\"label\"].tolist()\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in tqdm(df_split.iterrows(), total=len(df_split), desc=desc):\n",
        "        y_pred.append(classifier_func(row[\"text\"]))\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, pos_label=\"jailbreak\")\n",
        "    rec = recall_score(y_true, y_pred, pos_label=\"jailbreak\")\n",
        "    f1  = f1_score(y_true, y_pred, pos_label=\"jailbreak\")\n",
        "\n",
        "    print(f\"\\nResults: {desc}\")\n",
        "    print(f\" Accuracy : {acc:.4f}\")\n",
        "    print(f\" Precision: {prec:.4f}\")\n",
        "    print(f\" Recall   : {rec:.4f}\")\n",
        "    print(f\" F1-score : {f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKHpK8e3Sreg"
      },
      "outputs": [],
      "source": [
        "print(\"=== ZERO-SHOT on VAL ===\")\n",
        "results_zero_val = evaluate_llm_classifier(\n",
        "    classify_prompt_zero_shot,\n",
        "    val_df,\n",
        "    desc=\"LLM Zero-Shot (Validation)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP8zl03v5SCb"
      },
      "source": [
        "it isn't as good as the best models we have trained so far, but it's still decent, especially considering that it wasn't trained specifically for our classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxZCeOd3SspA"
      },
      "outputs": [],
      "source": [
        "print(\"=== FEW-SHOT on VAL (2 examples per class) ===\")\n",
        "results_few_val = evaluate_llm_classifier(\n",
        "    lambda txt: classify_prompt_few_shot(txt, n_examples=2),\n",
        "    val_df,\n",
        "    desc=\"LLM Few-Shot (Validation)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, the few-shot setting underperforms compared to zero-shot in this experiment.  \n",
        "While few-shot prompting usually helps small LLMs by providing concrete examples, in our case it may actually hurt performance due to several factors:\n",
        "\n",
        "1. **Small number of examples:**  \n",
        "   We only provide two examples per class. If these examples are not fully representative, the model may overfit to their style or wording, reducing its ability to generalize.\n",
        "\n",
        "2. **Phi-3 Mini’s limited context reasoning:**  \n",
        "   Although Phi-3 Mini is strong for its size, its reasoning ability with many in-context examples is limited.  \n",
        "   Adding extra demonstrations increases prompt complexity, which may confuse the model rather than guide it.\n",
        "\n",
        "3. **Prompt length and structure:**  \n",
        "   The few-shot prompt becomes longer and more verbose. Small LLMs sometimes degrade with long instructions or mixed-format demonstrations.\n",
        "\n",
        "4. **Distribution shift between examples and test prompts:**  \n",
        "   If the few-shot examples differ too much from the inputs in the validation/test set, the model might be biased toward incorrect patterns.\n",
        "\n",
        "Because of these reasons, the zero-shot classifier ends up being cleaner and more stable, while the few-shot prompt introduces noise and biases that degrade performance.\n"
      ],
      "metadata": {
        "id": "tGGYGsXxbV-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB8RqtO9SvUg"
      },
      "outputs": [],
      "source": [
        "print(\"=== ZERO-SHOT on TEST ===\")\n",
        "results_zero_test = evaluate_llm_classifier(\n",
        "    classify_prompt_zero_shot,\n",
        "    test_df,\n",
        "    desc=\"LLM Zero-Shot (Test)\"\n",
        ")\n",
        "\n",
        "print(\"\\n=== FEW-SHOT on TEST ===\")\n",
        "results_few_test = evaluate_llm_classifier(\n",
        "    lambda txt: classify_prompt_few_shot(txt, n_examples=2),\n",
        "    test_df,\n",
        "    desc=\"LLM Few-Shot (Test)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocXdY2ccVn0Y"
      },
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL-bhhCGBmJm"
      },
      "source": [
        "## Model and hyperparameters selection:\n",
        "\n",
        "Now we will use grid search to optimize some of the parameters of our models automatically.\n",
        "we will save our \"best\" models in the aformentioned dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElyxkAPQ5ZYl"
      },
      "source": [
        "##SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZSDiPj9BmJm"
      },
      "source": [
        "### Cross-validation based hyperparameter selection using Grid Search\n",
        "\n",
        "Grid Search is an exhaustive search over specified parameter values for an estimator.\n",
        "Scikit-Learn implements it in `GridSearchCV`, [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) you can find the documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISbJ7rI5zDL0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHU7f_cBmJm"
      },
      "source": [
        "We can try for example looking for the best regularisation parameter for a SVM on our data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwnB1eSMBmJm"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dmlb1LuBmJm"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NhW_fTFBmJm"
      },
      "source": [
        "Define the search space of the hyper parameters of the selected model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVh3e1wnBmJm"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': np.linspace(0.1, 5.0, 50), 'max_iter': [20000]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn8LWOMvBmJm"
      },
      "outputs": [],
      "source": [
        "param_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iUbUVqsBmJm"
      },
      "source": [
        "Create the `GridSearchCV` object.\n",
        "We are going to do a 3-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGZ13M-0BmJm"
      },
      "outputs": [],
      "source": [
        "search = GridSearchCV(clf, param_grid, cv=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Xb4igNBmJm"
      },
      "source": [
        "Fit the search estimator on the validation data to search for the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RlQB9SKBmJm"
      },
      "outputs": [],
      "source": [
        "search.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW7E8IXHBmJm"
      },
      "source": [
        "Display the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSADQO8UBmJm"
      },
      "outputs": [],
      "source": [
        "search.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grid search shows that increasing the regularization parameter C leads to a clear improvement in performance up to a plateau around C \\approx 3.0. Beyond this point, the mean test score stabilizes around 0.848–0.849, indicating diminishing returns from weaker regularization. The standard deviation across folds remains low throughout, suggesting stable and robust performance. Training and scoring times increase moderately with higher C, but without critical overhead. Overall, the results indicate a good bias–variance trade-off around C \\in [2.8, 3.2], which represents the optimal region for this model."
      ],
      "metadata": {
        "id": "iPUnhv9xKN3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcQ16LkGBmJm"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(search.cv_results_)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table reports the full cross-validation results of the grid search over the hyperparameter C for Logistic Regression.\n",
        "The mean test score shows a gradual improvement as C increases, followed by a plateau, indicating limited sensitivity to further regularization changes.\n",
        "The relatively low standard deviation across folds suggests stable performance and good generalization.\n"
      ],
      "metadata": {
        "id": "st4X8ldr5rkb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbAERFF7BmJm"
      },
      "source": [
        "Display the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIWIVtixBmJn"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8hA2A14BmJn"
      },
      "source": [
        "Retrieve the best estimator.\n",
        "\n",
        "Once the search is complete, `GridSearchCV` fits the estimator on all available data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WKhzvrqBmJn"
      },
      "outputs": [],
      "source": [
        "best_clf = search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3epARD5BmJn"
      },
      "source": [
        "Finally evaluate the best estimator in the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBvucUy3BmJn"
      },
      "outputs": [],
      "source": [
        "y_pred = search.predict(X_val_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-Uty8TVBmJn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(best_clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier shows solid overall performance (accuracy ≈ 86%), but it is clearly biased toward the benign class. While specificity is high (≈90%), recall on the jailbreak class is lower (≈81%), meaning that nearly 1 out of 5 jailbreak instances is missed. This trade-off may be problematic in safety-critical settings, where false negatives are more costly than false positives. The confusion matrix confirms this imbalance, suggesting that threshold tuning or recall-oriented optimization would be necessary if jailbreak detection is the primary goal."
      ],
      "metadata": {
        "id": "_ZEFdBEG6N_D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bE2_5BLppJi"
      },
      "source": [
        "#### other embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyWy8xZLptha"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_grid = {\n",
        "    'C': np.linspace(0.1, 5.0, 50),\n",
        "    'max_iter': [20000],\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch SVC su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    # Modello base\n",
        "    base_clf = SVC()\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit della grid search\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params per {emb_name}: {best_params}\")\n",
        "\n",
        "    # Nome descrittivo della variante\n",
        "    variant_name = \"grid_C_0.1_5.0\"\n",
        "\n",
        "    # Salvataggio nel registry\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"SVC\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GridSearch results show that the optimal regularization parameter C varies significantly across embeddings, confirming that the representation space strongly influences the margin–complexity trade-off of the SVM.\n",
        "Lower-dimensional embeddings such as glove_wiki favor smaller C values, suggesting that stronger regularization helps prevent overfitting, while richer representations like fastText and GloVe Twitter benefit from higher C, allowing the model to exploit their higher expressive power.\n",
        "\n",
        "Overall, the fact that all best C values fall well within the explored range [0.1, 5.0] indicates that the search space was appropriately chosen. However, the relatively close optimal values across embeddings also suggest limited sensitivity to fine-grained tuning of C, implying that embedding choice has a larger impact on performance than minor hyperparameter adjustments."
      ],
      "metadata": {
        "id": "Oa8ZV4En7Mkw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLkeOUQ4BmJn"
      },
      "source": [
        "### Random Search\n",
        "\n",
        "Differently from a grid search, a randomised one does not try all parameter values; instead a fixed number of parameter settings is sampled from the specified distributions.\n",
        "we will also briefly experiment with this technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggs8R59g2aKY"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lw--yXgBmJn"
      },
      "source": [
        "We can look again for the best regularisation parameter for a SVM on our data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbKjoxguBmJn"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyGRLlUBBmJn"
      },
      "outputs": [],
      "source": [
        "clf = SVC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbpmgH5bBmJn"
      },
      "source": [
        "Define the search space of the hyper parameters of the selected model.\n",
        "Here we have the real difference w.r.t. a Grid search, the parametrs are not provided as a list of values, but as a distribution to sample from. In this case we try a uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUox2_0UBmJn"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': uniform(0, scale=5), 'max_iter': [20000]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33X7QOFMBmJn"
      },
      "outputs": [],
      "source": [
        "uniform(0, scale=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQZebgLNBmJn"
      },
      "source": [
        "Create the `RandomizedSearchCV` object.\n",
        "We are going to do a 3-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDpfu5dyBmJn"
      },
      "outputs": [],
      "source": [
        "search = RandomizedSearchCV(clf, param_grid, n_iter=50, cv=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdPcsTvUBmJo"
      },
      "source": [
        "Fit the search estimator on the train data to search for the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lBqv6ihBmJo"
      },
      "outputs": [],
      "source": [
        "search.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUGNiZGUBmJo"
      },
      "source": [
        "Display the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41Zeu1ttBmJo"
      },
      "outputs": [],
      "source": [
        "search.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GridSearch shows that performance is relatively stable across a wide range of C values, with only marginal differences in mean validation score. This suggests that the SVC model is not highly sensitive to the exact regularization strength in this range, and that further fine-grained tuning of C is unlikely to yield substantial performance gains."
      ],
      "metadata": {
        "id": "mV7prWHJ9qs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAilQm3pBmJo"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(search.cv_results_)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhRZDdUvBmJo"
      },
      "source": [
        "Display the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzdz1TvkBmJo"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomized search yielded a very similar result to grid search"
      ],
      "metadata": {
        "id": "cgA1pV_2MI2m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-nL2TYuBmJo"
      },
      "source": [
        "Retrieve the best estimator.\n",
        "\n",
        "Once the search is complete, `RandomizedSearchCV` fits the estimator on all available data (unless differently instructed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrFlTFycBmJo"
      },
      "outputs": [],
      "source": [
        "best_clf = search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuI_uZoeBmJo"
      },
      "source": [
        "Finally evaluate the best estimator in the validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML75evWWBmJo"
      },
      "outputs": [],
      "source": [
        "y_pred = best_clf.predict(X_val_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-OYfBfiBmJp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "evaluate_classifier(best_clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model shows solid overall performance (Accuracy ≈ 0.86, F1 ≈ 0.82) with good balance between precision and recall for the jailbreak class. However, around 20% of jailbreak samples are still missed (false negatives), which is critical in a safety-oriented setting. While specificity is high (~0.90), the remaining false negatives suggest that further optimization should prioritize recall for the positive class, even at the cost of a slight increase in false positives."
      ],
      "metadata": {
        "id": "lzFx_LUe-CF0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnF48gaY5ctU"
      },
      "source": [
        "##Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ1b7NtKmwHI"
      },
      "source": [
        "###Cross-validation based hyperparameter selection using Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bu8RsFFlIcn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arAcAmzg5hoN"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_RPD2QaiX1f"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': np.linspace(0.1, 5.0, 50), 'max_iter': [20000]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0sJe3hSmRA7"
      },
      "outputs": [],
      "source": [
        "param_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "596tZqnPmRQ8"
      },
      "outputs": [],
      "source": [
        "search = GridSearchCV(clf, param_grid, cv=3, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9SwHd9AmRX4"
      },
      "outputs": [],
      "source": [
        "search.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wwbZSpqmRcV"
      },
      "outputs": [],
      "source": [
        "search.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GridSearch shows that model performance is relatively stable across a wide range of C values, with only marginal differences in mean test score. This suggests that the SVM is not highly sensitive to the regularization strength in this interval, and that gains from further fine-grained tuning of C are limited. Additionally, higher C values do not consistently improve performance and sometimes increase training time, indicating diminishing returns and a potential risk of overfitting."
      ],
      "metadata": {
        "id": "DofMUt5I-xnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbCpNpnkmRhA"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(search.cv_results_)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKPCYi0mRlU"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSaJTNNLmq-9"
      },
      "outputs": [],
      "source": [
        "best_clf = search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ53TnAw8Y8y"
      },
      "outputs": [],
      "source": [
        "# # Assumendo che il tuo models_registry esista già\n",
        "\n",
        "# model_name = \"LogisticRegression\"\n",
        "# embedding_name = \"bow\"   # cambialo se vuoi un nome diverso\n",
        "# variant_name = f\"gridsearch_C_{search.best_params_['C']:.3f}\"\n",
        "# model = best_clf\n",
        "# params = search.best_params_\n",
        "\n",
        "# register_model(\n",
        "#     models_registry,\n",
        "#     model_name=model_name,\n",
        "#     embedding_name=embedding_name,\n",
        "#     variant_name=variant_name,\n",
        "#     model=model,\n",
        "#     params=params\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asqmxSzqmpn6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LWw9kc1m8MX"
      },
      "source": [
        "###Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EILFNHminAMw"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(max_iter=20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3nGjnCWn-DX"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': uniform(0, scale=5)}\n",
        "uniform(0, scale=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T4h4mv5oCrw"
      },
      "outputs": [],
      "source": [
        "search = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_grid,\n",
        "    n_iter=50,\n",
        "    cv=3,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNLeA_jtoG53"
      },
      "outputs": [],
      "source": [
        "search.fit(X_train_vector, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k26PaU-IoHGg"
      },
      "outputs": [],
      "source": [
        "search.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The randomized hyperparameter search shows that performance is relatively stable across a wide range of C values, with mean cross-validation accuracy consistently around 0.85. The best configurations differ only marginally from the others, suggesting that the model is not highly sensitive to C in this range. While RandomizedSearch explores the space more efficiently than GridSearch, the resulting gain over simpler settings is limited, indicating that further improvements are more likely to come from better features or embeddings rather than additional tuning of C alone."
      ],
      "metadata": {
        "id": "SIm0KUwR_uB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC7FVWIeoHVD"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(search.cv_results_)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV52o3jHoL22"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXNycDtvoNdH"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_clf = search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIb416DxoNiY"
      },
      "outputs": [],
      "source": [
        "y_pred = best_clf.predict(X_val_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKi7thpNoRGm"
      },
      "outputs": [],
      "source": [
        "evaluate_classifier(best_clf, X_val_vector, y_val,\n",
        "                    pos_label=\"jailbreak\",\n",
        "                    neg_label=\"benign\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieves good overall performance (Accuracy ≈ 0.86, F1 ≈ 0.83) with a balanced trade-off between precision and recall for the jailbreak class. However, around 19% of jailbreak instances are still misclassified as benign, which is a relevant limitation in a safety-critical setting. While specificity remains high (~0.90), the remaining false negatives indicate that the classifier, although robust, is not fully reliable for strict jailbreak detection without further tuning or threshold adjustment."
      ],
      "metadata": {
        "id": "nPHva5_kANdC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zt0_6q1Ra-x"
      },
      "source": [
        "### other embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htlq63xtRdg_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# 🔧 griglia iperparametri per Logistic Regression\n",
        "param_grid_lr = {\n",
        "    \"C\": np.linspace(0.1, 5.0, 50),\n",
        "    \"max_iter\": [20000],\n",
        "}\n",
        "\n",
        "# includi bow più tutti gli embeddings\n",
        "all_representations = [\"bow\"] + embedding_names\n",
        "# embedding_names = [\"w2v\", \"glove_twitter\", \"glove_wiki\", \"fasttext\", \"sbert\"]\n",
        "\n",
        "for rep_name in all_representations:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch LogisticRegression su rappresentazione: {rep_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # --- scegli la matrice giusta ---\n",
        "    if rep_name == \"bow\":\n",
        "        X_train_vec = X_train_vector\n",
        "        X_val_vec   = X_val_vector\n",
        "    else:\n",
        "        X_train_vec = other_embeddings[\"train\"][rep_name]\n",
        "        X_val_vec   = other_embeddings[\"val\"][rep_name]\n",
        "\n",
        "    # modello base\n",
        "    base_clf = LogisticRegression()\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_lr,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params LogisticRegression per {rep_name}: {best_params}\")\n",
        "\n",
        "    # Valutazione sul validation set\n",
        "    evaluate_classifier(\n",
        "        best_model,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n",
        "\n",
        "    # nome variante parlante\n",
        "    C_val = best_params[\"C\"]\n",
        "    variant_name = f\"grid_lr_C_{C_val:.3f}_maxiter_20000\"\n",
        "\n",
        "    # 🔐 registra nel registry IL MODELLO GIUSTO\n",
        "    register_model(\n",
        "        registry=models_registry,\n",
        "        model_name=\"LogisticRegression\",  # <-- CORRETTO\n",
        "        embedding_name=rep_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Wl53Ol5jPG"
      },
      "source": [
        "##Multinomial Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy_fL_D1ljyT"
      },
      "source": [
        "make negative values positive in other embeddings to use the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NzdegRWlhQA"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "param_grid_nb = {\n",
        "    \"alpha\": np.linspace(0.1, 5.0, 50),\n",
        "    \"fit_prior\": [True, False],\n",
        "}\n",
        "\n",
        "all_representations = [\"bow\"] + embedding_names   # bow, w2v, glove_twitter, ...\n",
        "\n",
        "for rep_name in all_representations:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch MultinomialNB su rappresentazione: {rep_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if rep_name == \"bow\":\n",
        "        # BoW: already positive\n",
        "        X_train_vec = X_train_vector\n",
        "        X_val_vec   = X_val_vector\n",
        "    else:\n",
        "        # other embeddigs could contain negative values\n",
        "        X_train_raw = other_embeddings[\"train\"][rep_name]\n",
        "        X_val_raw   = other_embeddings[\"val\"][rep_name]\n",
        "\n",
        "        # make the features non-negative\n",
        "        scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
        "        X_train_vec = scaler.fit_transform(X_train_raw)\n",
        "        X_val_vec   = scaler.transform(X_val_raw)\n",
        "\n",
        "    # base model NB\n",
        "    base_clf = MultinomialNB()\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_nb,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params NB per {rep_name}: {best_params}\")\n",
        "\n",
        "    # evaluation on validation set\n",
        "    evaluate_classifier(\n",
        "        best_model,\n",
        "        X_val_vec,\n",
        "        y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "    )\n",
        "\n",
        "    # variant name\n",
        "    variant_name = \"grid_nb_alpha_0.1_5.0\"\n",
        "\n",
        "    register_model(\n",
        "        registry=models_registry,\n",
        "        model_name=\"MultinomialNB\",\n",
        "        embedding_name=rep_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultinomialNB model performs very well on BoW, achieving balanced metrics with strong recall (0.86) and a solid F1-score (0.79). This confirms that NB is well-suited for sparse count features. In contrast, all dense embeddings (Word2Vec, GloVe, FastText) lead to substantially worse performance: although recall remains high, specificity drops sharply, meaning many benign samples are misclassified as jailbreaks. This makes these representations unreliable for a security task where false positives are costly. SBERT is the only exception, delivering performance close to BoW, but still slightly less stable. Overall, BoW remains the best representation for Naive Bayes in this jailbreak-detection setting.\n"
      ],
      "metadata": {
        "id": "1gZbDh20Q2Fi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCbRs4HkrEKL"
      },
      "source": [
        "##K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGqFGVgNSFG5"
      },
      "outputs": [],
      "source": [
        "# === GRID SEARCH KNN SU BOW (CountVectorizer) ===\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔹 GridSearch KNN su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "param_grid_knn = {\n",
        "    \"n_neighbors\": [1, 3, 5, 7, 9, 11],\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "}\n",
        "\n",
        "base_clf = KNeighborsClassifier()\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=base_clf,\n",
        "    param_grid=param_grid_knn,\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "search.fit(X_train_vec_bow, y_train)\n",
        "\n",
        "best_model  = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "\n",
        "print(f\"\\n✅ Best params KNN per bow: {best_params}\")\n",
        "\n",
        "# Valutazione sul validation set\n",
        "evaluate_classifier(\n",
        "    best_model,\n",
        "    X_val_vec_bow,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "# Variante per il registry\n",
        "variant_name = \"grid_knn_bow_k_1_11\"\n",
        "\n",
        "# Registrazione nel models_registry\n",
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"KNN\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KNN model on BoW performs poorly for the target task despite hyperparameter tuning. While specificity is very high (0.96), the recall for the positive jailbreak class is extremely low (0.22), meaning that most jailbreak instances are missed. The seemingly acceptable accuracy (0.66) is misleading and mainly driven by the majority benign class. Overall, this model is unsuitable for jailbreak detection, where false negatives are particularly costly."
      ],
      "metadata": {
        "id": "kpelaMr3CSRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGUKPcVjrMAk"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_grid_knn = {\n",
        "    \"n_neighbors\": [1, 3, 5, 7, 9, 11],\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch KNN su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    base_clf = KNeighborsClassifier()\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_knn,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params KNN per {emb_name}: {best_params}\")\n",
        "\n",
        "    variant_name = \"grid_knn_k_1_11\"\n",
        "\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"KNN\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NI2GMqazCnn"
      },
      "outputs": [],
      "source": [
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"KNN\",\n",
        "    embedding_name=emb_name,\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe7xTFCu90D0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi6kQV04rOPv"
      },
      "source": [
        "##Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSn4OT92SeOv"
      },
      "outputs": [],
      "source": [
        "# === GRID SEARCH DECISION TREE SU BOW (CountVectorizer) ===\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔹 GridSearch DecisionTree su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"max_depth\": [None, 5, 10, 20],\n",
        "    \"min_samples_split\": [2, 10, 50],\n",
        "    \"min_samples_leaf\": [1, 5, 10],\n",
        "}\n",
        "\n",
        "base_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=base_clf,\n",
        "    param_grid=param_grid_dt,\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "search.fit(X_train_vec_bow, y_train)\n",
        "\n",
        "best_model  = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "\n",
        "print(f\"\\n✅ Best params DecisionTree per bow: {best_params}\")\n",
        "\n",
        "# Valutazione sul validation set\n",
        "evaluate_classifier(\n",
        "    best_model,\n",
        "    X_val_vec_bow,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "# Nome variante per il registry\n",
        "variant_name = \"grid_dt_bow_depth_split_leaf\"\n",
        "\n",
        "# Registrazione nel models_registry\n",
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"DecisionTree\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Decision Tree, even with the best hyperparameters (max_depth=10, min_samples_leaf=5, min_samples_split=50), shows clearly weaker performance compared to linear models. Accuracy is moderate (≈0.77), but the main issue is the low recall on the jailbreak class (≈0.58): the model misses a large portion of malicious samples, as confirmed by the 125 false negatives. The confusion matrix highlights a conservative behavior, with good specificity (≈0.89) but poor sensitivity, meaning the tree prefers predicting benign and fails to capture many jailbreaks. This is a typical limitation of Decision Trees on high-dimensional sparse representations like BoW, where they struggle to generalize and tend to underfit when regularized to avoid overfitting. Overall, this model is not suitable if detecting jailbreaks is a priority."
      ],
      "metadata": {
        "id": "im6Vy24IDX58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lAIkhH4rQmE"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"max_depth\": [None, 5, 10, 20],\n",
        "    \"min_samples_split\": [2, 10, 50],\n",
        "    \"min_samples_leaf\": [1, 5, 10],\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch DecisionTree su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    base_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_dt,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params DecisionTree per {emb_name}: {best_params}\")\n",
        "\n",
        "    variant_name = \"grid_dt_depth_split_leaf\"\n",
        "\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"DecisionTree\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQokeHQMrUSP"
      },
      "source": [
        "##Kernel SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This RBF Kernel SVM on BoW achieves solid but not outstanding performance. Accuracy is around 0.85, with a good balance between precision (0.84) and specificity (0.90), meaning the model is quite conservative and reliable when predicting the benign class. However, recall on the jailbreak class is lower (≈0.78), so a non-negligible fraction of jailbreak samples is still missed. The confusion matrix confirms this behavior: false negatives (67 jailbreaks predicted as benign) are noticeably higher than false positives (45 benign predicted as jailbreak). Overall, the model improves over simpler methods like KNN and Decision Trees, but it does not reach the performance of linear models or SVMs with stronger representations, indicating that the non-linear kernel adds limited value on sparse BoW features."
      ],
      "metadata": {
        "id": "OAwk9Pa6D_eE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuZYdZESswm"
      },
      "outputs": [],
      "source": [
        "# === GRID SEARCH KERNEL SVM (RBF) SU BOW (CountVectorizer) ===\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔹 GridSearch Kernel SVM (RBF) su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "param_grid_ksvm = {\n",
        "    \"C\": np.logspace(-2, 2, 7),        # 0.01 → 100\n",
        "    \"gamma\": np.logspace(-3, 1, 5),    # 0.001 → 10\n",
        "}\n",
        "\n",
        "base_clf = SVC(kernel=\"rbf\")\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=base_clf,\n",
        "    param_grid=param_grid_ksvm,\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "search.fit(X_train_vec_bow, y_train)\n",
        "\n",
        "best_model  = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "\n",
        "print(f\"\\n✅ Best params Kernel SVM per bow: {best_params}\")\n",
        "\n",
        "# Valutazione sul validation set\n",
        "evaluate_classifier(\n",
        "    best_model,\n",
        "    X_val_vec_bow,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "# Variante per il registry\n",
        "variant_name = \"grid_rbf_C_gamma_bow\"\n",
        "\n",
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"KernelSVM\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsU0i1WNrXlO"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_grid_ksvm = {\n",
        "    \"C\": np.logspace(-2, 2, 7),        # da 0.01 a 100\n",
        "    \"gamma\": np.logspace(-3, 1, 5),    # da 0.001 a 10\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch Kernel SVM (RBF) su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    base_clf = SVC(kernel=\"rbf\")\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_ksvm,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params Kernel SVM per {emb_name}: {best_params}\")\n",
        "\n",
        "    variant_name = \"grid_rbf_C_gamma\"\n",
        "\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"KernelSVM\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtYji8bSred3"
      },
      "source": [
        "##(Deep) Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83mNA7ekTBx2"
      },
      "outputs": [],
      "source": [
        "# === GRID SEARCH MLPClassifier SU BOW (CountVectorizer) ===\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔹 GridSearch MLPClassifier su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "param_grid_mlp = {\n",
        "    \"hidden_layer_sizes\": [(50,), (100,)],\n",
        "    \"alpha\": np.linspace(1e-4, 1.0, 10),\n",
        "}\n",
        "\n",
        "base_clf = MLPClassifier(\n",
        "    max_iter=200,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=base_clf,\n",
        "    param_grid=param_grid_mlp,\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "search.fit(X_train_vec_bow, y_train)\n",
        "\n",
        "best_model  = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "\n",
        "print(f\"\\n✅ Best params MLP per bow: {best_params}\")\n",
        "\n",
        "# Valutazione sul validation set\n",
        "evaluate_classifier(\n",
        "    best_model,\n",
        "    X_val_vec_bow,\n",
        "    y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        ")\n",
        "\n",
        "variant_name = \"grid_mlp_alpha_hidden_bow\"\n",
        "\n",
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"MLP\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This MLP trained on BoW achieves solid but not outstanding performance. Accuracy is reasonably high (0.85) and the F1-score (~0.81) indicates a fairly balanced trade-off between precision and recall. The model detects jailbreak prompts with good recall (0.81), meaning most malicious cases are caught, but it still misses a non-negligible fraction. At the same time, specificity (0.88) shows it is fairly reliable on benign inputs, although false positives are not trivial. The confusion matrix confirms this behavior: the model is slightly biased toward predicting jailbreak, which helps recall but increases benign misclassification. Overall, the MLP performs comparably to linear models on BoW, but it does not clearly outperform simpler approaches, suggesting that the added non-linearity brings limited benefit with this representation."
      ],
      "metadata": {
        "id": "H19UtQaTEfMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QVvMIvLrfxA"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_grid_mlp = {\n",
        "    \"hidden_layer_sizes\": [(50,), (100,)],\n",
        "    \"alpha\": np.linspace(1e-4, 1.0, 10),   # valori tipo quelli che usavi prima\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch MLPClassifier su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "    base_clf = MLPClassifier(\n",
        "        max_iter=200,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_mlp,\n",
        "        cv=3,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    search.fit(X_train_vec, y_train)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params MLP per {emb_name}: {best_params}\")\n",
        "\n",
        "    variant_name = \"grid_mlp_alpha_hidden\"\n",
        "\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"MLP\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdJmvnlPci9B"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etUYdLwcclQf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🌿 RandomForest su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "rf_bow = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_bow.fit(X_train_vec_bow, y_train)\n",
        "\n",
        "# Valutazione sul validation set\n",
        "evaluate_classifier(\n",
        "    clf=rf_bow,\n",
        "    X_val=X_val_vec_bow,\n",
        "    y_val=y_val,\n",
        "    pos_label=\"jailbreak\",\n",
        "    neg_label=\"benign\",\n",
        "    show_plots=True,\n",
        ")\n",
        "\n",
        "# Registrazione nel registry\n",
        "register_model(\n",
        "    registry=models_registry,\n",
        "    model_name=\"RandomForest\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=\"rf_fixed_200_10\",\n",
        "    model=rf_bow,\n",
        "    params={\n",
        "        \"n_estimators\": 200,\n",
        "        \"max_depth\": 10,\n",
        "        \"random_state\": 42,\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This RandomForest trained on BoW shows a very conservative behavior. Accuracy is moderate (≈0.75), but it is driven almost entirely by the benign class. The model achieves very high specificity (≈0.95), meaning it almost never flags benign samples as jailbreak, and precision is high because predicted jailbreaks are usually correct. However, recall on the jailbreak class is poor (≈0.44): more than half of the true jailbreak samples are missed and classified as benign, as clearly visible from the confusion matrix (167 false negatives vs 133 true positives). As a consequence, the F1-score is low. In practice, this model is unsuitable if the goal is to detect jailbreaks reliably, because it prioritizes avoiding false alarms over catching malicious cases."
      ],
      "metadata": {
        "id": "zfCrHu8IFI9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ8fAdvvco4T"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"Embedding disponibili:\", list(other_embeddings[\"train\"].keys()))\n",
        "\n",
        "for emb_name in other_embeddings[\"train\"].keys():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"🌿 RandomForest su embedding: {emb_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_train_emb = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_emb   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    print(\"Shapes:\", X_train_emb.shape, X_val_emb.shape)\n",
        "\n",
        "    rf_emb = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=10,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf_emb.fit(X_train_emb, y_train)\n",
        "\n",
        "    # Predizioni su validation (per loggare l'F1)\n",
        "    y_val_pred = rf_emb.predict(X_val_emb)\n",
        "    f1 = f1_score(y_val, y_val_pred, pos_label=\"jailbreak\")\n",
        "    print(f\"F1-score (validation) per {emb_name}: {f1:.4f}\")\n",
        "\n",
        "    # Valutazione completa con la tua funzione\n",
        "    evaluate_classifier(\n",
        "        clf=rf_emb,\n",
        "        X_val=X_val_emb,\n",
        "        y_val=y_val,\n",
        "        pos_label=\"jailbreak\",\n",
        "        neg_label=\"benign\",\n",
        "        show_plots=False,   # metti True se vuoi tutti i grafici per ogni embedding\n",
        "    )\n",
        "\n",
        "    # Registrazione nel registry PER QUESTO EMBEDDING\n",
        "    register_model(\n",
        "        registry=models_registry,\n",
        "        model_name=\"RandomForest\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=\"rf_fixed_200_10\",\n",
        "        model=rf_emb,\n",
        "        params={\n",
        "            \"n_estimators\": 200,\n",
        "            \"max_depth\": 10,\n",
        "            \"random_state\": 42\n",
        "        },\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc8LGknIc4rw"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr4wEUVzc60B"
      },
      "outputs": [],
      "source": [
        "# === GRID SEARCH XGBOOST SU BOW (CountVectorizer) ===\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔹 GridSearch XGBoost su rappresentazione: bow (CountVectorizer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_vec_bow = X_train_vector\n",
        "X_val_vec_bow   = X_val_vector\n",
        "\n",
        "# 1) Etichette numeriche per XGBoost: 0 = benign, 1 = jailbreak\n",
        "y_train_bin = y_train.map(to_binary_label)\n",
        "y_val_bin   = y_val.map(to_binary_label)\n",
        "\n",
        "# 2) Scorer che massimizza F1 sulla classe positiva (1 = jailbreak)\n",
        "f1_jb = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "param_grid_xgb = {\n",
        "    \"n_estimators\": [10, 50, 100, 200],\n",
        "    \"max_depth\": [5, 10, 15],\n",
        "}\n",
        "\n",
        "base_clf = XGBClassifier(\n",
        "    n_jobs=-1,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    objective=\"binary:logistic\",   # esplicito\n",
        ")\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=base_clf,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=3,\n",
        "    scoring=f1_jb,\n",
        "    verbose=True,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# ⬅️ usa y_train_bin (numerico)\n",
        "search.fit(X_train_vec_bow, y_train_bin)\n",
        "\n",
        "best_model  = search.best_estimator_\n",
        "best_params = search.best_params_\n",
        "\n",
        "print(f\"\\n✅ Best params XGBoost per bow: {best_params}\")\n",
        "\n",
        "# 3) Valutazione sul validation set (versione rapida con sklearn)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_val_pred_bin = best_model.predict(X_val_vec_bow)\n",
        "\n",
        "print(\"\\nConfusion matrix (binaria, 0=benign, 1=jailbreak):\")\n",
        "print(confusion_matrix(y_val_bin, y_val_pred_bin))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(\n",
        "    y_val_bin,\n",
        "    y_val_pred_bin,\n",
        "    target_names=[\"benign\", \"jailbreak\"]\n",
        "))\n",
        "\n",
        "# 4) Registrazione nel models_registry\n",
        "variant_name = \"grid_xgb_bow_n_est_depth\"\n",
        "\n",
        "register_model(\n",
        "    models_registry,\n",
        "    model_name=\"XGBoost\",\n",
        "    embedding_name=\"bow\",\n",
        "    variant_name=variant_name,\n",
        "    model=best_model,\n",
        "    params=best_params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XGBoost model trained on the BoW representation achieves a solid and well-balanced performance. With the best configuration found by grid search (max_depth = 15 and n_estimators = 100), the classifier reaches an overall accuracy of 0.84. Precision and recall are reasonably balanced for both classes: benign samples are correctly identified most of the time, while the model also maintains a good recall on the jailbreak class (0.81), which is critical for this task. The confusion matrix shows that errors are fairly symmetric, with a moderate number of false positives and false negatives, indicating no extreme bias toward either class. Overall, XGBoost performs competitively compared to other models on BoW, offering a good trade-off between precision and recall without being overly conservative or overly permissive."
      ],
      "metadata": {
        "id": "jNXLUSxqF-nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qupkFylSgM4n"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# ⬇️ etichette binarie per XGBoost: 0 = benign, 1 = jailbreak\n",
        "y_train_bin = y_train.map(to_binary_label)\n",
        "y_val_bin   = y_val.map(to_binary_label)\n",
        "\n",
        "# scorer F1 sulla classe positiva (1 = jailbreak)\n",
        "f1_jb = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "param_grid_xgb = {\n",
        "    \"n_estimators\": [10, 50, 100, 200],\n",
        "    \"max_depth\": [5, 10, 15],\n",
        "}\n",
        "\n",
        "for emb_name in embedding_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🔹 GridSearch XGBoost su embedding: {emb_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train_vec = other_embeddings[\"train\"][emb_name]\n",
        "    X_val_vec   = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "    base_clf = XGBClassifier(\n",
        "        n_jobs=-1,\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        objective=\"binary:logistic\",\n",
        "    )\n",
        "\n",
        "    search = GridSearchCV(\n",
        "        estimator=base_clf,\n",
        "        param_grid=param_grid_xgb,\n",
        "        cv=3,\n",
        "        scoring=f1_jb,\n",
        "        verbose=True,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    # ⬅️ usa le etichette binarie\n",
        "    search.fit(X_train_vec, y_train_bin)\n",
        "\n",
        "    best_model  = search.best_estimator_\n",
        "    best_params = search.best_params_\n",
        "\n",
        "    print(f\"\\n✅ Best params XGBoost per {emb_name}: {best_params}\")\n",
        "\n",
        "    # 🔎 valutazione sul validation set con la TUA funzione\n",
        "    evaluate_classifier(\n",
        "        best_model,\n",
        "        X_val_vec,\n",
        "        y_val_bin,   # ⬅️ etichette 0/1\n",
        "        pos_label=1,\n",
        "        neg_label=0,\n",
        "    )\n",
        "\n",
        "    variant_name = \"grid_xgb_n_est_depth\"\n",
        "\n",
        "    register_model(\n",
        "        models_registry,\n",
        "        model_name=\"XGBoost\",\n",
        "        embedding_name=emb_name,\n",
        "        variant_name=variant_name,\n",
        "        model=best_model,\n",
        "        params=best_params,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U07OPuJTmCZD"
      },
      "source": [
        "#saving all of the models on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOKp9so5ddkc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# folder on drive\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/AI_for_security\"\n",
        "FILENAME        = \"models_registry.pkl\"\n",
        "\n",
        "SAVE_PATH       = os.path.join(DRIVE_BASE_PATH, FILENAME)\n",
        "\n",
        "print(f\"Cartella di salvataggio:\\n{DRIVE_BASE_PATH}\")\n",
        "\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"📁 Sto salvando (e sovrascrivendo se necessario) su: {SAVE_PATH}\")\n",
        "\n",
        "# saving\n",
        "try:\n",
        "    with open(SAVE_PATH, \"wb\") as f:\n",
        "        pickle.dump(models_registry, f)\n",
        "    print(\"\\n✅ Salvataggio completato! Il file è stato aggiornato con i nuovi modelli.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Errore durante il salvataggio: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJLRXK4KmIeD"
      },
      "source": [
        "#Loading all of the models from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENox8kSXmN8C"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "LOAD_PATH = \"/content/drive/MyDrive/AI_for_security/models_registry.pkl\"\n",
        "\n",
        "with open(LOAD_PATH, \"rb\") as f:\n",
        "    models_registry = pickle.load(f)\n",
        "\n",
        "print(\"✅ Modelli caricati!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6rB3kSOnUPD"
      },
      "outputs": [],
      "source": [
        "for model_name, emb_dict in models_registry.items():\n",
        "    print(f\"\\n📌 MODEL: {model_name}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "        print(f\"  🔹 Embedding: {emb_name}\")\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            print(f\"     ➝ Variant: {variant_name}\")\n",
        "            print(f\"        Parameters: {info['params']}\")\n",
        "            print(f\"        Type of model: {type(info['model']).__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW1xIYuG1TXm"
      },
      "source": [
        "# final evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK38vpGO1Zmu"
      },
      "source": [
        "## all models, all embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev1NCUr-2Duy"
      },
      "source": [
        "###precision-recall curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RHpY9hqua1L"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# function to generate colors\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n  # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# 1) labels\n",
        "y_test_bin = y_test.map(to_binary_label).values\n",
        "\n",
        "num_curves = sum(\n",
        "    len(variants)\n",
        "    for _, emb_dict in models_registry.items()\n",
        "    for emb_name, variants in emb_dict.items()\n",
        "    if emb_name in other_embeddings[\"test\"]\n",
        ")\n",
        "\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "curve_idx = 0\n",
        "\n",
        "# 3) Loop\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"test\"]:\n",
        "            continue\n",
        "\n",
        "        X_test_emb = other_embeddings[\"test\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_test_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_test_emb)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            precision, recall, _ = precision_recall_curve(y_test_bin, scores)\n",
        "            ap = average_precision_score(y_test_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AP={ap:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(recall, precision, label=label, color=color, linewidth=2)\n",
        "\n",
        "# 4) Styling\n",
        "plt.xlabel(\"Recall\", fontsize=13)\n",
        "plt.ylabel(\"Precision\", fontsize=13)\n",
        "plt.title(\"Precision–Recall curves (Test Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the Precision–Recall curves and the Average Precision (AP) values, the three best models are all based on SBERT embeddings, which clearly dominate the ranking.\n",
        "\n",
        "The SVC with SBERT is the top performer, achieving the highest AP (≈ 0.93). Its curve stays consistently high across almost the entire recall range, meaning it maintains strong precision even when recall increases. This indicates very reliable separation between benign and jailbreak samples.\n",
        "\n",
        "Immediately behind it is the Kernel SVM (RBF) with SBERT, with an AP essentially identical (≈ 0.93). Its curve almost overlaps with the linear SVC, showing that the non-linear kernel does not add much beyond what SBERT already provides, but still delivers excellent and very stable performance.\n",
        "\n",
        "The third best model is the MLP with SBERT (AP ≈ 0.93). While slightly below the two SVMs, it still shows a strong precision–recall trade-off and confirms that neural embeddings combined with a simple neural classifier work extremely well for this task.\n",
        "\n",
        "Overall, the takeaway is clear: the representation (SBERT) matters more than the specific classifier, and SVM-based models extract the most consistent benefit from it."
      ],
      "metadata": {
        "id": "nFj-yQUAJOMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGaM0_2S2LtN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del VALIDATION SET\n",
        "# -------------------------------------------\n",
        "y_val_bin = y_val.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante curve verranno REALMENTE disegnate\n",
        "#    (solo modelli che hanno predict_proba o decision_function,\n",
        "#     e solo embedding per cui abbiamo il \"val\")\n",
        "# -------------------------------------------\n",
        "def count_real_curves_val():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"val\"]:\n",
        "                continue   # niente embedding val -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                    count += 1   # questa curva la disegniamo davvero\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_curves_val()\n",
        "print(\"Numero curve PR sul validation set:\", num_curves)\n",
        "\n",
        "# Palette colori\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul VALIDATION\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"val\"]:\n",
        "            continue\n",
        "\n",
        "        X_val_emb = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # score continuo\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_val_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_val_emb)\n",
        "            else:\n",
        "                continue  # niente score -> niente curva\n",
        "\n",
        "            precision, recall, _ = precision_recall_curve(y_val_bin, scores)\n",
        "            ap = average_precision_score(y_val_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AP={ap:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(recall, precision, label=label, color=color, linewidth=2)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Styling\n",
        "# -------------------------------------------\n",
        "plt.xlabel(\"Recall\", fontsize=13)\n",
        "plt.ylabel(\"Precision\", fontsize=13)\n",
        "plt.title(\"Precision–Recall curves (Validation Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three best models on the validation set are all based on SBERT embeddings, which clearly dominate the Precision–Recall analysis.\n",
        "\n",
        "The top performer is the linear SVC with SBERT (AP ≈ 0.939). Its PR curve stays consistently high across a wide recall range, indicating an excellent balance between precision and recall. This suggests that SBERT representations make the classes almost linearly separable, allowing a relatively simple classifier to perform extremely well.\n",
        "\n",
        "Very close behind is the Kernel SVM (RBF) with SBERT (AP ≈ 0.935). The performance is nearly indistinguishable from the linear SVC, meaning that adding non-linearity brings little benefit here. This reinforces the idea that the embedding quality, rather than model complexity, is the key factor.\n",
        "\n",
        "The MLP with SBERT ranks third (AP ≈ 0.934) and shows similarly strong behavior. However, its slight drop compared to SVC suggests that the neural network does not extract substantially more information than what is already encoded in the embeddings, while being more complex and potentially less stable.\n",
        "\n",
        "Overall, the takeaway is clear: SBERT is the decisive component, and simpler classifiers like linear SVC already exploit its structure almost optimally."
      ],
      "metadata": {
        "id": "NpLX_K2jKOoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idnrsmoo2u0e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del TRAINING SET\n",
        "# -------------------------------------------\n",
        "y_train_bin = y_train.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante curve verranno REALMENTE disegnate sul TRAIN\n",
        "# -------------------------------------------\n",
        "def count_real_curves_train():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"train\"]:\n",
        "                continue   # niente embedding train -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                    count += 1   # questa curva la disegniamo davvero\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_curves_train()\n",
        "print(\"Numero curve PR sul training set:\", num_curves)\n",
        "\n",
        "# Palette colori\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul TRAINING\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"train\"]:\n",
        "            continue\n",
        "\n",
        "        X_train_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # score continuo\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_train_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_train_emb)\n",
        "            else:\n",
        "                continue  # niente score -> niente curva\n",
        "\n",
        "            precision, recall, _ = precision_recall_curve(y_train_bin, scores)\n",
        "            ap = average_precision_score(y_train_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AP={ap:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(recall, precision, label=label, color=color, linewidth=2)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Styling\n",
        "# -------------------------------------------\n",
        "plt.xlabel(\"Recall\", fontsize=13)\n",
        "plt.ylabel(\"Precision\", fontsize=13)\n",
        "plt.title(\"Precision–Recall curves (Training Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the training-set Precision–Recall curves, the three best models all achieve an Average Precision (AP) of 1.00, which means they separate the two classes perfectly on the training data.\n",
        "\n",
        "The top performers are SVC with SBERT embeddings, Kernel SVM (RBF) with SBERT embeddings, and MLP with SBERT embeddings. In all three cases, the PR curves stay at precision ≈ 1 across almost the entire recall range, indicating near-zero false positives and false negatives on the training set.\n",
        "\n",
        "This result is extremely strong but also a red flag for overfitting: perfect AP on training suggests that these models may be memorizing patterns rather than learning generalizable decision boundaries. Their true value must therefore be judged by comparing these results with the validation and test PR curves, where any drop in AP will reveal how well this apparent performance transfers to unseen data."
      ],
      "metadata": {
        "id": "K-FYDF3FK47Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIo-fNpn3R8M"
      },
      "source": [
        "### ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgiFGDTt31__"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del TEST SET\n",
        "# -------------------------------------------\n",
        "y_test_bin = y_test.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante ROC verranno REALMENTE disegnate sul TEST\n",
        "# -------------------------------------------\n",
        "def count_real_roc_curves_test():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"test\"]:\n",
        "                continue   # niente embedding test -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                    count += 1   # questa curva la disegniamo davvero\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_roc_curves_test()\n",
        "print(\"Numero curve ROC sul test set:\", num_curves)\n",
        "\n",
        "# Palette colori distinte\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul TEST\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"test\"]:\n",
        "            continue\n",
        "\n",
        "        X_test_emb = other_embeddings[\"test\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # score continuo\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_test_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_test_emb)\n",
        "            else:\n",
        "                continue  # niente score → niente curva\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_test_bin, scores)\n",
        "            auc_val = roc_auc_score(y_test_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AUC={auc_val:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(fpr, tpr, label=label, color=color, linewidth=2)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Diagonale random + styling\n",
        "# -------------------------------------------\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", linewidth=1, label=\"Random baseline\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=13)\n",
        "plt.ylabel(\"True Positive Rate (Recall)\", fontsize=13)\n",
        "plt.title(\"ROC curves (Test Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the ROC curves on the test set, the three best models are clearly those based on SBERT embeddings, which consistently dominate the upper-left region of the plot and achieve the highest AUC values.\n",
        "\n",
        "The best overall model is linear SVC with SBERT, reaching an AUC of about 0.95. This indicates an excellent ranking ability: the model separates jailbreak and benign samples very well across almost all thresholds, with high true positive rates even at low false positive rates.\n",
        "\n",
        "Very close behind is Kernel SVM (RBF) with SBERT, with an AUC just below 0.95. Its ROC curve almost overlaps with the linear SVC one, suggesting that the non-linear kernel does not add much on top of SBERT representations, which are already highly expressive.\n",
        "\n",
        "The third best model is MLP with SBERT, with an AUC around 0.94. While slightly inferior to the SVM-based approaches, it still shows strong discriminative power and a smooth ROC curve, confirming that neural models also benefit significantly from high-quality sentence embeddings.\n",
        "\n",
        "Overall, these results show that representation quality (SBERT) matters more than model complexity, and that SVM-based classifiers on SBERT provide the most reliable performance on the test set."
      ],
      "metadata": {
        "id": "oJ9n-xq0Lu5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45bUYf9b3owS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del VALIDATION SET\n",
        "# -------------------------------------------\n",
        "y_val_bin = y_val.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante ROC verranno REALMENTE disegnate sul VAL\n",
        "# -------------------------------------------\n",
        "def count_real_roc_curves_val():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"val\"]:\n",
        "                continue   # niente embedding val -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                    count += 1   # questa curva la disegniamo davvero\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_roc_curves_val()\n",
        "print(\"Numero curve ROC sul validation set:\", num_curves)\n",
        "\n",
        "# Palette colori\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul VALIDATION\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"val\"]:\n",
        "            continue\n",
        "\n",
        "        X_val_emb = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # score continuo\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_val_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_val_emb)\n",
        "            else:\n",
        "                continue  # niente score -> niente curva\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_val_bin, scores)\n",
        "            auc_val = roc_auc_score(y_val_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AUC={auc_val:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(fpr, tpr, label=label, color=color, linewidth=2)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Diagonale random + styling\n",
        "# -------------------------------------------\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", linewidth=1, label=\"Random baseline\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=13)\n",
        "plt.ylabel(\"True Positive Rate (Recall)\", fontsize=13)\n",
        "plt.title(\"ROC curves (Validation Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the validation ROC curves, the three best models are all based on SBERT embeddings, which clearly dominate the ranking.\n",
        "\n",
        "The best overall model is SVC with SBERT, reaching an AUC of about 0.954. Its ROC curve stays closest to the top-left corner across almost the entire range, meaning it achieves a very strong trade-off between true positive rate and false positive rate. This indicates excellent separability between benign and jailbreak samples.\n",
        "\n",
        "Very close behind is Kernel SVM (RBF) with SBERT, with an AUC of roughly 0.952. Its curve is nearly indistinguishable from the linear SVC + SBERT, suggesting that once high-quality contextual embeddings are used, adding non-linearity brings only marginal gains.\n",
        "\n",
        "The third best model is MLP with SBERT, with an AUC around 0.949. While slightly below the two SVM-based approaches, it still shows consistently high recall at low false positive rates, confirming that SBERT embeddings provide a very strong signal even for neural classifiers.\n",
        "\n",
        "Overall, the takeaway is clear: representation matters more than the classifier. SBERT embeddings consistently outperform word-based embeddings, and SVM-based models exploit them slightly better than MLPs on the validation set."
      ],
      "metadata": {
        "id": "NB-oosvkMbwy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmdfKSHb3Upd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del TRAINING SET\n",
        "# -------------------------------------------\n",
        "y_train_bin = y_train.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante ROC verranno REALMENTE disegnate sul TRAIN\n",
        "# -------------------------------------------\n",
        "def count_real_roc_curves_train():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"train\"]:\n",
        "                continue   # niente embedding train -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                    count += 1   # questa curva la disegniamo davvero\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_roc_curves_train()\n",
        "print(\"Numero curve ROC sul training set:\", num_curves)\n",
        "\n",
        "# Palette colori\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul TRAINING\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"train\"]:\n",
        "            continue\n",
        "\n",
        "        X_train_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # score continuo\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                scores = clf.predict_proba(X_train_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_train_emb)\n",
        "            else:\n",
        "                continue  # niente score -> niente curva\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_train_bin, scores)\n",
        "            auc_val = roc_auc_score(y_train_bin, scores)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (AUC={auc_val:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(fpr, tpr, label=label, color=color, linewidth=2)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Diagonale random + styling\n",
        "# -------------------------------------------\n",
        "# baseline classificatore random\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", linewidth=1, label=\"Random baseline\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=13)\n",
        "plt.ylabel(\"True Positive Rate (Recall)\", fontsize=13)\n",
        "plt.title(\"ROC curves (Training Set)\", fontsize=16)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the training set ROC curves, the three best-performing models all achieve near-perfect discrimination, with AUC values equal to 1.000, which clearly indicates overfitting rather than genuine generalization ability.\n",
        "\n",
        "The top performer is XGBoost with SBERT embeddings, which reaches an AUC of 1.000 and shows a curve almost perfectly hugging the top-left corner. This means the model separates benign and jailbreak samples almost perfectly on the training data, a strong signal that it has memorized training patterns.\n",
        "\n",
        "Very close to it is Kernel SVM with SBERT embeddings, also achieving an AUC of 1.000. Its ROC curve is indistinguishable from the ideal classifier on the training set, again suggesting extremely high capacity relative to the dataset size.\n",
        "\n",
        "The third best is MLP with SBERT embeddings, which likewise reaches an AUC of 1.000. This confirms a consistent pattern: SBERT embeddings combined with high-capacity models dominate on training data, but such perfect performance is unrealistic and should be interpreted as a warning sign.\n",
        "\n",
        "Overall, these results show that while SBERT-based models are extremely expressive, training-set ROC is not informative for model selection here. Validation and test ROC curves are essential to distinguish true performance from overfitting."
      ],
      "metadata": {
        "id": "YrZb0G0jM-vy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onBAcAQX4hsM"
      },
      "source": [
        "### calibration curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7q6g6no4nAs"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Funzione per generare N colori molto diversi (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equispaziati tra 0 e 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Etichette binarie del TEST SET\n",
        "# -------------------------------------------\n",
        "y_test_bin = y_test.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Conta quante calibration curves verranno disegnate sul TEST\n",
        "# -------------------------------------------\n",
        "def count_real_calib_curves_test():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"test\"]:\n",
        "                continue\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    count += 1\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_calib_curves_test()\n",
        "print(\"Numero calibration curves sul test set:\", num_curves)\n",
        "\n",
        "# Palette colori distinte\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(16, 12))   # 🔥 immagine molto più grande\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop modelli/embedding sul TEST\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"test\"]:\n",
        "            continue\n",
        "\n",
        "        X_test_emb = other_embeddings[\"test\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # SOLO modelli con predict_proba\n",
        "            if not hasattr(clf, \"predict_proba\"):\n",
        "                continue\n",
        "\n",
        "            probs = clf.predict_proba(X_test_emb)[:, 1]\n",
        "\n",
        "            # reliability diagram\n",
        "            prob_true, prob_pred = calibration_curve(\n",
        "                y_test_bin, probs, n_bins=10, strategy=\"quantile\"\n",
        "            )\n",
        "\n",
        "            # Brier score\n",
        "            brier = brier_score_loss(y_test_bin, probs)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (Brier={brier:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(prob_pred, prob_true, marker=\"o\", linewidth=2,\n",
        "                     label=label, color=color)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Perfect calibration line + styling\n",
        "# -------------------------------------------\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
        "\n",
        "plt.xlabel(\"Predicted probability\", fontsize=16)   # 🔥 inglese\n",
        "plt.ylabel(\"Fraction of positives\", fontsize=16)   # 🔥 inglese\n",
        "plt.title(\"Calibration curves (Test Set)\", fontsize=20)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the calibration curves on the test set and the corresponding Brier scores, the three best-calibrated models are all based on strong semantic embeddings and probabilistic outputs.\n",
        "\n",
        "The best result is obtained by MLP with SBERT embeddings, which achieves the lowest Brier score (≈ 0.099). Its calibration curve stays very close to the diagonal, indicating that predicted probabilities closely match the true empirical frequencies. This means the model’s confidence estimates are highly reliable, not just its classifications.\n",
        "\n",
        "The second best model is XGBoost with SBERT embeddings (Brier ≈ 0.121). Its curve is also well aligned with the perfect calibration line across most probability bins, showing only mild deviations at higher confidence levels. This suggests a good balance between discriminative power and probability calibration.\n",
        "\n",
        "The third best performer is KNN with SBERT embeddings (Brier ≈ 0.126). While slightly less calibrated than MLP and XGBoost, it still provides reasonably accurate probability estimates compared to other models, especially in the mid-probability range.\n",
        "\n",
        "Overall, these results confirm that SBERT embeddings consistently lead to better-calibrated models, and that neural or boosted models on top of them (MLP, XGBoost) are particularly effective when reliable probability estimates are required."
      ],
      "metadata": {
        "id": "FhAoJDPZNY5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmF9hWJX4-KX"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Function to generate N very distinct colors (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equally spaced between 0 and 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Binary labels for the VALIDATION SET\n",
        "# -------------------------------------------\n",
        "y_val_bin = y_val.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Count how many calibration curves will actually be drawn on VAL\n",
        "#    (only models with predict_proba and embeddings with \"val\")\n",
        "# -------------------------------------------\n",
        "def count_real_calib_curves_val():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"val\"]:\n",
        "                continue   # no val embedding -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    count += 1\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_calib_curves_val()\n",
        "print(\"Number of calibration curves on validation set:\", num_curves)\n",
        "\n",
        "# Distinct color palette\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(16, 12))   # bigger figure\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop over models/embeddings on VALIDATION\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"val\"]:\n",
        "            continue\n",
        "\n",
        "        X_val_emb = other_embeddings[\"val\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # ONLY models with predict_proba\n",
        "            if not hasattr(clf, \"predict_proba\"):\n",
        "                continue\n",
        "\n",
        "            probs = clf.predict_proba(X_val_emb)[:, 1]\n",
        "\n",
        "            # reliability diagram\n",
        "            prob_true, prob_pred = calibration_curve(\n",
        "                y_val_bin, probs, n_bins=10, strategy=\"quantile\"\n",
        "            )\n",
        "\n",
        "            # Brier score\n",
        "            brier = brier_score_loss(y_val_bin, probs)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (Brier={brier:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(prob_pred, prob_true, marker=\"o\", linewidth=2,\n",
        "                     label=label, color=color)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Perfect calibration line + styling\n",
        "# -------------------------------------------\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
        "\n",
        "plt.xlabel(\"Predicted probability\", fontsize=16)\n",
        "plt.ylabel(\"Fraction of positives\", fontsize=16)\n",
        "plt.title(\"Calibration curves (Validation Set)\", fontsize=20)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the validation-set calibration results, the three best-calibrated models are all based on SBERT embeddings, which clearly dominate in terms of probability reliability.\n",
        "\n",
        "The MLP with SBERT is the best overall, achieving the lowest Brier score (~0.095) and a calibration curve that stays closest to the diagonal. This indicates that its predicted probabilities closely match the true empirical frequencies across bins.\n",
        "\n",
        "The Logistic Regression with SBERT comes next (Brier ~0.107), showing very stable and well-aligned probabilities despite being a simpler linear model. This suggests that SBERT embeddings already encode the class separation in a way that is easy to calibrate.\n",
        "\n",
        "The XGBoost with SBERT ranks third (Brier ~0.111), still providing good calibration, though with slightly more deviation at intermediate probability levels, likely due to the model’s higher flexibility.\n",
        "\n",
        "Overall, these results indicate that SBERT embeddings consistently lead to well-calibrated models, and among them, MLP achieves the best balance between expressive power and probability reliability."
      ],
      "metadata": {
        "id": "gEO7dj1KN3CU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DL5J5_957Sc"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# -------------------------------------------\n",
        "# Function to generate N very distinct colors (HLS)\n",
        "# -------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        hue = i / n        # equally spaced between 0 and 1\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1) Binary labels for the TRAINING SET\n",
        "# -------------------------------------------\n",
        "y_train_bin = y_train.map(to_binary_label).values\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2) Count how many calibration curves will actually be drawn on TRAIN\n",
        "#    (only models with predict_proba and embeddings with \"train\")\n",
        "# -------------------------------------------\n",
        "def count_real_calib_curves_train():\n",
        "    count = 0\n",
        "    for model_name, emb_dict in models_registry.items():\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "\n",
        "            if emb_name not in other_embeddings[\"train\"]:\n",
        "                continue   # no train embedding -> skip\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    count += 1\n",
        "    return count\n",
        "\n",
        "num_curves = count_real_calib_curves_train()\n",
        "print(\"Number of calibration curves on training set:\", num_curves)\n",
        "\n",
        "# Distinct color palette\n",
        "colors = generate_distinct_colors(num_curves)\n",
        "\n",
        "plt.figure(figsize=(16, 12))   # bigger figure\n",
        "curve_idx = 0\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3) Loop over models/embeddings on TRAINING\n",
        "# -------------------------------------------\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        if emb_name not in other_embeddings[\"train\"]:\n",
        "            continue\n",
        "\n",
        "        X_train_emb = other_embeddings[\"train\"][emb_name]\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # ONLY models with predict_proba\n",
        "            if not hasattr(clf, \"predict_proba\"):\n",
        "                continue\n",
        "\n",
        "            probs = clf.predict_proba(X_train_emb)[:, 1]\n",
        "\n",
        "            # reliability diagram\n",
        "            prob_true, prob_pred = calibration_curve(\n",
        "                y_train_bin, probs, n_bins=10, strategy=\"quantile\"\n",
        "            )\n",
        "\n",
        "            # Brier score\n",
        "            brier = brier_score_loss(y_train_bin, probs)\n",
        "\n",
        "            label = f\"{model_name} | {emb_name} | {variant_name} (Brier={brier:.3f})\"\n",
        "            color = colors[curve_idx]\n",
        "            curve_idx += 1\n",
        "\n",
        "            plt.plot(prob_pred, prob_true, marker=\"o\", linewidth=2,\n",
        "                     label=label, color=color)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4) Perfect calibration line + styling\n",
        "# -------------------------------------------\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
        "\n",
        "plt.xlabel(\"Predicted probability\", fontsize=16)\n",
        "plt.ylabel(\"Fraction of positives\", fontsize=16)\n",
        "plt.title(\"Calibration curves (Training Set)\", fontsize=20)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the training Precision–Recall curves, the three best-performing models all achieve perfect or near-perfect Average Precision (AP ≈ 1.00), which clearly separates them from the rest.\n",
        "\n",
        "The SVC with SBERT embeddings shows a PR curve that stays essentially at precision ≈ 1 across almost the entire recall range, indicating that the model ranks positive (jailbreak) examples almost perfectly on the training data. This suggests extremely strong separability in the SBERT feature space.\n",
        "\n",
        "The Kernel SVM (RBF) with SBERT embeddings exhibits an almost identical behavior, with AP equal to 1.00 and a curve tightly hugging the top-right corner. This confirms that the non-linear decision boundary can fully exploit the expressive power of SBERT on the training set.\n",
        "\n",
        "The MLP with SBERT embeddings also reaches AP ≈ 1.00, maintaining very high precision even at high recall values. This indicates that the neural network easily fits the training distribution when using contextual sentence embeddings.\n",
        "\n",
        "Overall, all three models demonstrate near-perfect training performance, which is a strong signal of representational power but also raises a clear overfitting warning. Their dominance on the training set makes validation and test performance crucial to assess real generalization."
      ],
      "metadata": {
        "id": "4SaKUURhO-og"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66LbSieD6Oxb"
      },
      "source": [
        "## all models for each embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezr-FZ7E6TaT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import colorsys\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, average_precision_score,\n",
        "    roc_curve, roc_auc_score, brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1) Funzione colori molto distinti (HLS)\n",
        "# --------------------------------------------------------------\n",
        "def generate_distinct_colors(n):\n",
        "    colors = []\n",
        "    if n == 0:\n",
        "        return colors\n",
        "    for i in range(n):\n",
        "        hue = i / n\n",
        "        lightness = 0.5\n",
        "        saturation = 0.9\n",
        "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "        colors.append(rgb)\n",
        "    return colors\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2) Etichette binarie\n",
        "# --------------------------------------------------------------\n",
        "y_train_bin = y_train.map(to_binary_label).values\n",
        "y_val_bin   = y_val.map(to_binary_label).values\n",
        "y_test_bin  = y_test.map(to_binary_label).values\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3) Split dictionary\n",
        "# --------------------------------------------------------------\n",
        "splits = {\n",
        "    \"train\": {\"y\": y_train_bin, \"X_dict\": other_embeddings[\"train\"]},\n",
        "    \"val\":   {\"y\": y_val_bin,   \"X_dict\": other_embeddings[\"val\"]},\n",
        "    \"test\":  {\"y\": y_test_bin,  \"X_dict\": other_embeddings[\"test\"]},\n",
        "}\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4) Embedding list\n",
        "# --------------------------------------------------------------\n",
        "embedding_list = list(other_embeddings[\"train\"].keys())\n",
        "print(\"Embeddings trovati:\", embedding_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeLOFaUa67BC"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PRECISION–RECALL CURVES per embedding e split\n",
        "# (ordine: TEST -> VAL -> TRAIN)\n",
        "# ============================================================\n",
        "\n",
        "for emb_name in embedding_list:\n",
        "    print(f\"\\n=== Precision–Recall curves per embedding: {emb_name} ===\")\n",
        "\n",
        "    # ordine esplicito degli split\n",
        "    for split_name in [\"test\", \"val\", \"train\"]:\n",
        "\n",
        "        split_data = splits[split_name]\n",
        "\n",
        "        if emb_name not in split_data[\"X_dict\"]:\n",
        "            print(f\"  -> {emb_name} non presente in {split_name}, skip.\")\n",
        "            continue\n",
        "\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_emb = split_data[\"X_dict\"][emb_name]\n",
        "\n",
        "        # Conta PR curve reali\n",
        "        def count_real_pr_curves():\n",
        "            c = 0\n",
        "            for model_name, emb_dict in models_registry.items():\n",
        "                if emb_name not in emb_dict:\n",
        "                    continue\n",
        "                for variant_name, info in emb_dict[emb_name].items():\n",
        "                    clf = info[\"model\"]\n",
        "                    if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_real_pr_curves()\n",
        "        print(f\"  [{split_name}] num PR curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for model_name, emb_dict in models_registry.items():\n",
        "            if emb_name not in emb_dict:\n",
        "                continue\n",
        "\n",
        "            for variant_name, info in emb_dict[emb_name].items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    scores = clf.predict_proba(X_emb)[:, 1]\n",
        "                elif hasattr(clf, \"decision_function\"):\n",
        "                    scores = clf.decision_function(X_emb)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                precision, recall, _ = precision_recall_curve(y_bin, scores)\n",
        "                ap = average_precision_score(y_bin, scores)\n",
        "\n",
        "                label = f\"{model_name} | {variant_name} (AP={ap:.3f})\"\n",
        "                plt.plot(recall, precision,\n",
        "                         label=label,\n",
        "                         color=colors[curve_idx],\n",
        "                         linewidth=2)\n",
        "\n",
        "                curve_idx += 1\n",
        "\n",
        "        plt.xlabel(\"Recall\", fontsize=14)\n",
        "        plt.ylabel(\"Precision\", fontsize=14)\n",
        "        plt.title(f\"Precision–Recall — {emb_name} — {split_name}\", fontsize=16)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-9MbtpU68vi"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ROC CURVES per embedding e split\n",
        "# (ordine: TEST -> VAL -> TRAIN)\n",
        "# ============================================================\n",
        "\n",
        "for emb_name in embedding_list:\n",
        "    print(f\"\\n=== ROC curves per embedding: {emb_name} ===\")\n",
        "\n",
        "    # ordine esplicito degli split\n",
        "    for split_name in [\"test\", \"val\", \"train\"]:\n",
        "\n",
        "        split_data = splits[split_name]\n",
        "\n",
        "        if emb_name not in split_data[\"X_dict\"]:\n",
        "            print(f\"  -> {emb_name} non presente in {split_name}, skip.\")\n",
        "            continue\n",
        "\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_emb = split_data[\"X_dict\"][emb_name]\n",
        "\n",
        "        def count_real_roc_curves():\n",
        "            c = 0\n",
        "            for model_name, emb_dict in models_registry.items():\n",
        "                if emb_name not in emb_dict:\n",
        "                    continue\n",
        "                for variant_name, info in emb_dict[emb_name].items():\n",
        "                    clf = info[\"model\"]\n",
        "                    if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_real_roc_curves()\n",
        "        print(f\"  [{split_name}] num ROC curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for model_name, emb_dict in models_registry.items():\n",
        "            if emb_name not in emb_dict:\n",
        "                continue\n",
        "\n",
        "            for variant_name, info in emb_dict[emb_name].items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    scores = clf.predict_proba(X_emb)[:, 1]\n",
        "                elif hasattr(clf, \"decision_function\"):\n",
        "                    scores = clf.decision_function(X_emb)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                fpr, tpr, _ = roc_curve(y_bin, scores)\n",
        "                auc_val = roc_auc_score(y_bin, scores)\n",
        "\n",
        "                label = f\"{model_name} | {variant_name} (AUC={auc_val:.3f})\"\n",
        "                plt.plot(\n",
        "                    fpr,\n",
        "                    tpr,\n",
        "                    label=label,\n",
        "                    color=colors[curve_idx],\n",
        "                    linewidth=2\n",
        "                )\n",
        "                curve_idx += 1\n",
        "\n",
        "        # baseline random\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random baseline\")\n",
        "\n",
        "        plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
        "        plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
        "        plt.title(f\"ROC — {emb_name} — {split_name}\", fontsize=16)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BThMhqhi6_Dq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CALIBRATION CURVES per embedding e split\n",
        "# (ordine: TEST -> VAL -> TRAIN)\n",
        "# ============================================================\n",
        "\n",
        "for emb_name in embedding_list:\n",
        "    print(f\"\\n=== Calibration curves per embedding: {emb_name} ===\")\n",
        "\n",
        "    # ordine esplicito degli split\n",
        "    for split_name in [\"test\", \"val\", \"train\"]:\n",
        "\n",
        "        split_data = splits[split_name]\n",
        "\n",
        "        if emb_name not in split_data[\"X_dict\"]:\n",
        "            print(f\"  -> {emb_name} non presente in {split_name}, skip.\")\n",
        "            continue\n",
        "\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_emb = split_data[\"X_dict\"][emb_name]\n",
        "\n",
        "        # Conta curve reali\n",
        "        def count_real_calib_curves():\n",
        "            c = 0\n",
        "            for model_name, emb_dict in models_registry.items():\n",
        "                if emb_name not in emb_dict:\n",
        "                    continue\n",
        "                for variant_name, info in emb_dict[emb_name].items():\n",
        "                    if hasattr(info[\"model\"], \"predict_proba\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_real_calib_curves()\n",
        "        print(f\"  [{split_name}] num Calibration curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Disegno curve\n",
        "        for model_name, emb_dict in models_registry.items():\n",
        "            if emb_name not in emb_dict:\n",
        "                continue\n",
        "\n",
        "            for variant_name, info in emb_dict[emb_name].items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if not hasattr(clf, \"predict_proba\"):\n",
        "                    continue\n",
        "\n",
        "                probs = clf.predict_proba(X_emb)[:, 1]\n",
        "\n",
        "                prob_true, prob_pred = calibration_curve(\n",
        "                    y_bin,\n",
        "                    probs,\n",
        "                    n_bins=10,\n",
        "                    strategy=\"quantile\"\n",
        "                )\n",
        "\n",
        "                brier = brier_score_loss(y_bin, probs)\n",
        "\n",
        "                label = f\"{model_name} | {variant_name} (Brier={brier:.3f})\"\n",
        "\n",
        "                plt.plot(\n",
        "                    prob_pred,\n",
        "                    prob_true,\n",
        "                    marker=\"o\",\n",
        "                    label=label,\n",
        "                    color=colors[curve_idx],\n",
        "                    linewidth=2\n",
        "                )\n",
        "\n",
        "                curve_idx += 1\n",
        "\n",
        "        # Linea modello perfettamente calibrato\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
        "\n",
        "        plt.xlabel(\"Predicted probability\", fontsize=14)\n",
        "        plt.ylabel(\"Fraction of positives\", fontsize=14)\n",
        "        plt.title(f\"Calibration — {emb_name} — {split_name}\", fontsize=16)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmhr9apPA15n"
      },
      "source": [
        "## all embeddings for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY6xO8JhA5oY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "split_order = [\"test\", \"val\", \"train\"]\n",
        "\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"=== PRECISION–RECALL: {model_name} ===\")\n",
        "    print(f\"============================\")\n",
        "\n",
        "    for split_name in split_order:\n",
        "        split_data = splits[split_name]\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_dict = split_data[\"X_dict\"]\n",
        "\n",
        "        # Conta quante curve avremo (embedding+variante di QUESTO modello che esistono nello split)\n",
        "        def count_curves_for_model_split():\n",
        "            c = 0\n",
        "            for emb_name, variants in emb_dict.items():\n",
        "                if emb_name not in X_dict:\n",
        "                    continue\n",
        "                for variant_name, info in variants.items():\n",
        "                    clf = info[\"model\"]\n",
        "                    if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_curves_for_model_split()\n",
        "        print(f\"  [{split_name}] num PR curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "            if emb_name not in X_dict:\n",
        "                continue\n",
        "            X_emb = X_dict[emb_name]\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    scores = clf.predict_proba(X_emb)[:, 1]\n",
        "                elif hasattr(clf, \"decision_function\"):\n",
        "                    scores = clf.decision_function(X_emb)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                precision, recall, _ = precision_recall_curve(y_bin, scores)\n",
        "                ap = average_precision_score(y_bin, scores)\n",
        "\n",
        "                label = f\"{emb_name} | {variant_name} (AP={ap:.3f})\"\n",
        "                plt.plot(\n",
        "                    recall, precision,\n",
        "                    label=label,\n",
        "                    color=colors[curve_idx],\n",
        "                    linewidth=2\n",
        "                )\n",
        "                curve_idx += 1\n",
        "\n",
        "        plt.xlabel(\"Recall\", fontsize=13)\n",
        "        plt.ylabel(\"Precision\", fontsize=13)\n",
        "        plt.title(f\"PR – {model_name} – {split_name}\", fontsize=15)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"lower left\", fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQbF-7O7CVQg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "split_order = [\"test\", \"val\", \"train\"]\n",
        "\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"=== ROC: {model_name} ===\")\n",
        "    print(f\"============================\")\n",
        "\n",
        "    for split_name in split_order:\n",
        "        split_data = splits[split_name]\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_dict = split_data[\"X_dict\"]\n",
        "\n",
        "        def count_curves_for_model_split():\n",
        "            c = 0\n",
        "            for emb_name, variants in emb_dict.items():\n",
        "                if emb_name not in X_dict:\n",
        "                    continue\n",
        "                for variant_name, info in variants.items():\n",
        "                    clf = info[\"model\"]\n",
        "                    if hasattr(clf, \"predict_proba\") or hasattr(clf, \"decision_function\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_curves_for_model_split()\n",
        "        print(f\"  [{split_name}] num ROC curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "            if emb_name not in X_dict:\n",
        "                continue\n",
        "            X_emb = X_dict[emb_name]\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if hasattr(clf, \"predict_proba\"):\n",
        "                    scores = clf.predict_proba(X_emb)[:, 1]\n",
        "                elif hasattr(clf, \"decision_function\"):\n",
        "                    scores = clf.decision_function(X_emb)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                fpr, tpr, _ = roc_curve(y_bin, scores)\n",
        "                auc_val = roc_auc_score(y_bin, scores)\n",
        "\n",
        "                label = f\"{emb_name} | {variant_name} (AUC={auc_val:.3f})\"\n",
        "                plt.plot(\n",
        "                    fpr, tpr,\n",
        "                    label=label,\n",
        "                    color=colors[curve_idx],\n",
        "                    linewidth=2\n",
        "                )\n",
        "                curve_idx += 1\n",
        "\n",
        "        # baseline random\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random baseline\")\n",
        "\n",
        "        plt.xlabel(\"False Positive Rate\", fontsize=13)\n",
        "        plt.ylabel(\"True Positive Rate\", fontsize=13)\n",
        "        plt.title(f\"ROC – {model_name} – {split_name}\", fontsize=15)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"lower right\", fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6lc5tnJCXXe"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "split_order = [\"test\", \"val\", \"train\"]\n",
        "\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"=== CALIBRATION: {model_name} ===\")\n",
        "    print(f\"============================\")\n",
        "\n",
        "    for split_name in split_order:\n",
        "        split_data = splits[split_name]\n",
        "        y_bin = split_data[\"y\"]\n",
        "        X_dict = split_data[\"X_dict\"]\n",
        "\n",
        "        def count_curves_for_model_split():\n",
        "            c = 0\n",
        "            for emb_name, variants in emb_dict.items():\n",
        "                if emb_name not in X_dict:\n",
        "                    continue\n",
        "                for variant_name, info in variants.items():\n",
        "                    clf = info[\"model\"]\n",
        "                    if hasattr(clf, \"predict_proba\"):\n",
        "                        c += 1\n",
        "            return c\n",
        "\n",
        "        num_curves = count_curves_for_model_split()\n",
        "        print(f\"  [{split_name}] num Calibration curves: {num_curves}\")\n",
        "        if num_curves == 0:\n",
        "            continue\n",
        "\n",
        "        colors = generate_distinct_colors(num_curves)\n",
        "        curve_idx = 0\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for emb_name, variants in emb_dict.items():\n",
        "            if emb_name not in X_dict:\n",
        "                continue\n",
        "            X_emb = X_dict[emb_name]\n",
        "\n",
        "            for variant_name, info in variants.items():\n",
        "                clf = info[\"model\"]\n",
        "\n",
        "                if not hasattr(clf, \"predict_proba\"):\n",
        "                    continue\n",
        "\n",
        "                probs = clf.predict_proba(X_emb)[:, 1]\n",
        "\n",
        "                prob_true, prob_pred = calibration_curve(\n",
        "                    y_bin, probs, n_bins=10, strategy=\"quantile\"\n",
        "                )\n",
        "\n",
        "                brier = brier_score_loss(y_bin, probs)\n",
        "\n",
        "                label = f\"{emb_name} | {variant_name} (Brier={brier:.3f})\"\n",
        "                plt.plot(\n",
        "                    prob_pred,\n",
        "                    prob_true,\n",
        "                    marker=\"o\",\n",
        "                    label=label,\n",
        "                    color=colors[curve_idx],\n",
        "                    linewidth=2\n",
        "                )\n",
        "                curve_idx += 1\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
        "\n",
        "        plt.xlabel(\"Predicted probability\", fontsize=13)\n",
        "        plt.ylabel(\"Fraction of positives\", fontsize=13)\n",
        "        plt.title(f\"Calibration – {model_name} – {split_name}\", fontsize=15)\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXdDUiFEJHG7"
      },
      "source": [
        "## Best models for each metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCE7HTVeJMJz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# 1️⃣ label → 0/1\n",
        "def to_binary_labels(y_series):\n",
        "    return y_series.map(lambda v: 1 if str(v).lower() == \"jailbreak\" else 0).values\n",
        "\n",
        "# 2️⃣ get the respective data split\n",
        "def get_X_for_split(emb_name: str, split_name: str):\n",
        "    if emb_name == \"bow\":\n",
        "        if split_name == \"train\":\n",
        "            return X_train_vector\n",
        "        elif split_name == \"val\":\n",
        "            return X_val_vector\n",
        "        elif split_name == \"test\":\n",
        "            return X_test_vector\n",
        "        else:\n",
        "            raise ValueError(f\"Split sconosciuto: {split_name}\")\n",
        "    else:\n",
        "        return other_embeddings[split_name][emb_name]\n",
        "\n",
        "# Label binarie per tutti gli split\n",
        "y_train_bin = to_binary_labels(y_train)\n",
        "y_val_bin   = to_binary_labels(y_val)\n",
        "y_test_bin  = to_binary_labels(y_test)\n",
        "\n",
        "print(\"Setup OK!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MySnAIOkKai7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             roc_auc_score, average_precision_score)\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Ensure we use the binary ground truth\n",
        "y_true_test_bin = y_test_bin\n",
        "\n",
        "for model_name, emb_dict in models_registry.items():\n",
        "    for emb_name, variants in emb_dict.items():\n",
        "\n",
        "        # Check if embedding is available\n",
        "        try:\n",
        "            X_test_emb = get_X_for_split(emb_name, \"test\")\n",
        "        except KeyError:\n",
        "            print(f\"⚠️ No test embedding for {model_name}/{emb_name}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        for variant_name, info in variants.items():\n",
        "            clf = info[\"model\"]\n",
        "\n",
        "            # 1. Discrete predictions\n",
        "            try:\n",
        "                y_pred = clf.predict(X_test_emb)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Predict failed for {model_name}/{emb_name}/{variant_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # --- 🛠️ CRITICAL FIX HERE 🛠️ ---\n",
        "            # If the model predicts strings (e.g., 'jailbreak'), convert them to integers (0/1).\n",
        "            # This ensures types match y_true_test_bin.\n",
        "            if y_pred.dtype.kind in {'U', 'S', 'O'}:  # Check for string/object types\n",
        "                # Map 'jailbreak' to 1, everything else (e.g., 'benign') to 0\n",
        "                y_pred = np.where(y_pred == 'jailbreak', 1, 0)\n",
        "\n",
        "            # Now both y_true_test_bin and y_pred are integers.\n",
        "            # --------------------------------\n",
        "\n",
        "            acc = accuracy_score(y_true_test_bin, y_pred)\n",
        "\n",
        "            # pos_label=1 because we are now working with 0s and 1s\n",
        "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "                y_true_test_bin, y_pred,\n",
        "                average=\"binary\", pos_label=1\n",
        "            )\n",
        "\n",
        "            # 2. Continuous scores (probabilities)\n",
        "            if hasattr(clf, \"predict_proba\"):\n",
        "                try:\n",
        "                    # Usually predict_proba returns [prob_class_0, prob_class_1]\n",
        "                    # We need to check the order of classes in clf.classes_\n",
        "                    class_1_index = np.where(clf.classes_ == 'jailbreak')[0][0]\n",
        "                    scores = clf.predict_proba(X_test_emb)[:, class_1_index]\n",
        "                except (IndexError, AttributeError):\n",
        "                    # Fallback if classes are not strings or logic fails\n",
        "                    scores = clf.predict_proba(X_test_emb)[:, 1]\n",
        "            elif hasattr(clf, \"decision_function\"):\n",
        "                scores = clf.decision_function(X_test_emb)\n",
        "            else:\n",
        "                scores = None\n",
        "\n",
        "            if scores is not None:\n",
        "                try:\n",
        "                    auc_roc = roc_auc_score(y_true_test_bin, scores)\n",
        "                    ap      = average_precision_score(y_true_test_bin, scores)\n",
        "                except ValueError:\n",
        "                    auc_roc = np.nan\n",
        "                    ap      = np.nan\n",
        "            else:\n",
        "                auc_roc = np.nan\n",
        "                ap      = np.nan\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"embedding\": emb_name,\n",
        "                \"variant\": variant_name,\n",
        "                \"accuracy\": acc,\n",
        "                \"precision\": prec,\n",
        "                \"recall\": rec,\n",
        "                \"f1\": f1,\n",
        "                \"auc_roc\": auc_roc,\n",
        "                \"avg_precision\": ap,\n",
        "            })\n",
        "\n",
        "# DataFrame sorted by descending F1\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df_sorted = results_df.sort_values(by=\"f1\", ascending=False)\n",
        "\n",
        "print(\"\\n🎯 Top 10 models on Test (sorted by F1):\")\n",
        "results_df_sorted.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXjzWiQrKkvK"
      },
      "outputs": [],
      "source": [
        "N_BEST = 3\n",
        "\n",
        "best_models = results_df_sorted.head(N_BEST).copy()\n",
        "print(\"🏆 best models:\")\n",
        "best_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1vKXKViKvNp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_on_all_splits(model, emb_name: str, title: str = \"\"):\n",
        "    print(\"\\n\" + \"#\" * 80)\n",
        "    print(title)\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    # Ordine desiderato: TEST → VAL → TRAIN\n",
        "    for split_name, (X_mat, y_vec) in {\n",
        "        \"TEST\":  (get_X_for_split(emb_name, \"test\"),  y_test),\n",
        "        \"VAL\":   (get_X_for_split(emb_name, \"val\"),   y_val),\n",
        "        \"TRAIN\": (get_X_for_split(emb_name, \"train\"), y_train),\n",
        "    }.items():\n",
        "\n",
        "        print(f\"\\n🔹 Split: {split_name}\")\n",
        "        evaluate_classifier(\n",
        "            model,\n",
        "            X_mat,\n",
        "            y_vec,\n",
        "            pos_label=\"jailbreak\",\n",
        "            neg_label=\"benign\",\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsupV63vNWDo"
      },
      "outputs": [],
      "source": [
        "# 🔁 Metriche che vuoi considerare\n",
        "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc_roc\", \"avg_precision\"]\n",
        "\n",
        "TOP_K = 3  # quanti modelli top per metrica\n",
        "\n",
        "for metric in metrics:\n",
        "    if metric not in results_df.columns:\n",
        "        print(f\"\\n⚠️ Metrica {metric} non trovata nel DataFrame, skip.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"🏅 TOP {TOP_K} MODELS PER METRIC {metric} (evaluated on TEST SET)\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # scarta eventuali NaN per quella metrica\n",
        "    df_metric = results_df.dropna(subset=[metric])\n",
        "\n",
        "    if df_metric.empty:\n",
        "        print(f\"⚠️ Nessun modello valido per metrica {metric}, skip.\")\n",
        "        continue\n",
        "\n",
        "    # ordina decrescente per la metrica\n",
        "    df_metric_sorted = df_metric.sort_values(by=metric, ascending=False)\n",
        "\n",
        "    # prendi i top-K\n",
        "    top_k = df_metric_sorted.head(TOP_K).copy()\n",
        "    display(top_k)  # comodo in notebook per vedere la tabella\n",
        "\n",
        "    # per ciascuno dei top-K: chiama evaluate_model_on_all_splits\n",
        "    for rank, (_, row) in enumerate(top_k.iterrows(), start=1):\n",
        "        model_name   = row[\"model\"]\n",
        "        emb_name     = row[\"embedding\"]\n",
        "        variant_name = row[\"variant\"]\n",
        "\n",
        "        clf = models_registry[model_name][emb_name][variant_name][\"model\"]\n",
        "\n",
        "        title = (\n",
        "            f\"🏆 Metric: {metric} | Rank: {rank} | \"\n",
        "            f\"MODEL: {model_name} | EMB: {emb_name} | VAR: {variant_name} | \"\n",
        "            f\"{metric} (test) = {row[metric]:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Usa la tua funzione già definita\n",
        "        evaluate_model_on_all_splits(clf, emb_name, title)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9r_0yoRmmqVi",
        "vKTtNaN1d29l",
        "m6aF1t_oWqSW",
        "WFrmYkDeWqSX",
        "aE3qsy_SRZ9L",
        "_RXz-ztvWcY1",
        "TrnsYK3uWcY5",
        "JUISfhC-yHHX",
        "otQCXzhEnnlr",
        "Q8QwBULsPXoM",
        "5734Yis2Qhsu",
        "drcZXzmCE1ej",
        "UkJFPAODbrql",
        "33hxhtKHbYkA",
        "mr6Ohf8BjKtY",
        "W1aIqWyanT6x",
        "SawR1-_e8_iH",
        "xpQXx9Y7zJrl",
        "IcpL7L0bW3fw",
        "RgkEjXxhW3fy",
        "Wg01oWb4W3fz",
        "9lMc3O8kW3f6",
        "zNYKhMkHW3f-",
        "TlZDNLJmWU2Z",
        "mBjo7uFMSLCI",
        "-NC7sXIaOo88",
        "InwW36b-3B_7",
        "aEB2MOyJ3B_8",
        "Z8-0WP-A3B_9",
        "ocXdY2ccVn0Y",
        "Q_E6kc06BmJi",
        "ElyxkAPQ5ZYl",
        "lLkeOUQ4BmJn",
        "HnF48gaY5ctU",
        "5LWw9kc1m8MX",
        "w7Wl53Ol5jPG",
        "gzw7CnTqNFoQ",
        "pCbRs4HkrEKL",
        "Qi6kQV04rOPv",
        "ZQokeHQMrUSP",
        "NtYji8bSred3",
        "zdJmvnlPci9B",
        "pc8LGknIc4rw",
        "pK38vpGO1Zmu",
        "kIo-fNpn3R8M",
        "onBAcAQX4hsM",
        "66LbSieD6Oxb",
        "qmhr9apPA15n"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}